{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chemprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./dataset/chemprop/tests/data/regression/mol/mol.csv\"\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'smiles' # name of the column containing SMILES strings\n",
    "target_columns = ['lipo'] # list of names of the columns containing targets\n",
    "\n",
    "df_input = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smis = df_input.loc[:, smiles_column].values\n",
    "ys = df_input.loc[:, target_columns].values\n",
    "\n",
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]\n",
    "\n",
    "mols = [d.mol for d in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x14e005070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mols[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Into pyg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "\n",
    "# Initialize the Chemprop featurizer\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "def smiles_to_graph_with_targets(smiles, targets, idx):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None  # Handle invalid SMILES\n",
    "\n",
    "    # Use Chemprop featurizer to extract graph features\n",
    "    try:\n",
    "        mol_graph = featurizer(mol)  # MolGraph object\n",
    "    except Exception as e:\n",
    "        print(f\"Error featurizing molecule {smiles}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Extract atom and bond features\n",
    "    atom_features = torch.tensor(mol_graph.V, dtype=torch.float)  # Node features\n",
    "    bond_features = torch.tensor(mol_graph.E, dtype=torch.float)  # Edge features\n",
    "\n",
    "    # Extract edge index\n",
    "    edge_index = torch.tensor(mol_graph.edge_index, dtype=torch.long)\n",
    "\n",
    "    # Reverse edge indices for bidirectional edges (optional, depends on model requirements)\n",
    "    rev_edge_index = torch.tensor(mol_graph.rev_edge_index, dtype=torch.long)\n",
    "\n",
    "    # Convert targets to numeric and handle invalid values\n",
    "    targets = np.array(targets, dtype=np.float64)\n",
    "    targets = torch.tensor(targets, dtype=torch.float).view(1, -1)\n",
    "\n",
    "    # Create PyG Data object\n",
    "    graph_data = Data(\n",
    "        x=atom_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=bond_features,\n",
    "        y=targets,\n",
    "        smiles=smiles,\n",
    "        idx=idx\n",
    "    )\n",
    "    return graph_data\n",
    "\n",
    "# Function to filter simple molecules\n",
    "def filter_simple_molecules(data):\n",
    "    filtered_data = data.copy()\n",
    "    for i, row in data.iterrows():\n",
    "        smiles = row['smiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            num_atoms = mol.GetNumAtoms()\n",
    "            num_bonds = mol.GetNumBonds()\n",
    "            # Exclude molecules with fewer than 3 atoms or 2 bonds\n",
    "            if num_atoms < 3 or num_bonds < 2:\n",
    "                filtered_data = filtered_data.drop(index=i)\n",
    "        else:\n",
    "            # Drop invalid SMILES\n",
    "            filtered_data = filtered_data.drop(index=i)\n",
    "    return filtered_data\n",
    "\n",
    "# Preprocess the dataset to filter out simple molecules\n",
    "df_filtered = filter_simple_molecules(df_input)\n",
    "\n",
    "# Apply the function to the filtered dataset\n",
    "graphs = []\n",
    "for i, row in df_filtered.iterrows():\n",
    "    try:\n",
    "        # Extract regression targets and ensure they are numeric\n",
    "        targets = row[['lipo']].values\n",
    "        graph = smiles_to_graph_with_targets(row['smiles'], targets, i)\n",
    "        if graph:\n",
    "            graphs.append(graph)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[24, 72], edge_index=[2, 54], edge_attr=[54, 14], y=[1, 1], smiles='Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14', idx=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data, self.slices = self.collate(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dataset = CustomDataset(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.transforms import VirtualNode, AddLaplacianEigenvectorPE\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_dense_adj, degree\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        \n",
    "\n",
    "###Visual Node\n",
    "def apply_vn(pyg_dataset):\n",
    "    vn_dataset = copy.deepcopy(pyg_dataset)  # Make a deep copy to preserve the original dataset\n",
    "    transform = VirtualNode()\n",
    "\n",
    "    # Create a list to store transformed graphs\n",
    "    transformed_data_list = []\n",
    "\n",
    "    for data in vn_dataset:\n",
    "        transformed_data_list.append(transform(data))\n",
    "\n",
    "    vn_dataset = CustomDataset(transformed_data_list)\n",
    "\n",
    "    return vn_dataset\n",
    "\n",
    "###Centrality\n",
    "def add_centrality_to_node_features(data, centrality_measure='degree'):\n",
    "    G = to_networkx(data, node_attrs=['x'], to_undirected=True)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'betweenness':\n",
    "        centrality = nx.betweenness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "    \n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = list(centrality.values())\n",
    "    centrality_tensor = torch.tensor(centrality_values, dtype=torch.float).view(-1, 1)\n",
    "    centrality_tensor = (centrality_tensor - centrality_tensor.mean()) / (centrality_tensor.std() + 1e-8)\n",
    "    data.x = torch.cat([data.x, centrality_tensor], dim=-1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def centrality(dataset, centrality_measure='degree'):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    addCentrality_list = []\n",
    "    for data in original_dataset:\n",
    "        if centrality_measure == 'degree':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='degree')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'closeness':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='closeness')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'betweenness':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='betweenness')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'eigenvector':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='eigenvector')\n",
    "            addCentrality_list.append(data)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "        \n",
    "    addCentrality_dataset = CustomDataset(addCentrality_list)\n",
    "\n",
    "    return addCentrality_dataset\n",
    "\n",
    "###Distance Encoding\n",
    "def distance_encoding_node_augmentation(data):\n",
    "    G = to_networkx(data, node_attrs=['x'], to_undirected = True)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Initialize the distance matrix with infinity\n",
    "    distance_matrix = [[float('inf')] * num_nodes for _ in range(num_nodes)]\n",
    "    shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    # Populate the distance matrix with actual shortest path lengths\n",
    "    for i in range(num_nodes):\n",
    "        distance_matrix[i][i] = 0  # Distance to self is 0\n",
    "        if i in shortest_paths:\n",
    "            for j, d in shortest_paths[i].items():\n",
    "                distance_matrix[i][j] = d\n",
    "\n",
    "    # Convert the distance matrix to a tensor\n",
    "    distance_tensor = torch.tensor(distance_matrix, dtype=torch.float)\n",
    "    \n",
    "    # Example: Add average distance to node features\n",
    "    finite_distances = torch.where(distance_tensor == float('inf'), torch.tensor(float('nan')), distance_tensor)\n",
    "    average_distance = torch.nanmean(finite_distances, dim=1).view(-1, 1)  # Use nanmean to ignore infinities\n",
    "    data.x = torch.cat([data.x, average_distance], dim=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def distance_encoding_edge_rewiring(data):\n",
    "    \"\"\"\n",
    "    Add edges between all pairs of nodes with shortest path distance as a new edge attribute,\n",
    "    while preserving original edge attributes.\n",
    "    \"\"\"\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'], to_undirected=True)\n",
    "    G_transformed = G.copy()\n",
    "\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    shortest_paths = {}\n",
    "\n",
    "    # Compute shortest paths for each connected component\n",
    "    for component in connected_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        component_paths = dict(nx.all_pairs_shortest_path_length(subgraph))\n",
    "        shortest_paths.update(component_paths)\n",
    "\n",
    "    num_edge_attrs = data.edge_attr.shape[1] if data.edge_attr is not None else 0\n",
    "\n",
    "    nodes = list(G.nodes)\n",
    "    for i in nodes:\n",
    "        for j in nodes:\n",
    "            if i != j:\n",
    "                if G.has_edge(i, j):\n",
    "                    original_attr = G[i][j].get('edge_attr', [])\n",
    "                    if not isinstance(original_attr, list):\n",
    "                        original_attr = [original_attr]\n",
    "                    G_transformed[i][j]['edge_attr'] = original_attr + [1]\n",
    "                else:\n",
    "                    if j in shortest_paths[i]:\n",
    "                        distance = shortest_paths[i][j]\n",
    "                    else:\n",
    "                        distance = 1e9  # Replace inf with a large finite value\n",
    "                    new_attr = [0] * num_edge_attrs + [distance]\n",
    "                    G_transformed.add_edge(i, j, edge_attr=new_attr)\n",
    "\n",
    "    new_data = from_networkx(G_transformed, group_node_attrs=['x'], group_edge_attrs=['edge_attr'])\n",
    "    new_data.edge_attr = torch.tensor(new_data.edge_attr, dtype=torch.float)\n",
    "\n",
    "    # Check for invalid values\n",
    "    if torch.isnan(new_data.edge_attr).any() or torch.isinf(new_data.edge_attr).any():\n",
    "        raise ValueError(\"Edge attributes contain invalid values!\")\n",
    "\n",
    "    new_data.y = data.y\n",
    "    return new_data\n",
    "\n",
    "def distance_encoding(dataset, method = 'node_augmentation'):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    distance_encoding_list = []\n",
    "    for data in original_dataset:\n",
    "        if method == 'node_augmentation':\n",
    "            data = distance_encoding_node_augmentation(data)\n",
    "            distance_encoding_list.append(data)\n",
    "        elif method == 'edge_rewiring':\n",
    "            data = distance_encoding_edge_rewiring(data)\n",
    "            distance_encoding_list.append(data)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown distance encoding method: {method}')\n",
    "    distance_encoding_dataset = CustomDataset(distance_encoding_list)\n",
    "    return distance_encoding_dataset\n",
    "\n",
    "###Subgraph Extraction\n",
    "def extract_local_subgraph_features(data, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'], to_undirected=True)\n",
    "\n",
    "    # Initialize a list to store subgraph features for each node\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        # Extract the ego graph (subgraph) around the node\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        \n",
    "        # Example feature 1: Size of the subgraph (number of nodes)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "        \n",
    "        # Example feature 2: Average degree of the subgraph\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "        \n",
    "    # Convert the features to tensors and add them as node features\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Concatenate the new features to the existing node features\n",
    "    data.x = torch.cat([data.x, subgraph_sizes_tensor, subgraph_degrees_tensor], dim=-1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def subgraph_extraction(dataset, radius=2):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    subgraph_extraction_list = []\n",
    "    for data in original_dataset:\n",
    "        data = extract_local_subgraph_features(data, radius=radius)\n",
    "        subgraph_extraction_list.append(data)\n",
    "    subgraph_extraction_dataset = CustomDataset(subgraph_extraction_list)\n",
    "    return subgraph_extraction_dataset\n",
    "\n",
    "def canonicalize_eigenvectors(eigenvectors):\n",
    "    \"\"\"\n",
    "    Canonicalize eigenvectors by fixing their signs for consistency.\n",
    "    This ensures that isomorphic graphs will have the same eigenvectors.\n",
    "    \"\"\"\n",
    "    for i in range(eigenvectors.shape[1]):\n",
    "        if eigenvectors[0, i] < 0:  # Flip sign if the first element is negative\n",
    "            eigenvectors[:, i] = -eigenvectors[:, i]\n",
    "    return eigenvectors\n",
    "\n",
    "def add_canonicalized_laplacian_pe_pyg(data, k=5, max_features=12):\n",
    "    \"\"\"\n",
    "    Add canonicalized Laplacian positional encoding to a PyG data object.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): PyG data object.\n",
    "        k (int): Number of Laplacian eigenvectors to compute.\n",
    "        max_features (int): Total desired node feature dimensions after encoding.\n",
    "\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): PyG data object with Laplacian PE appended to node features.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert PyG graph to NetworkX graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Step 2: Compute sparse adjacency matrix\n",
    "    adj = nx.to_scipy_sparse_array(G, format='csr').astype(np.float64)\n",
    "\n",
    "    # Step 3: Compute normalized Laplacian matrix\n",
    "    laplacian = csgraph.laplacian(adj, normed=True)\n",
    "\n",
    "    # Step 4: Handle small graphs\n",
    "    num_nodes = adj.shape[0]\n",
    "    if num_nodes <= 1:\n",
    "        raise ValueError(f\"Graph has too few nodes ({num_nodes}) for Laplacian PE.\")\n",
    "    num_eigenvectors = min(k, num_nodes - 1)  # Ensure k < num_nodes\n",
    "\n",
    "    # Step 5: Compute the smallest eigenvectors using sparse methods\n",
    "    eigenvalues, eigenvectors = eigsh(laplacian, k=num_eigenvectors, which='SM')  # Smallest magnitude eigenvalues\n",
    "\n",
    "    # Step 6: Canonicalize eigenvectors\n",
    "    eigenvectors = canonicalize_eigenvectors(torch.tensor(eigenvectors, dtype=torch.float))\n",
    "\n",
    "    # Step 7: Pad eigenvectors if less than max_features\n",
    "    padding = torch.zeros((eigenvectors.shape[0], max_features - num_eigenvectors))\n",
    "    eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "\n",
    "    # Step 8: Append the eigenvectors as new node features\n",
    "    if 'x' in data:\n",
    "        num_existing_features = data.x.shape[1]\n",
    "        padding_existing = torch.zeros((data.x.shape[0], max_features - num_existing_features))\n",
    "        data.x = torch.cat([data.x, padding_existing, eigenvectors], dim=1)\n",
    "    else:\n",
    "        data.x = eigenvectors\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_encoding(dataset, k=3, max_features=76, batch_size=100):\n",
    "    \"\"\"\n",
    "    Apply canonicalized Laplacian positional encoding to a PyG dataset in batches.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of torch_geometric.data.Data): List of PyG data objects.\n",
    "        k (int): Number of Laplacian eigenvectors to compute.\n",
    "        max_features (int): Total desired node feature dimensions after encoding.\n",
    "        batch_size (int): Number of graphs to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset (list of torch_geometric.data.Data): List of PyG data objects with Laplacian PE added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        for data in batch:\n",
    "            data_copy = data.clone()  # Ensure original dataset remains unchanged\n",
    "            try:\n",
    "                graph_pe = add_canonicalized_laplacian_pe_pyg(data_copy, k=k, max_features=max_features)\n",
    "                encoded_dataset.append(graph_pe)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing graph (index {i}): {e}\")\n",
    "    ge_dataset = CustomDataset(encoded_dataset)\n",
    "    return ge_dataset\n",
    "\n",
    "###Add Extra Node on Each Edge\n",
    "def add_extra_node_on_each_edge(data):\n",
    "    # Convert PyG data to a NetworkX graph for easier manipulation\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs = ['edge_attr'])\n",
    "    \n",
    "    # Original number of nodes\n",
    "    num_original_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Prepare lists for new features\n",
    "    edges = list(G.edges(data=True))\n",
    "    new_node_features = []\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_edge_features = []\n",
    "\n",
    "    for u, v, edge_data in edges:\n",
    "        # Remove the original edge\n",
    "        G.remove_edge(u, v)\n",
    "\n",
    "        # Create new node as the mean of connected node features\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        new_node_feature = (data.x[u] + data.x[v]) / 2\n",
    "        new_node_features.append(new_node_feature)\n",
    "        \n",
    "        # Add new node with feature\n",
    "        G.add_node(new_node_id, x=new_node_feature)\n",
    "\n",
    "        # Add edges from new node to each original node\n",
    "        G.add_edge(u, new_node_id)\n",
    "        G.add_edge(new_node_id, v)\n",
    "\n",
    "        # Use original edge feature for each new edge\n",
    "        edge_feature = edge_data['edge_attr']\n",
    "        edge_feature_tensor = (\n",
    "            edge_feature if isinstance(edge_feature, torch.Tensor) else torch.tensor(edge_feature)\n",
    "        )\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (u, new_node_id)\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (new_node_id, v)\n",
    "    \n",
    "    # Convert back to PyG Data object\n",
    "    modified_data = from_networkx(G)\n",
    "\n",
    "    # Update node features\n",
    "    modified_data.x = torch.cat([data.x, torch.stack(new_node_features)], dim=0)\n",
    "\n",
    "    # Update edge features to include only the new edges\n",
    "    modified_data.edge_attr = torch.stack(new_edge_features)  # Only include new edge features\n",
    "\n",
    "    # Preserve any additional global attributes\n",
    "    modified_data.y = data.y\n",
    "    \n",
    "    return modified_data\n",
    "\n",
    "def extra_node(dataset):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    extra_node_list = []\n",
    "    for data in original_dataset:\n",
    "        data = add_extra_node_on_each_edge(data)\n",
    "        extra_node_list.append(data)\n",
    "    extra_node_dataset = CustomDataset(extra_node_list)\n",
    "    return extra_node_dataset\n",
    "\n",
    "def count_3_star(G):\n",
    "    \"\"\"Count 3-star graphlets for each node.\"\"\"\n",
    "    star_counts = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degree = len(neighbors)\n",
    "        # Count the number of 3-combinations of neighbors\n",
    "        star_counts[node] = max(0, (degree * (degree - 1) * (degree - 2)) // 6)\n",
    "    return star_counts\n",
    "\n",
    "def count_tailed_triangle(G):\n",
    "    \"\"\"Count tailed triangle graphlets for each node.\"\"\"\n",
    "    tail_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for neighbor in neighbors:\n",
    "            for other in neighbors:\n",
    "                if neighbor != other and G.has_edge(neighbor, other):\n",
    "                    for extra in G.neighbors(node):\n",
    "                        if extra not in {neighbor, other}:\n",
    "                            tail_counts[node] += 1\n",
    "    return tail_counts\n",
    "\n",
    "def count_4_cycle(G):\n",
    "    \"\"\"Count 4-cycle graphlets for each node in an undirected graph G.\"\"\"\n",
    "    cycle_counts = {node: 0 for node in G.nodes()}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        # Iterate over all unique neighbor pairs\n",
    "        for i, neighbor1 in enumerate(neighbors):\n",
    "            for neighbor2 in neighbors[i + 1:]:\n",
    "                # Find common neighbors of neighbor1 and neighbor2\n",
    "                shared_neighbors = set(G.neighbors(neighbor1)).intersection(G.neighbors(neighbor2))\n",
    "                # Add the count of shared neighbors to node's 4-cycle count\n",
    "                cycle_counts[node] += len(shared_neighbors)\n",
    "    \n",
    "    # Each 4-cycle is counted 4 times (once per node in the cycle)\n",
    "    cycle_counts = {node: count // 4 for node, count in cycle_counts.items()}\n",
    "    \n",
    "    return cycle_counts\n",
    "\n",
    "def graphlet_based_encoding_pyg(data):\n",
    "    \"\"\"\n",
    "    Add graphlet-based features (3-star, triangle, tailed triangle, 4-cycle) to node features in PyG.\n",
    "\n",
    "    Args:\n",
    "        data: PyG Data object.\n",
    "\n",
    "    Returns:\n",
    "        data: PyG Data object with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    # Convert PyG graph to NetworkX\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Count graphlets\n",
    "    triangle_counts = nx.triangles(G)  # Triangle counts\n",
    "    star_counts = count_3_star(G)  # 3-star graphlets\n",
    "    tail_counts = count_tailed_triangle(G)  # Tailed triangles\n",
    "    cycle_counts = count_4_cycle(G)  # 4-cycles\n",
    "\n",
    "    # Combine features into tensors\n",
    "    num_nodes = data.num_nodes\n",
    "    triangle_tensor = torch.tensor([triangle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    star_tensor = torch.tensor([star_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    tail_tensor = torch.tensor([tail_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    cycle_tensor = torch.tensor([cycle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Concatenate all graphlet features\n",
    "    graphlet_features = torch.cat([triangle_tensor, star_tensor, tail_tensor, cycle_tensor], dim=1)\n",
    "\n",
    "    # Add to node features\n",
    "    if data.x is not None:\n",
    "        data.x = torch.cat([data.x, graphlet_features], dim=1)\n",
    "    else:\n",
    "        data.x = graphlet_features\n",
    "\n",
    "    return data\n",
    "\n",
    "def graphlet_encoding_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Apply graphlet-based encoding to a PyG dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of PyG Data objects.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset: List of PyG Data objects with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for data in dataset:\n",
    "        data_copy = data.clone()  # Use PyG's clone method for deep copy\n",
    "        graph_encoded = graphlet_based_encoding_pyg(data_copy)\n",
    "        encoded_dataset.append(graph_encoded)\n",
    "    gle_dataset = CustomDataset(encoded_dataset)\n",
    "    return gle_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/8lnl08s13tj5jbb_fkzftzfw0000gn/T/ipykernel_8008/50909257.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data.edge_attr = torch.tensor(new_data.edge_attr, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "chemprop_original = copy.deepcopy(chemprop_dataset)\n",
    "chemprop_vn = apply_vn(chemprop_dataset)\n",
    "chemprop_deg = centrality(chemprop_dataset, centrality_measure='degree')\n",
    "chemprop_clo = centrality(chemprop_dataset, centrality_measure='closeness')\n",
    "chemprop_bet = centrality(chemprop_dataset, centrality_measure='betweenness')\n",
    "chemprop_eig = centrality(chemprop_dataset, centrality_measure='eigenvector')\n",
    "chemprop_de_n = distance_encoding(chemprop_dataset, method='node_augmentation')\n",
    "chemprop_de_g = distance_encoding(chemprop_dataset, method='edge_rewiring')\n",
    "chemprop_ge = graph_encoding(chemprop_dataset, k=3)\n",
    "chemprop_se = subgraph_extraction(chemprop_dataset, radius=3)\n",
    "chemprop_exN = extra_node(chemprop_dataset)\n",
    "chemprop_gle = graphlet_encoding_dataset(chemprop_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PNAConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg):\n",
    "        super(PNANet, self).__init__()\n",
    "        self.input_proj = Linear(num_node_features, dim_h)  # Project input features to dim_h\n",
    "        self.conv1 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.bn1 = BatchNorm1d(dim_h)\n",
    "        self.conv2 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.bn2 = BatchNorm1d(dim_h)\n",
    "        self.conv3 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.bn3 = BatchNorm1d(dim_h)\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = x.to(torch.float32)\n",
    "        edge_attr = edge_attr.to(torch.float32)\n",
    "\n",
    "        # Project input features\n",
    "        x_proj = self.input_proj(x)\n",
    "\n",
    "        h1 = F.relu(self.bn1(self.conv1(x_proj, edge_index, edge_attr)))\n",
    "        h1 = F.dropout(h1, p=0.3, training=self.training)\n",
    "        h2 = F.relu(self.bn2(self.conv2(h1 + x_proj, edge_index, edge_attr)))  # Residual connection\n",
    "        h2 = F.dropout(h2, p=0.3, training=self.training)\n",
    "        h3 = F.relu(self.bn3(self.conv3(h2 + h1, edge_index, edge_attr)))  # Residual connection\n",
    "        h3 = F.dropout(h3, p=0.3, training=self.training)\n",
    "\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = F.relu(self.lin1(h))\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        return self.lin2(h)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class GINENet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr):\n",
    "        super(GINENet, self).__init__()\n",
    "        \n",
    "        # Define GINE layers with the specified edge_dim\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        \n",
    "        # Define linear layers for classification or regression\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through GINE layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training for one epoch.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): Training data divided into batches.\n",
    "        model (nn.Module): GNN model to train on.\n",
    "        loss (nn.Module): Loss function to use during training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer during training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training loss (float), updated model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure node features are in float format\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        l = loss(out, data.y.view(-1, 1))\n",
    "        total_loss += l.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Return average loss for the epoch\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, model\n",
    "\n",
    "\n",
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation step.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): Validation set in batches.\n",
    "        model (nn.Module): Current trained model.\n",
    "        loss (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(data)\n",
    "\n",
    "            # Compute loss\n",
    "            l = loss(out, data.y.view(-1, 1))\n",
    "            total_loss += l.item()\n",
    "\n",
    "    # Return average validation loss\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing step.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): Test dataset in batches.\n",
    "        model (nn.Module): Trained model.\n",
    "\n",
    "    Returns:\n",
    "        float: Test loss.\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        l = loss(out, data.y.view(-1, 1))\n",
    "        total_loss += l.item()\n",
    "\n",
    "    # Return average test loss\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, model_path):\n",
    "    \"\"\"\n",
    "    Train the model for a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of epochs to train.\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader, optional): DataLoader for validation data (can be None).\n",
    "        model_path (str): Path to save the best model.\n",
    "\n",
    "    Returns:\n",
    "        best_val_loss (float): Lowest validation loss achieved.\n",
    "        train_losses (list of float): Training losses over epochs.\n",
    "        val_losses (list of float): Validation losses over epochs.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = torch.nn.L1Loss()  # Adjust criterion based on your task (e.g., regression or classification)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)  # Replace with appropriate criterion\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "        # Validation loop (if val_loader is provided)\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    out = model(batch)\n",
    "                    loss = criterion(out, batch.y)  # Replace with appropriate criterion\n",
    "                    val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            val_losses.append(val_loss / len(val_loader.dataset))\n",
    "\n",
    "            # Save model if it improves validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            # If no validation loader, save the model periodically\n",
    "            if epoch % 10 == 0:\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Print progress\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
    "        #       f\"Val Loss: {val_losses[-1]:.4f}\" if val_loader else f\"Train Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "    return best_val_loss, train_losses, val_losses if val_loader else train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))  # unpack the tuple into three separate lists\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k_splits = KFold(n_splits=5)\n",
    "k_train_indices, k_val_indices, k_test_indices = [], [], []\n",
    "for fold in k_splits.split(mols):\n",
    "    k_train_indices.append(fold[0])\n",
    "    k_val_indices.append([])\n",
    "    k_test_indices.append(fold[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "# Assuming precomputed indices from Chemprop\n",
    "# Example: These indices should be prepared using Chemprop's splitting logic\n",
    "# train_indices, val_indices, test_indices for single splits\n",
    "# k_train_indices, k_val_indices, k_test_indices for k-fold splits\n",
    "\n",
    "results_folder = \"./results/chemprop/lipo_cv/\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# List of datasets with their respective names and transformations\n",
    "datasets = [\n",
    "    ('Base', chemprop_original),\n",
    "    ('VN', chemprop_vn),\n",
    "    ('DEG', chemprop_deg),\n",
    "    ('BET', chemprop_bet),\n",
    "    ('CLO', chemprop_clo),\n",
    "    ('DE_N', chemprop_de_n),\n",
    "    ('DE_G', chemprop_de_g),\n",
    "    ('EXN', chemprop_exN),\n",
    "    ('SE', chemprop_se),\n",
    "    ('GE', chemprop_ge),\n",
    "    ('EIG', chemprop_eig),\n",
    "    ('GLE', chemprop_gle)\n",
    "]\n",
    "\n",
    "# Common parameters\n",
    "epochs = 60\n",
    "dim_h = 32\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to compute mean and standard deviation of results\n",
    "def compute_mean_std(results):\n",
    "    results = np.array(results)\n",
    "    mean = np.mean(results, axis=0)\n",
    "    std = np.std(results, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "# Loop through each dataset transformation\n",
    "for name, dataset in datasets:\n",
    "    print(f\"\\nProcessing dataset: {name}\")\n",
    "\n",
    "    # Normalize target\n",
    "    data_mean = dataset.data.y.mean()\n",
    "    data_std = dataset.data.y.std()\n",
    "    dataset.data.y = (dataset.data.y - data_mean) / data_std\n",
    "\n",
    "    # Use precomputed k-fold indices\n",
    "    pna_fold_losses = []\n",
    "    gin_fold_losses = []\n",
    "\n",
    "    for fold in range(len(k_train_indices)):  # Iterate over k-fold splits\n",
    "        print(f\"\\nFold {fold + 1}/{len(k_train_indices)}\")\n",
    "\n",
    "        # Get precomputed indices for this fold\n",
    "        train_idx = k_train_indices[fold]\n",
    "        val_idx = k_val_indices[fold] if k_val_indices else None\n",
    "        test_idx = k_test_indices[fold]\n",
    "\n",
    "        # Prepare PyG subsets\n",
    "        train_data = [dataset[i] for i in train_idx]\n",
    "        test_data = [dataset[i] for i in test_idx]\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Determine maximum degree and degree histogram for training set\n",
    "        max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in train_data]))\n",
    "        deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "        for data in train_data:\n",
    "            d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "            deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "        # Train and evaluate PNA model\n",
    "        model = PNANet(dataset[0].x.shape[1], dim_h, edge_attr=dataset[0].edge_attr.shape[1],\n",
    "                       aggregators=aggregators, scalers=scalers, deg=deg).to(device)\n",
    "        model_path = f\"{results_folder}/PNA_lipo_model_{name}_fold{fold + 1}.pt\"\n",
    "        pna_best_loss, _, _ = train_epochs(epochs, model, train_loader, None, model_path)\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        pna_test_loss = testing(test_loader, model)\n",
    "        pna_fold_losses.append(pna_test_loss)\n",
    "\n",
    "        # Train and evaluate GIN model\n",
    "        model = GINENet(dataset[0].x.shape[1], dim_h=dim_h, edge_attr=dataset[0].edge_attr.shape[1]).to(device)\n",
    "        model_path = f\"{results_folder}/GIN_lipo_model_{name}_fold{fold + 1}.pt\"\n",
    "        gin_best_loss, _, _ = train_epochs(epochs, model, train_loader, None, model_path)\n",
    "\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        gin_test_loss = testing(test_loader, model)\n",
    "        gin_fold_losses.append(gin_test_loss)\n",
    "\n",
    "    # Compute mean and standard deviation for PNA and GIN across folds\n",
    "    pna_mean_loss, pna_std_loss = compute_mean_std(pna_fold_losses)\n",
    "    gin_mean_loss, gin_std_loss = compute_mean_std(gin_fold_losses)\n",
    "\n",
    "    # Print summary for each dataset\n",
    "    print(f\"\\nResults for dataset: {name}\")\n",
    "    print(f\"  PNA Test Loss: {pna_mean_loss:.4f}  {pna_std_loss:.4f}\")\n",
    "    print(f\"  GIN Test Loss: {gin_mean_loss:.4f}  {gin_std_loss:.4f}\")\n",
    "\n",
    "    # Save results to file\n",
    "    with open(f\"{results_folder}/{name}_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Dataset: {name}\\n\")\n",
    "        f.write(f\"PNA Test Loss: {pna_mean_loss:.4f}  {pna_std_loss:.4f}\\n\")\n",
    "        f.write(f\"GIN Test Loss: {gin_mean_loss:.4f}  {gin_std_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN-thesis",
   "language": "python",
   "name": "gnn-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
