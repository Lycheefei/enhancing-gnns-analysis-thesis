{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc7e376-734a-40ef-9c73-f38cdd293443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd919fc-297f-4a8e-b0d5-e9a4e5b83530",
   "metadata": {},
   "source": [
    "# Prepare SR25 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb83b7ff-a443-40bc-8012-a37e0aa22057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.transforms import VirtualNode\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, global_add_pool, PNAConv\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix, degree\n",
    "from torch_geometric.data.data import Data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d918972-7174-4381-a9a4-1dbb69d2a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class SRDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(SRDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"sr251256.g6\"]  #sr251256  sr351668\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list. \n",
    "        b=self.processed_paths[0]   \n",
    "        dataset = nx.read_graph6(self.raw_paths[0])\n",
    "        data_list = []\n",
    "        for i,datum in enumerate(dataset):\n",
    "            x = torch.ones(datum.number_of_nodes(),1)\n",
    "            edge_index = to_undirected(torch.tensor(list(datum.edges())).transpose(1,0))            \n",
    "            data_list.append(Data(edge_index=edge_index, x=x, y=0))\n",
    "            \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a8e92f8-cc59-4700-924c-a950bb8c0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralDesign(object):   \n",
    "\n",
    "    def __init__(self,nmax=0,recfield=1,dv=5,nfreq=5,adddegree=False,laplacien=True,addadj=False,vmax=None):\n",
    "        # receptive field. 0: adj, 1; adj+I, n: n-hop area \n",
    "        self.recfield=recfield  \n",
    "        # b parameter\n",
    "        self.dv=dv\n",
    "        # number of sampled point of spectrum\n",
    "        self.nfreq=  nfreq\n",
    "        # if degree is added to node feature\n",
    "        self.adddegree=adddegree\n",
    "        # use laplacian or adjacency for spectrum\n",
    "        self.laplacien=laplacien\n",
    "        # add adjacecny as edge feature\n",
    "        self.addadj=addadj\n",
    "        # use given max eigenvalue\n",
    "        self.vmax=vmax\n",
    "\n",
    "        # max node for PPGN algorithm, set 0 if you do not use PPGN\n",
    "        self.nmax=nmax    \n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        n =data.x.shape[0]     \n",
    "        nf=data.x.shape[1]  \n",
    "\n",
    "\n",
    "        data.x=data.x.type(torch.float32)  \n",
    "               \n",
    "        nsup=self.nfreq+1\n",
    "        if self.addadj:\n",
    "            nsup+=1\n",
    "            \n",
    "        A=np.zeros((n,n),dtype=np.float32)\n",
    "        SP=np.zeros((nsup,n,n),dtype=np.float32) \n",
    "        A[data.edge_index[0],data.edge_index[1]]=1\n",
    "        \n",
    "        if self.adddegree:\n",
    "            data.x=torch.cat([data.x,torch.tensor(A.sum(0)).unsqueeze(-1)],1)\n",
    "\n",
    "        # calculate receptive field. 0: adj, 1; adj+I, n: n-hop area\n",
    "        if self.recfield==0:\n",
    "            M=A\n",
    "        else:\n",
    "            M=(A+np.eye(n))\n",
    "            for i in range(1,self.recfield):\n",
    "                M=M.dot(M) \n",
    "\n",
    "        M=(M>0)\n",
    "\n",
    "        \n",
    "        d = A.sum(axis=0) \n",
    "        # normalized Laplacian matrix.\n",
    "        dis=1/np.sqrt(d)\n",
    "        dis[np.isinf(dis)]=0\n",
    "        dis[np.isnan(dis)]=0\n",
    "        D=np.diag(dis)\n",
    "        nL=np.eye(D.shape[0])-(A.dot(D)).T.dot(D)\n",
    "        V,U = np.linalg.eigh(nL) \n",
    "        V[V<0]=0\n",
    "        # keep maximum eigenvalue for Chebnet if it is needed\n",
    "        data.lmax=V.max().astype(np.float32)\n",
    "\n",
    "        if not self.laplacien:        \n",
    "            V,U = np.linalg.eigh(A)\n",
    "\n",
    "        # design convolution supports\n",
    "        vmax=self.vmax\n",
    "        if vmax is None:\n",
    "            vmax=V.max()\n",
    "\n",
    "        freqcenter=np.linspace(V.min(),vmax,self.nfreq)\n",
    "        \n",
    "        # design convolution supports (aka edge features)         \n",
    "        for i in range(0,len(freqcenter)): \n",
    "            SP[i,:,:]=M* (U.dot(np.diag(np.exp(-(self.dv*(V-freqcenter[i])**2))).dot(U.T))) \n",
    "        # add identity\n",
    "        SP[len(freqcenter),:,:]=np.eye(n)\n",
    "        # add adjacency if it is desired\n",
    "        if self.addadj:\n",
    "            SP[len(freqcenter)+1,:,:]=A\n",
    "           \n",
    "        # set convolution support weigths as an edge feature\n",
    "        E=np.where(M>0)\n",
    "        data.edge_index2=torch.Tensor(np.vstack((E[0],E[1]))).type(torch.int64)\n",
    "        data.edge_attr2 = torch.Tensor(SP[:,E[0],E[1]].T).type(torch.float32)  \n",
    "\n",
    "        # set tensor for Maron's PPGN         \n",
    "        if self.nmax>0:       \n",
    "            H=torch.zeros(1,nf+2,self.nmax,self.nmax)\n",
    "            H[0,0,data.edge_index[0],data.edge_index[1]]=1 \n",
    "            H[0,1,0:n,0:n]=torch.diag(torch.ones(data.x.shape[0]))\n",
    "            for j in range(0,nf):      \n",
    "                H[0,j+2,0:n,0:n]=torch.diag(data.x[:,j])\n",
    "            data.X2= H \n",
    "            M=torch.zeros(1,2,self.nmax,self.nmax)\n",
    "            for i in range(0,n):\n",
    "                M[0,0,i,i]=1\n",
    "            M[0,1,0:n,0:n]=1-M[0,0,0:n,0:n]\n",
    "            data.M= M        \n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e462527-94ec-45ad-acf3-ff27ec114c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2176769/3868519769.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    }
   ],
   "source": [
    "transform = SpectralDesign(nmax=25,recfield=1,dv=2,nfreq=5,adddegree=True)\n",
    "SR25_dataset = SRDataset(root=\"../dataset/sr25/\", pre_transform=transform)\n",
    "\n",
    "SR25_dataset.data.y = torch.arange(len(dataset.data.y)).long() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731843f-676a-4439-84cc-164d858bced4",
   "metadata": {},
   "source": [
    "# Add Isomorphic Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05c4b4fd-8dfb-44b2-9a03-c52cbe56a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_isomorphic_pairs_dgl(dataset, num_pairs=5):\n",
    "    isomorphic_pair = []\n",
    "    original_indices = []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        # Pick a random graph from the dataset\n",
    "        original_graph = random.choice(dataset)\n",
    "        original_idx = dataset.index(original_graph)\n",
    "\n",
    "        # Convert to NetworkX and create isomorphic graphs\n",
    "        G = dgl.to_networkx(original_graph)\n",
    "        nodes = list(G.nodes())\n",
    "        random.shuffle(nodes)  # Shuffle to get isomorphic graph\n",
    "        mapping = {node: nodes[i] for i, node in enumerate(nodes)}\n",
    "        isomorphic_G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        # Convert back to DGL\n",
    "        isomorphic_dgl_graph = dgl.from_networkx(isomorphic_G)\n",
    "        isomorphic_dgl_graph.ndata['x'] = original_graph.ndata['x']  # Copy node features\n",
    "\n",
    "        isomorphic_pair.append(isomorphic_dgl_graph)\n",
    "        original_indices.append(original_idx)\n",
    "\n",
    "    return dataset, isomorphic_pair, original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778e01d-b7eb-46a7-b1c9-2e5f9414c968",
   "metadata": {},
   "source": [
    "# Graph Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44e05d54-affd-4d8f-b4b8-f8d8daf1061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VN\n",
    "transform = VirtualNode()\n",
    "def apply_vn(dgl_graphs):\n",
    "  vn_EXP_dgl = []\n",
    "  for graph in dgl_graphs:\n",
    "    graph_pyg = from_dgl(graph)\n",
    "    graph_pyg_copy = copy.deepcopy(graph_pyg)\n",
    "    graph_vn = transform(graph_pyg_copy)\n",
    "    graph_vn_dgl = to_dgl(graph_vn)\n",
    "    vn_EXP_dgl.append(graph_vn_dgl)\n",
    "\n",
    "  return vn_EXP_dgl\n",
    "\n",
    "# Centrality\n",
    "def add_centrality_to_node_features(dgl_graph, centrality_measure='degree'):\n",
    "    # Convert DGL data to NetworkX graph\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'betweenness':\n",
    "        centrality = nx.betweenness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "\n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = np.array([centrality[node] for node in range(dgl_graph.number_of_nodes())], dtype=np.float32).reshape(-1, 1)\n",
    "    centrality_values = torch.round(torch.tensor(centrality_values) * 10000) / 10000\n",
    "    # Concatenate the centrality with existing node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], centrality_values], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = centrality_values\n",
    "    return dgl_graph\n",
    "\n",
    "# Degree\n",
    "def degree_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_degree = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)  # Create a deep copy of the graph\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='degree')\n",
    "        Graph_data_degree.append(data_copy)\n",
    "    return Graph_data_degree\n",
    "\n",
    "# Closeness\n",
    "def closeness_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_clo = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='closeness')\n",
    "        Graph_data_clo.append(data_copy)\n",
    "    return Graph_data_clo\n",
    "\n",
    "#Betweenness\n",
    "def betweenness_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_bet = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='betweenness')\n",
    "        Graph_data_bet.append(data_copy)\n",
    "    return Graph_data_bet\n",
    "\n",
    "# Eigenvector\n",
    "def eigenvector_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_eig = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='eigenvector')\n",
    "        Graph_data_eig.append(data_copy)\n",
    "    return Graph_data_eig\n",
    "\n",
    "# DE\n",
    "def add_distance_encoding(dgl_graph):\n",
    "    # Compute the shortest distance matrix using dgl.shortest_dist\n",
    "    dist = dgl.shortest_dist(dgl_graph).float()  # Convert to float to handle inf\n",
    "\n",
    "    # Replace -1 with inf (to handle unreachable nodes similar to NetworkX's behavior)\n",
    "    dist[dist == -1] = float('inf')\n",
    "\n",
    "    # Calculate the average shortest distance for each node\n",
    "    finite_distances = torch.where(dist == float('inf'), torch.tensor(float('nan')), dist)\n",
    "    average_distance = torch.nanmean(finite_distances, dim=1).view(-1, 1)  # Use nanmean to ignore infinities\n",
    "\n",
    "    # Add the average distance to the existing node features in the DGL graph\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], average_distance], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = average_distance\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def distance_encoding(dataset):\n",
    "    Graph_data_DE = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_distance_encoding(data_copy)\n",
    "        Graph_data_DE.append(data_copy)\n",
    "    return Graph_data_DE\n",
    "\n",
    "# GE\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "def canonicalize_eigenvectors(eigenvectors):\n",
    "    \"\"\"Canonicalize eigenvectors by fixing their signs for consistency.\"\"\"\n",
    "    for i in range(eigenvectors.shape[1]):\n",
    "        if eigenvectors[0, i] < 0:  # Flip sign if the first element is negative\n",
    "            eigenvectors[:, i] = -eigenvectors[:, i]\n",
    "    return eigenvectors\n",
    "\n",
    "def add_canonicalized_laplacian_pe(dgl_graph, k=5):\n",
    "    \"\"\"\n",
    "    Add canonicalized Laplacian positional encoding to a DGL graph.\n",
    "\n",
    "    Args:\n",
    "        dgl_graph: Input DGL graph.\n",
    "        k: Number of Laplacian eigenvectors to compute.\n",
    "\n",
    "    Returns:\n",
    "        dgl_graph: DGL graph with Laplacian PE appended to node features.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert DGL graph to adjacency matrix\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "    adj = nx.to_numpy_array(G)\n",
    "\n",
    "    # Step 2: Compute Laplacian matrix\n",
    "    degree_matrix = np.diag(np.sum(adj, axis=1))\n",
    "    laplacian = degree_matrix - adj\n",
    "\n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "\n",
    "    # Step 4: Select the smallest k eigenvectors (sorted by eigenvalues)\n",
    "    idx = np.argsort(eigenvalues)[:k]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Step 5: Canonicalize eigenvectors\n",
    "    eigenvectors = canonicalize_eigenvectors(torch.tensor(eigenvectors, dtype=torch.float))\n",
    "\n",
    "    # Step 6: Add the eigenvectors as new node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], eigenvectors], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = eigenvectors\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def Graph_encoding(dataset, k=3):\n",
    "    \"\"\"\n",
    "    Apply canonicalized Laplacian positional encoding to a list of DGL graphs.\n",
    "\n",
    "    Args:\n",
    "        dgl_graphs: List of DGL graphs.\n",
    "        k: Number of Laplacian eigenvectors to compute.\n",
    "\n",
    "    Returns:\n",
    "        GE_EXP_dgl: List of DGL graphs with Laplacian PE added.\n",
    "    \"\"\"\n",
    "    GE_EXP_dgl = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        graph_pe = add_canonicalized_laplacian_pe(data_copy, k=k)\n",
    "        GE_EXP_dgl.append(graph_pe)\n",
    "    return GE_EXP_dgl\n",
    "\n",
    "# Sub\n",
    "def extract_local_subgraph_features(dgl_graph, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Initialize a list to store subgraph features for each node\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        # Extract the ego graph (subgraph) around the node\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "\n",
    "        # Example feature 1: Size of the subgraph (number of nodes)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "\n",
    "        # Example feature 2: Average degree of the subgraph\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "\n",
    "    # Convert the features to tensors and add them as node features\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], subgraph_sizes_tensor, subgraph_degrees_tensor], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = torch.cat([subgraph_sizes_tensor, subgraph_degrees_tensor], dim=1)\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def subgraph_dataset(dataset, radius=3):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_sub = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = extract_local_subgraph_features(data_copy, radius=radius)\n",
    "        Graph_data_sub.append(data_copy)\n",
    "    return Graph_data_sub\n",
    "\n",
    "# ExN\n",
    "def add_extra_node_on_each_edge(dgl_graph):\n",
    "    # Collect new edges (source, destination) and the new node features\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_node_features = []\n",
    "\n",
    "    # Original number of nodes\n",
    "    num_original_nodes = dgl_graph.num_nodes()\n",
    "\n",
    "    # Use a set to track edges we have already processed (to avoid duplicates)\n",
    "    processed_edges = set()\n",
    "\n",
    "    # Iterate over all edges\n",
    "    for i in range(dgl_graph.num_edges()):\n",
    "        u, v = dgl_graph.edges()[0][i].item(), dgl_graph.edges()[1][i].item()\n",
    "\n",
    "        # Avoid processing reverse edges (v, u) if (u, v) is already processed\n",
    "        if (u, v) in processed_edges or (v, u) in processed_edges:\n",
    "            continue\n",
    "\n",
    "        # Mark the edge as processed\n",
    "        processed_edges.add((u, v))\n",
    "        processed_edges.add((v, u))  # In case there is a reverse edge\n",
    "\n",
    "        # Add a new node\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        mean_feature = (dgl_graph.ndata['x'][u] + dgl_graph.ndata['x'][v]) / 2\n",
    "        new_node_features.append(mean_feature)\n",
    "\n",
    "        # Add new edges connecting the new node to the original nodes\n",
    "        new_edges_src.append(u)\n",
    "        new_edges_dst.append(new_node_id)\n",
    "\n",
    "        new_edges_src.append(new_node_id)\n",
    "        new_edges_dst.append(v)\n",
    "\n",
    "    # Add new nodes to the DGL graph\n",
    "    dgl_graph.add_nodes(len(new_node_features), {'x': torch.stack(new_node_features)})\n",
    "\n",
    "    # Remove the original edges\n",
    "    dgl_graph.remove_edges(torch.arange(dgl_graph.num_edges()))\n",
    "\n",
    "    # Add new edges to the DGL graph\n",
    "    dgl_graph.add_edges(new_edges_src, new_edges_dst)\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def extra_node_dataset(dataset):\n",
    "    Graph_data_exN = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        dgl_graph = add_extra_node_on_each_edge(data_copy)\n",
    "        Graph_data_exN.append(dgl_graph)\n",
    "    return Graph_data_exN\n",
    "\n",
    "def count_3_star(G):\n",
    "    \"\"\"Count 3-star graphlets for each node.\"\"\"\n",
    "    # A 3-star is a node with at least three neighbors\n",
    "    star_counts = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degree = len(neighbors)\n",
    "        # Count the number of 3-combinations of neighbors\n",
    "        star_counts[node] = max(0, (degree * (degree - 1) * (degree - 2)) // 6)\n",
    "    return star_counts\n",
    "\n",
    "def count_tailed_triangle(G):\n",
    "    \"\"\"Count tailed triangle graphlets for each node.\"\"\"\n",
    "    tail_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for neighbor in neighbors:\n",
    "            # For each pair of neighbors, check if there's a triangle\n",
    "            for other in neighbors:\n",
    "                if neighbor != other and G.has_edge(neighbor, other):\n",
    "                    # Found a triangle, check for a tail\n",
    "                    for extra in G.neighbors(node):\n",
    "                        if extra not in {neighbor, other}:\n",
    "                            tail_counts[node] += 1\n",
    "    return tail_counts\n",
    "\n",
    "def count_4_cycle(G):\n",
    "    \"\"\"Count 4-cycle graphlets for each node.\"\"\"\n",
    "    cycle_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for i, neighbor1 in enumerate(neighbors):\n",
    "            for neighbor2 in neighbors[i + 1:]:\n",
    "                # Check for shared neighbors forming a 4-cycle\n",
    "                for shared_neighbor in G.neighbors(neighbor1):\n",
    "                    if shared_neighbor in G.neighbors(neighbor2):\n",
    "                        cycle_counts[node] += 1\n",
    "    return cycle_counts\n",
    "\n",
    "def graphlet_based_encoding(dgl_graph):\n",
    "    \"\"\"\n",
    "    Add graphlet-based features (3-star, triangle, tailed triangle, 4-cycle) to node features.\n",
    "\n",
    "    Args:\n",
    "        dgl_graph: Input DGL graph.\n",
    "\n",
    "    Returns:\n",
    "        dgl_graph: DGL graph with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    # Convert DGL graph to NetworkX\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Count graphlets\n",
    "    triangle_counts = nx.triangles(G)  # Triangle counts\n",
    "    star_counts = count_3_star(G)  # 3-star graphlets\n",
    "    tail_counts = count_tailed_triangle(G)  # Tailed triangles\n",
    "    cycle_counts = count_4_cycle(G)  # 4-cycles\n",
    "\n",
    "    # Combine features into tensors\n",
    "    num_nodes = dgl_graph.num_nodes()\n",
    "    triangle_tensor = torch.tensor([triangle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    star_tensor = torch.tensor([star_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    tail_tensor = torch.tensor([tail_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    cycle_tensor = torch.tensor([cycle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Concatenate all graphlet features\n",
    "    graphlet_features = torch.cat([triangle_tensor, star_tensor, tail_tensor, cycle_tensor], dim=1)\n",
    "\n",
    "    # Add to node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], graphlet_features], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = graphlet_features\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def graphlet_encoding_dataset(dgl_dataset):\n",
    "    \"\"\"\n",
    "    Apply graphlet-based encoding to a list of DGL graphs.\n",
    "\n",
    "    Args:\n",
    "        dgl_dataset: List of DGL graphs.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset: List of DGL graphs with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for dgl_graph in dgl_dataset:\n",
    "        graph_copy = copy.deepcopy(dgl_graph)\n",
    "        graph_encoded = graphlet_based_encoding(graph_copy)\n",
    "        encoded_dataset.append(graph_encoded)\n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fe227-9752-4060-8c60-915cc2e751b1",
   "metadata": {},
   "source": [
    "## SR25 original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e14304a-1db5-40ad-9c1e-e9317e1f8468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/SR25_dataset.pkl.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "SR25_dgl_graphs = []\n",
    "for data in SR25_dataset:\n",
    "    dgl_graph = to_dgl(data)\n",
    "    SR25_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/SR25/SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0aadef-5466-4065-a55a-1f385e1d6d4b",
   "metadata": {},
   "source": [
    "## SR25 dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "569351a8-a71c-4fdc-99a8-8cf7910db3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25, SR25_iso, SR25_indices = add_isomorphic_pairs_dgl(SR25_dgl_graphs, num_pairs=15)\n",
    "\n",
    "SR25_dummy_dgl = SR25 + SR25_iso\n",
    "\n",
    "output_file = '../data/SR25/SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6bec6-b8b7-4057-b9d0-0e95310de8ab",
   "metadata": {},
   "source": [
    "## VN on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebbb7087-bd8e-43fc-9fc3-8f47b596ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/vn_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/vn_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "vn_SR25_dgl = apply_vn(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/vn_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_SR25_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_SR25_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "vn_SR25_dgl_dummy = apply_vn(SR25_dummy_dgl)\n",
    "output_file = '../data/SR25/vn_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_SR25_dgl_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(vn_SR25_dgl_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cf120-8230-420e-bdd8-39a65935bdac",
   "metadata": {},
   "source": [
    "## Degree Centrality on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e28fa65a-7249-417b-b980-6ad72436e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/deg_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/deg_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_deg = degree_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/deg_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_deg, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_deg)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_deg_dummy = degree_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/deg_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_deg_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_deg_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5ebc8-f5b1-4637-b48b-e562f2abf224",
   "metadata": {},
   "source": [
    "## Closeness Centrality on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3fa7c89-52f7-4225-b551-b190d26c97f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/clo_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/clo_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_clo = closeness_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/clo_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_clo, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_clo)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_clo_dummy = closeness_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/clo_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_clo_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_clo_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14ab83-2741-4473-97de-961c6530a6af",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3f8fb3b-2645-417f-b4dd-e8aff7eec2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/bet_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/bet_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_bet = betweenness_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/bet_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_bet, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_bet)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_bet_dummy = betweenness_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/bet_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_bet_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_bet_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827809d-596b-413e-a8db-c86cfe2fb528",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18498b2c-d970-42ba-9828-b4bf09e8ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/eig_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/eig_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_eig = eigenvector_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/eig_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_eig, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_eig)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SR25_eig_dummy = eigenvector_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/eig_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_eig_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_eig_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e949fc8-13a9-4ac9-809b-5e914e787679",
   "metadata": {},
   "source": [
    "## Distance Encoding on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca73a97b-d33d-4367-bbde-4ce65dd5b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/DE_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/DE_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_DE = distance_encoding(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/DE_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_DE, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_DE)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_DE_dummy = distance_encoding(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/DE_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_DE_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_DE_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01c4de-9d00-4481-8210-f349fb2f1329",
   "metadata": {},
   "source": [
    "## Graph Encoding on SR25 original and SR25 dummy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5470015e-6009-4b37-959c-704b9e6a817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/GE_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/GE_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_GE = Graph_encoding(SR25_dgl_graphs, k=3)\n",
    "\n",
    "output_file = '../data/SR25/GE_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_GE, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_GE)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_GE_dummy = Graph_encoding(SR25_dummy_dgl, k=3)\n",
    "\n",
    "output_file = '../data/SR25/GE_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_GE_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_GE_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d1807-8b62-4007-8f2f-49f6e451703d",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64c7be4b-17e0-4ce2-b215-69db9c7fbdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/sub_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/sub_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_sub = subgraph_dataset(SR25_dgl_graphs, radius=3)\n",
    "\n",
    "output_file = '../data/SR25/sub_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_sub, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_sub)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_sub_dummy = subgraph_dataset(SR25_dummy_dgl, radius=3)\n",
    "\n",
    "output_file = '../data/SR25/sub_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_sub_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_sub_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca48af-1e06-4821-b043-73e9218bb105",
   "metadata": {},
   "source": [
    "## Extra Node on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97ead2cd-63b0-4412-a0c2-387ec56a8276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/exN_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/exN_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_exN = extra_node_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/exN_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_exN, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_exN)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "\n",
    "SR25_exN_dummy = extra_node_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/exN_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_exN_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_exN_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013704d-dba6-4eab-9e28-5a2084c21f75",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on SR25 original and SR25 dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f25c9d7-5f81-44a2-b1dc-fd260bfdcdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15 graphs to DGL format and saved to ../data/SR25/GLE_SR25_dataset.pkl.\n",
      "Converted 30 graphs (including isomorphisms) to DGL format and saved to ../data/SR25/GLE_SR25_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SR25_GLE = graphlet_encoding_dataset(SR25_dgl_graphs)\n",
    "\n",
    "output_file = '../data/SR25/GLE_SR25_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_GLE, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_GLE)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SR25_GLE_dummy = graphlet_encoding_dataset(SR25_dummy_dgl)\n",
    "\n",
    "output_file = '../data/SR25/GLE_SR25_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SR25_GLE_dummy, f)\n",
    "\n",
    "print(f\"Converted {len(SR25_GLE_dummy)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf8fbb-92a4-4ce8-a596-7e0229f5c7b3",
   "metadata": {},
   "source": [
    "# Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "980dd7fb-7af1-4820-b34c-8e42e99d925c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/SR25/DE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/exN_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {90, 60}\n",
      "Number of unique embeddings with GIN: 2\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {0}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/sub_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/deg_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/GE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1440, 930, 1890, 870, 1290, 1260, 2190, 1710, 1200, 720, 1650, 1170, 2040, 540, 990}\n",
      "Number of unique embeddings with GIN: 15\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2340, 2310, 2280, 2250, 2220, 2190, 2130}\n",
      "Number of unique embeddings with PNA: 7\n",
      "\n",
      "------------- Processing ../data/SR25/vn_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {780}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1950}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/clo_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/GLE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/eig_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/bet_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 1000) / 1000\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "            #h = torch.round(h * 1000) / 1000\n",
    "            h = h/torch.sum(h) * 10\n",
    "        #g_embedding = self.pool(g, h)\n",
    "        return h\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 1000) / 1000\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "            #h = torch.round(h * 1000) / 1000\n",
    "            h = h/torch.sum(h) * 10\n",
    "        #g_embedding = self.pool(g, h)\n",
    "        return h\n",
    "\n",
    "# Define a DeepSet model\n",
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.input_dim = input_dim  # Store input dimension\n",
    "        # First neural network to map node features to node embeddings\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Second neural network to map summed node embeddings to final graph embedding\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # Output a single value\n",
    "        )\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply the first neural network to each node feature\n",
    "        h = self.phi(h)\n",
    "        h = torch.round(h * 10000) / 10000  # Round to reduce precision\n",
    "        # Sum the node embeddings to create a graph embedding\n",
    "        g_embedding = h.sum(dim=0, keepdim=True)\n",
    "        # Apply the second neural network to the summed embedding\n",
    "        output = self.rho(g_embedding)*100\n",
    "        return output\n",
    "\n",
    "\n",
    "def calculate_integer_embedding(embedding):\n",
    "    # Convert embedding to a flattened list of integers\n",
    "    embedding = embedding.numpy()\n",
    "    embedding = np.nan_to_num(embedding, nan=0.0) * 10\n",
    "    embedding = embedding.astype(int)\n",
    "    if embedding.ndim == 1:\n",
    "        embedding = embedding.reshape(1, -1)  # Convert 1D array to a 2D array with one row\n",
    "\n",
    "    column_sum = embedding.sum(axis=0)\n",
    "    column_mean = embedding.mean(axis=0)\n",
    "    column_min = embedding.min(axis=0)\n",
    "\n",
    "    # Concatenate the results into a single 1D array\n",
    "    final_embedding = np.concatenate((column_sum, column_mean, column_min))\n",
    "\n",
    "    return int(np.sum((final_embedding.flatten()))*10)\n",
    "\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "\n",
    "    for epoch_in in range(10):\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "            target = torch.randn_like(output)*100  # Target is a random tensor of the same shape as output\n",
    "\n",
    "            loss = F.mse_loss(output, target.float(), reduction='mean')  # Ensure the target is a float tensor\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    embeddings = set()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            #print(\"embedding: \", embedding)\n",
    "            final_embedding = calculate_integer_embedding(embedding.flatten())\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/SR25/*SR25_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973f011-87c4-436a-bfaf-772cba84aad6",
   "metadata": {},
   "source": [
    "# Organize Graphs in Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe8950be-d876-435d-9d7d-a641623adafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def organize_pairs_sr25(dummy_dataset, original_size, original_indices):\n",
    "    \"\"\"\n",
    "    Organize non-isomorphic and isomorphic pairs for SR25.\n",
    "\n",
    "    Parameters:\n",
    "    - dummy_dataset: Dataset containing both original and isomorphic graphs.\n",
    "    - original_size: Number of original SR25 graphs.\n",
    "    - original_indices: Indices of original graphs that generated isomorphic pairs.\n",
    "\n",
    "    Returns:\n",
    "    - non_isomorphic_pairs: List of all unique non-isomorphic pairs.\n",
    "    - isomorphic_pairs: List of all isomorphic pairs.\n",
    "    \"\"\"\n",
    "    non_isomorphic_pairs = []\n",
    "    isomorphic_pairs = []\n",
    "\n",
    "    # Generate all unique non-isomorphic pairs from the original graphs\n",
    "    for i, j in combinations(range(original_size), 2):\n",
    "        non_isomorphic_pairs.append((dummy_dataset[i], dummy_dataset[j]))\n",
    "\n",
    "    # Generate isomorphic pairs using the indices\n",
    "    for i, original_idx in enumerate(original_indices):\n",
    "        isomorphic_pairs.append((dummy_dataset[original_idx], dummy_dataset[original_size + i]))\n",
    "\n",
    "    return non_isomorphic_pairs, isomorphic_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf21529-8ae0-4c4f-8b1b-a560a154c788",
   "metadata": {},
   "source": [
    "# Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35d67003-8cfe-4db1-b5c3-3249c76dfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss2(embedding1, embedding2, label, margin=1.0):\n",
    "    \"\"\"Optimized contrastive loss: Pull embeddings together if label == 1, push them apart if label == 0.\"\"\"\n",
    "    euclidean_distance = F.pairwise_distance(embedding1, embedding2)\n",
    "    euclidean_distance_squared = torch.pow(euclidean_distance, 2)\n",
    "    \n",
    "    # Ensure that label is a tensor, convert to float tensor\n",
    "    if isinstance(label, int):  # Check if label is a scalar integer\n",
    "        label = torch.tensor(label).float().to(embedding1.device)\n",
    "    else:\n",
    "        label = label.float()\n",
    "\n",
    "    # Compute positive and negative losses\n",
    "    loss_positive = euclidean_distance_squared  # For label == 1\n",
    "    loss_negative = torch.pow(F.relu(margin - euclidean_distance), 2)  # For label == 0\n",
    "    \n",
    "    # Use torch.where with tensor inputs\n",
    "    loss = torch.where(label == 1, loss_positive, loss_negative)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, model, input_dim):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    import dgl\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Custom collate function to handle DGLGraphs\n",
    "    def collate_fn(batch):\n",
    "        # Unpack the batch of pairs\n",
    "        g1_batch, g2_batch = zip(*batch)\n",
    "    \n",
    "        # Batch the graphs using DGL's batch function\n",
    "        batched_g1 = dgl.batch(g1_batch)\n",
    "        batched_g2 = dgl.batch(g2_batch)\n",
    "        \n",
    "        return batched_g1, batched_g2\n",
    "    \n",
    "    # Custom Dataset for graph pairs (as you already have)\n",
    "    class GraphPairDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, pairs):\n",
    "            self.pairs = pairs\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.pairs)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            return self.pairs[idx]\n",
    "    \n",
    "    # Assuming you already have non_isomorphic_pairs and isomorphic_pairs\n",
    "    non_isomorphic_dataset = GraphPairDataset(non_isomorphic_pairs)\n",
    "    isomorphic_dataset = GraphPairDataset(isomorphic_pairs)\n",
    "    \n",
    "    # Define DataLoader with custom collate_fn\n",
    "    non_isomorphic_loader = DataLoader(non_isomorphic_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    isomorphic_loader = DataLoader(isomorphic_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "    \n",
    "        # Iterate over non-isomorphic pairs in batches\n",
    "        for batched_g1, batched_g2 in non_isomorphic_loader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            batched_g1 = batched_g1.to(device)\n",
    "            batched_g2 = batched_g2.to(device)\n",
    "    \n",
    "            embedding1 = model(batched_g1, batched_g1.ndata['x'])\n",
    "            embedding2 = model(batched_g2, batched_g2.ndata['x'])\n",
    "    \n",
    "            # Label is 0 for non-isomorphic pairs\n",
    "            loss = contrastive_loss2(embedding1, embedding2, label=torch.zeros(batched_g1.batch_size).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        # Iterate over isomorphic pairs in batches\n",
    "        for batched_g1, batched_g2 in isomorphic_loader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            batched_g1 = batched_g1.to(device)\n",
    "            batched_g2 = batched_g2.to(device)\n",
    "    \n",
    "            embedding1 = model(batched_g1, batched_g1.ndata['x'])\n",
    "            embedding2 = model(batched_g2, batched_g2.ndata['x'])\n",
    "    \n",
    "            # Label is 1 for isomorphic pairs\n",
    "            loss = contrastive_loss2(embedding1, embedding2, label=torch.ones(batched_g1.batch_size).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{100}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    non_isomorphic_different_count = 0\n",
    "    isomorphic_same_count = 0\n",
    "    isomorphic_different_count = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        # Compare embeddings of non-isomorphic pairs\n",
    "        for g1, g2 in non_isomorphic_pairs:\n",
    "            g1 = g1.to(device)\n",
    "            g2 = g2.to(device)\n",
    "            embedding1 = model(g1, g1.ndata['x']).to(device)\n",
    "            embedding2 = model(g2, g2.ndata['x']).to(device)\n",
    "            cosine_sim = F.cosine_similarity(embedding1, embedding2).item()\n",
    "            if cosine_sim < 0.99:\n",
    "                non_isomorphic_different_count += 1  # Correctly classified as different\n",
    "            #final_embedding1 = calculate_integer_embedding(embedding1)\n",
    "            #final_embedding2 = calculate_integer_embedding(embedding2)\n",
    "            #if final_embedding1 != final_embedding2:\n",
    "            #    non_isomorphic_different_count += 1\n",
    "                \n",
    "\n",
    "        # Compare embeddings of isomorphic pairs\n",
    "        for g1, g2 in isomorphic_pairs:\n",
    "            g1 = g1.to(device)\n",
    "            g2 = g2.to(device)\n",
    "            embedding1 = model(g1, g1.ndata['x']).to(device)\n",
    "            embedding2 = model(g2, g2.ndata['x']).to(device)\n",
    "            cosine_sim = F.cosine_similarity(embedding1, embedding2).item()\n",
    "            if cosine_sim > 0.99:\n",
    "                isomorphic_same_count += 1  # Correctly classified as the same\n",
    "            else:\n",
    "                isomorphic_different_count += 1  # Incorrectly classified as different\n",
    "\n",
    "            #final_embedding1 = calculate_integer_embedding(embedding1)\n",
    "            #final_embedding2 = calculate_integer_embedding(embedding2)\n",
    "\n",
    "            #if final_embedding1 == final_embedding2:\n",
    "            #   isomorphic_same_count += 1\n",
    "            #else:\n",
    "            #    isomorphic_different_count += 1\n",
    "\n",
    "    print(f\"Correctly classified non-isomorphic pairs: {non_isomorphic_different_count}\")\n",
    "    print(f\"Correctly classified isomorphic pairs: {isomorphic_same_count}\")\n",
    "    print(f\"Incorrectly classified isomorphic pairs: {isomorphic_different_count}\")\n",
    "\n",
    "    return non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b3a71e2d-b5db-4ca9-b784-9d8adc301bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/SR25/DE_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/exN_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.0070\n",
      "Epoch 2/100, Loss: 0.0000\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0001\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0004\n",
      "Epoch 13/100, Loss: 0.0015\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0005\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0008\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0002\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0002\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0007\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0044\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0002\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0024\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0000\n",
      "Epoch 2/100, Loss: 0.0000\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0000\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0000\n",
      "Epoch 13/100, Loss: 0.0000\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 43\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/sub_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/deg_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/GE_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.0000\n",
      "Epoch 2/100, Loss: 0.0000\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0000\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0000\n",
      "Epoch 13/100, Loss: 0.0000\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 103\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0000\n",
      "Epoch 2/100, Loss: 0.0000\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0000\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0000\n",
      "Epoch 13/100, Loss: 0.0000\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 105\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/vn_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.9764\n",
      "Epoch 2/100, Loss: 1.9979\n",
      "Epoch 3/100, Loss: 1.9982\n",
      "Epoch 4/100, Loss: 1.9982\n",
      "Epoch 5/100, Loss: 1.9978\n",
      "Epoch 6/100, Loss: 1.9981\n",
      "Epoch 7/100, Loss: 1.9981\n",
      "Epoch 8/100, Loss: 1.9981\n",
      "Epoch 9/100, Loss: 1.9977\n",
      "Epoch 10/100, Loss: 1.9981\n",
      "Epoch 11/100, Loss: 1.9973\n",
      "Epoch 12/100, Loss: 1.9976\n",
      "Epoch 13/100, Loss: 1.9976\n",
      "Epoch 14/100, Loss: 1.9980\n",
      "Epoch 15/100, Loss: 1.9979\n",
      "Epoch 16/100, Loss: 1.9978\n",
      "Epoch 17/100, Loss: 1.9972\n",
      "Epoch 18/100, Loss: 1.9969\n",
      "Epoch 19/100, Loss: 1.9972\n",
      "Epoch 20/100, Loss: 1.9962\n",
      "Epoch 21/100, Loss: 1.9962\n",
      "Epoch 22/100, Loss: 1.9943\n",
      "Epoch 23/100, Loss: 1.9969\n",
      "Epoch 24/100, Loss: 1.9961\n",
      "Epoch 25/100, Loss: 1.9966\n",
      "Epoch 26/100, Loss: 1.9960\n",
      "Epoch 27/100, Loss: 1.9929\n",
      "Epoch 28/100, Loss: 1.9983\n",
      "Epoch 29/100, Loss: 1.9985\n",
      "Epoch 30/100, Loss: 1.9983\n",
      "Epoch 31/100, Loss: 1.9981\n",
      "Epoch 32/100, Loss: 1.9982\n",
      "Epoch 33/100, Loss: 1.9980\n",
      "Epoch 34/100, Loss: 1.9977\n",
      "Epoch 35/100, Loss: 1.9965\n",
      "Epoch 36/100, Loss: 1.9914\n",
      "Epoch 37/100, Loss: 1.9933\n",
      "Epoch 38/100, Loss: 1.9929\n",
      "Epoch 39/100, Loss: 1.9902\n",
      "Epoch 40/100, Loss: 1.9968\n",
      "Epoch 41/100, Loss: 1.9962\n",
      "Epoch 42/100, Loss: 1.9951\n",
      "Epoch 43/100, Loss: 1.9947\n",
      "Epoch 44/100, Loss: 1.9911\n",
      "Epoch 45/100, Loss: 1.9883\n",
      "Epoch 46/100, Loss: 1.9954\n",
      "Epoch 47/100, Loss: 1.9958\n",
      "Epoch 48/100, Loss: 1.9949\n",
      "Epoch 49/100, Loss: 1.9939\n",
      "Epoch 50/100, Loss: 1.9935\n",
      "Epoch 51/100, Loss: 1.9923\n",
      "Epoch 52/100, Loss: 1.9892\n",
      "Epoch 53/100, Loss: 1.9864\n",
      "Epoch 54/100, Loss: 1.9844\n",
      "Epoch 55/100, Loss: 1.9835\n",
      "Epoch 56/100, Loss: 1.9826\n",
      "Epoch 57/100, Loss: 1.9792\n",
      "Epoch 58/100, Loss: 1.9726\n",
      "Epoch 59/100, Loss: 1.9654\n",
      "Epoch 60/100, Loss: 1.9557\n",
      "Epoch 61/100, Loss: 1.9647\n",
      "Epoch 62/100, Loss: 1.9553\n",
      "Epoch 63/100, Loss: 1.9597\n",
      "Epoch 64/100, Loss: 1.9392\n",
      "Epoch 65/100, Loss: 1.9520\n",
      "Epoch 66/100, Loss: 1.9419\n",
      "Epoch 67/100, Loss: 1.9815\n",
      "Epoch 68/100, Loss: 1.9859\n",
      "Epoch 69/100, Loss: 1.9890\n",
      "Epoch 70/100, Loss: 1.9874\n",
      "Epoch 71/100, Loss: 1.9908\n",
      "Epoch 72/100, Loss: 1.9918\n",
      "Epoch 73/100, Loss: 1.9900\n",
      "Epoch 74/100, Loss: 1.9902\n",
      "Epoch 75/100, Loss: 1.9906\n",
      "Epoch 76/100, Loss: 1.9913\n",
      "Epoch 77/100, Loss: 1.9902\n",
      "Epoch 78/100, Loss: 1.9885\n",
      "Epoch 79/100, Loss: 1.9898\n",
      "Epoch 80/100, Loss: 1.9897\n",
      "Epoch 81/100, Loss: 1.9900\n",
      "Epoch 82/100, Loss: 1.9892\n",
      "Epoch 83/100, Loss: 1.9872\n",
      "Epoch 84/100, Loss: 1.9881\n",
      "Epoch 85/100, Loss: 1.9876\n",
      "Epoch 86/100, Loss: 1.9876\n",
      "Epoch 87/100, Loss: 1.9828\n",
      "Epoch 88/100, Loss: 1.9640\n",
      "Epoch 89/100, Loss: 1.9915\n",
      "Epoch 90/100, Loss: 1.9800\n",
      "Epoch 91/100, Loss: 1.9901\n",
      "Epoch 92/100, Loss: 1.9894\n",
      "Epoch 93/100, Loss: 1.9898\n",
      "Epoch 94/100, Loss: 1.9883\n",
      "Epoch 95/100, Loss: 1.9918\n",
      "Epoch 96/100, Loss: 1.9888\n",
      "Epoch 97/100, Loss: 1.9901\n",
      "Epoch 98/100, Loss: 1.9901\n",
      "Epoch 99/100, Loss: 1.9877\n",
      "Epoch 100/100, Loss: 1.9854\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/clo_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/GLE_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/eig_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/SR25/bet_SR25_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 2.0000\n",
      "Epoch 2/100, Loss: 2.0000\n",
      "Epoch 3/100, Loss: 2.0000\n",
      "Epoch 4/100, Loss: 2.0000\n",
      "Epoch 5/100, Loss: 2.0000\n",
      "Epoch 6/100, Loss: 2.0000\n",
      "Epoch 7/100, Loss: 2.0000\n",
      "Epoch 8/100, Loss: 2.0000\n",
      "Epoch 9/100, Loss: 2.0000\n",
      "Epoch 10/100, Loss: 2.0000\n",
      "Epoch 11/100, Loss: 2.0000\n",
      "Epoch 12/100, Loss: 2.0000\n",
      "Epoch 13/100, Loss: 2.0000\n",
      "Epoch 14/100, Loss: 2.0000\n",
      "Epoch 15/100, Loss: 2.0000\n",
      "Epoch 16/100, Loss: 2.0000\n",
      "Epoch 17/100, Loss: 2.0000\n",
      "Epoch 18/100, Loss: 2.0000\n",
      "Epoch 19/100, Loss: 2.0000\n",
      "Epoch 20/100, Loss: 2.0000\n",
      "Epoch 21/100, Loss: 2.0000\n",
      "Epoch 22/100, Loss: 2.0000\n",
      "Epoch 23/100, Loss: 2.0000\n",
      "Epoch 24/100, Loss: 2.0000\n",
      "Epoch 25/100, Loss: 2.0000\n",
      "Epoch 26/100, Loss: 2.0000\n",
      "Epoch 27/100, Loss: 2.0000\n",
      "Epoch 28/100, Loss: 2.0000\n",
      "Epoch 29/100, Loss: 2.0000\n",
      "Epoch 30/100, Loss: 2.0000\n",
      "Epoch 31/100, Loss: 2.0000\n",
      "Epoch 32/100, Loss: 2.0000\n",
      "Epoch 33/100, Loss: 2.0000\n",
      "Epoch 34/100, Loss: 2.0000\n",
      "Epoch 35/100, Loss: 2.0000\n",
      "Epoch 36/100, Loss: 2.0000\n",
      "Epoch 37/100, Loss: 2.0000\n",
      "Epoch 38/100, Loss: 2.0000\n",
      "Epoch 39/100, Loss: 2.0000\n",
      "Epoch 40/100, Loss: 2.0000\n",
      "Epoch 41/100, Loss: 2.0000\n",
      "Epoch 42/100, Loss: 2.0000\n",
      "Epoch 43/100, Loss: 2.0000\n",
      "Epoch 44/100, Loss: 2.0000\n",
      "Epoch 45/100, Loss: 2.0000\n",
      "Epoch 46/100, Loss: 2.0000\n",
      "Epoch 47/100, Loss: 2.0000\n",
      "Epoch 48/100, Loss: 2.0000\n",
      "Epoch 49/100, Loss: 2.0000\n",
      "Epoch 50/100, Loss: 2.0000\n",
      "Epoch 51/100, Loss: 2.0000\n",
      "Epoch 52/100, Loss: 2.0000\n",
      "Epoch 53/100, Loss: 2.0000\n",
      "Epoch 54/100, Loss: 2.0000\n",
      "Epoch 55/100, Loss: 2.0000\n",
      "Epoch 56/100, Loss: 2.0000\n",
      "Epoch 57/100, Loss: 2.0000\n",
      "Epoch 58/100, Loss: 2.0000\n",
      "Epoch 59/100, Loss: 2.0000\n",
      "Epoch 60/100, Loss: 2.0000\n",
      "Epoch 61/100, Loss: 2.0000\n",
      "Epoch 62/100, Loss: 2.0000\n",
      "Epoch 63/100, Loss: 2.0000\n",
      "Epoch 64/100, Loss: 2.0000\n",
      "Epoch 65/100, Loss: 2.0000\n",
      "Epoch 66/100, Loss: 2.0000\n",
      "Epoch 67/100, Loss: 2.0000\n",
      "Epoch 68/100, Loss: 2.0000\n",
      "Epoch 69/100, Loss: 2.0000\n",
      "Epoch 70/100, Loss: 2.0000\n",
      "Epoch 71/100, Loss: 2.0000\n",
      "Epoch 72/100, Loss: 2.0000\n",
      "Epoch 73/100, Loss: 2.0000\n",
      "Epoch 74/100, Loss: 2.0000\n",
      "Epoch 75/100, Loss: 2.0000\n",
      "Epoch 76/100, Loss: 2.0000\n",
      "Epoch 77/100, Loss: 2.0000\n",
      "Epoch 78/100, Loss: 2.0000\n",
      "Epoch 79/100, Loss: 2.0000\n",
      "Epoch 80/100, Loss: 2.0000\n",
      "Epoch 81/100, Loss: 2.0000\n",
      "Epoch 82/100, Loss: 2.0000\n",
      "Epoch 83/100, Loss: 2.0000\n",
      "Epoch 84/100, Loss: 2.0000\n",
      "Epoch 85/100, Loss: 2.0000\n",
      "Epoch 86/100, Loss: 2.0000\n",
      "Epoch 87/100, Loss: 2.0000\n",
      "Epoch 88/100, Loss: 2.0000\n",
      "Epoch 89/100, Loss: 2.0000\n",
      "Epoch 90/100, Loss: 2.0000\n",
      "Epoch 91/100, Loss: 2.0000\n",
      "Epoch 92/100, Loss: 2.0000\n",
      "Epoch 93/100, Loss: 2.0000\n",
      "Epoch 94/100, Loss: 2.0000\n",
      "Epoch 95/100, Loss: 2.0000\n",
      "Epoch 96/100, Loss: 2.0000\n",
      "Epoch 97/100, Loss: 2.0000\n",
      "Epoch 98/100, Loss: 2.0000\n",
      "Epoch 99/100, Loss: 2.0000\n",
      "Epoch 100/100, Loss: 2.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 15\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/SR25/*SR25_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs_sr25(graphs, 15, SR25_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f4a6125-1c6e-4bb9-91e9-ad49e1002165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/SR25/DE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/exN_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {120, 90, 60}\n",
      "Number of unique embeddings with GIN: 3\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {60, 30}\n",
      "Number of unique embeddings with PNA: 2\n",
      "\n",
      "------------- Processing ../data/SR25/exN_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {360, 330, 300, 390}\n",
      "Number of unique embeddings with GIN: 4\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {240, 300, 270}\n",
      "Number of unique embeddings with PNA: 3\n",
      "\n",
      "------------- Processing ../data/SR25/GLE_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/vn_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1680}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1440}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/eig_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {0}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/sub_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/GE_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1440, 1890, 2430, 1380, 1830, 1350, 1260, 1170, 1650, 1140, 1590, 1110, 990}\n",
      "Number of unique embeddings with GIN: 13\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1920, 1890, 1860, 1830, 1800, 1770, 2010, 1950}\n",
      "Number of unique embeddings with PNA: 8\n",
      "\n",
      "------------- Processing ../data/SR25/deg_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/GE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1920, 1890, 1800, 1290, 1740, 1230, 1710, 1650, 1530, 1950}\n",
      "Number of unique embeddings with GIN: 10\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1920, 1890, 1860, 1830, 1800, 1770, 1710, 1650, 2100, 2040}\n",
      "Number of unique embeddings with PNA: 10\n",
      "\n",
      "------------- Processing ../data/SR25/sub_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/vn_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2430}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2580}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/clo_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {0}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/GLE_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/DE_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/deg_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/eig_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/bet_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {750}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/bet_SR25_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/SR25/clo_SR25_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1500}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2250}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 1000) / 1000\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "            #h = torch.round(h * 1000) / 1000\n",
    "            h = h/torch.sum(h) * 10\n",
    "        #g_embedding = self.pool(g, h)\n",
    "        return h\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 1000) / 1000\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "            #h = torch.round(h * 1000) / 1000\n",
    "            h = h/torch.sum(h) * 10\n",
    "        #g_embedding = self.pool(g, h)\n",
    "        return h\n",
    "\n",
    "# Define a DeepSet model\n",
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.input_dim = input_dim  # Store input dimension\n",
    "        # First neural network to map node features to node embeddings\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Second neural network to map summed node embeddings to final graph embedding\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # Output a single value\n",
    "        )\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply the first neural network to each node feature\n",
    "        h = self.phi(h)\n",
    "        h = torch.round(h * 10000) / 10000  # Round to reduce precision\n",
    "        # Sum the node embeddings to create a graph embedding\n",
    "        g_embedding = h.sum(dim=0, keepdim=True)\n",
    "        # Apply the second neural network to the summed embedding\n",
    "        output = self.rho(g_embedding)*100\n",
    "        return output\n",
    "\n",
    "\n",
    "def calculate_integer_embedding(embedding):\n",
    "    # Convert embedding to a flattened list of integers\n",
    "    embedding = embedding.numpy()\n",
    "    embedding = np.nan_to_num(embedding, nan=0.0) * 10\n",
    "    embedding = embedding.astype(int)\n",
    "    if embedding.ndim == 1:\n",
    "        embedding = embedding.reshape(1, -1)  # Convert 1D array to a 2D array with one row\n",
    "\n",
    "    column_sum = embedding.sum(axis=0)\n",
    "    column_mean = embedding.mean(axis=0)\n",
    "    column_min = embedding.min(axis=0)\n",
    "\n",
    "    # Concatenate the results into a single 1D array\n",
    "    final_embedding = np.concatenate((column_sum, column_mean, column_min))\n",
    "\n",
    "    return int(np.sum((final_embedding.flatten()))*10)\n",
    "\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "\n",
    "    for epoch_in in range(10):\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim) \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "            target = torch.randn_like(output)*100  # Target is a random tensor of the same shape as output\n",
    "\n",
    "            loss = F.mse_loss(output, target.float(), reduction='mean')  # Ensure the target is a float tensor\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    embeddings = set()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            #print(\"embedding: \", embedding)\n",
    "            final_embedding = calculate_integer_embedding(embedding.flatten())\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/SR25/*SR25_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef1956-3c41-4e38-827c-fd6b6bc30593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
