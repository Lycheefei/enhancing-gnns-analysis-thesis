{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d04bc7-612b-40d2-9511-b21153d9ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596af5ed-01f7-493f-928c-674c5a52997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.transforms import VirtualNode\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, global_add_pool, PNAConv\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix, degree\n",
    "from torch_geometric.data.data import Data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a627b-a19f-4683-9e60-7a82b591467d",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1621889-9cac-4361-9643-6b3526bfe369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = \"../dataset/BREC/raw/brec_nonGNN.npy\"\n",
    "data = np.load(file_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdd16d1-20e1-442e-821c-a7e8bf73d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_dict = {\n",
    "    \"Basic\": (0, 60),\n",
    "    \"Regular\": (60, 160),\n",
    "    \"Extension\": (160, 260),\n",
    "    \"CFI\": (260, 360),\n",
    "    \"4-Vertex_Condition\": (360, 380),\n",
    "    \"Distance_Regular\": (380, 400),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b99f04a-3df5-4d21-b03c-a5887621f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store extracted graphs for each part\n",
    "graphs_parts = {}\n",
    "\n",
    "# Loop through the part_dict and extract the corresponding graphs\n",
    "for part_name, (start_idx, end_idx) in part_dict.items():\n",
    "    graphs_parts[part_name] = []\n",
    "    \n",
    "    # Iterate through the graph pairs and add them to the corresponding part\n",
    "    for graph_pair in data[start_idx:end_idx]:\n",
    "        for graph in graph_pair:  # Extract individual graphs from the pair\n",
    "            graphs_parts[part_name].append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf0791b-2e9b-48ed-9378-a2098c694c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_Basics = graphs_parts[\"Basic\"]\n",
    "G_Regular = graphs_parts[\"Regular\"]\n",
    "G_Extension = graphs_parts[\"Extension\"]\n",
    "G_CFI = graphs_parts[\"CFI\"]\n",
    "G_4Vertex = graphs_parts[\"4-Vertex_Condition\"]\n",
    "G_Distance_Regular = graphs_parts[\"Distance_Regular\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e913219-b22c-40d3-8d5c-6f1c6c69727c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801bbcf6-f0d5-44cf-905b-11c3c895bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VN\n",
    "transform = VirtualNode()\n",
    "def apply_vn(dgl_graphs):\n",
    "  vn_EXP_dgl = []\n",
    "  for graph in dgl_graphs:\n",
    "    graph_pyg = from_dgl(graph)\n",
    "    graph_pyg_copy = copy.deepcopy(graph_pyg)\n",
    "    graph_vn = transform(graph_pyg_copy)\n",
    "    graph_vn_dgl = to_dgl(graph_vn)\n",
    "    vn_EXP_dgl.append(graph_vn_dgl)\n",
    "\n",
    "  return vn_EXP_dgl\n",
    "\n",
    "# Centrality\n",
    "def add_centrality_to_node_features(dgl_graph, centrality_measure='degree'):\n",
    "    # Convert DGL data to NetworkX graph\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'betweenness':\n",
    "        centrality = nx.betweenness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "\n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = np.array([centrality[node] for node in range(dgl_graph.number_of_nodes())], dtype=np.float32).reshape(-1, 1)\n",
    "    centrality_values = torch.round(torch.tensor(centrality_values) * 10000) / 10000\n",
    "    # Concatenate the centrality with existing node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], centrality_values], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = centrality_values\n",
    "    return dgl_graph\n",
    "\n",
    "# Degree\n",
    "def degree_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_degree = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)  # Create a deep copy of the graph\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='degree')\n",
    "        Graph_data_degree.append(data_copy)\n",
    "    return Graph_data_degree\n",
    "\n",
    "# Closeness\n",
    "def closeness_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_clo = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='closeness')\n",
    "        Graph_data_clo.append(data_copy)\n",
    "    return Graph_data_clo\n",
    "\n",
    "#Betweenness\n",
    "def betweenness_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_bet = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='betweenness')\n",
    "        Graph_data_bet.append(data_copy)\n",
    "    return Graph_data_bet\n",
    "\n",
    "# Eigenvector\n",
    "def eigenvector_dataset(dataset):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_eig = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_centrality_to_node_features(data_copy, centrality_measure='eigenvector')\n",
    "        Graph_data_eig.append(data_copy)\n",
    "    return Graph_data_eig\n",
    "\n",
    "# DE\n",
    "def add_distance_encoding(dgl_graph):\n",
    "    # Compute the shortest distance matrix using dgl.shortest_dist\n",
    "    dist = dgl.shortest_dist(dgl_graph).float()  # Convert to float to handle inf\n",
    "\n",
    "    # Replace -1 with inf (to handle unreachable nodes similar to NetworkX's behavior)\n",
    "    dist[dist == -1] = float('inf')\n",
    "\n",
    "    # Calculate the average shortest distance for each node\n",
    "    finite_distances = torch.where(dist == float('inf'), torch.tensor(float('nan')), dist)\n",
    "    average_distance = torch.nanmean(finite_distances, dim=1).view(-1, 1)  # Use nanmean to ignore infinities\n",
    "\n",
    "    # Add the average distance to the existing node features in the DGL graph\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], average_distance], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = average_distance\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def distance_encoding(dataset):\n",
    "    Graph_data_DE = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = add_distance_encoding(data_copy)\n",
    "        Graph_data_DE.append(data_copy)\n",
    "    return Graph_data_DE\n",
    "\n",
    "# GE\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "\n",
    "def canonicalize_eigenvectors(eigenvectors):\n",
    "    \"\"\"Canonicalize eigenvectors by fixing their signs for consistency.\"\"\"\n",
    "    for i in range(eigenvectors.shape[1]):\n",
    "        if eigenvectors[0, i] < 0:  # Flip sign if the first element is negative\n",
    "            eigenvectors[:, i] = -eigenvectors[:, i]\n",
    "    return eigenvectors\n",
    "\n",
    "def add_canonicalized_laplacian_pe(dgl_graph, k=5):\n",
    "    \"\"\"\n",
    "    Add canonicalized Laplacian positional encoding to a DGL graph.\n",
    "\n",
    "    Args:\n",
    "        dgl_graph: Input DGL graph.\n",
    "        k: Number of Laplacian eigenvectors to compute.\n",
    "\n",
    "    Returns:\n",
    "        dgl_graph: DGL graph with Laplacian PE appended to node features.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert DGL graph to adjacency matrix\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "    adj = nx.to_numpy_array(G)\n",
    "\n",
    "    # Step 2: Compute Laplacian matrix\n",
    "    degree_matrix = np.diag(np.sum(adj, axis=1))\n",
    "    laplacian = degree_matrix - adj\n",
    "\n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "\n",
    "    # Step 4: Select the smallest k eigenvectors (sorted by eigenvalues)\n",
    "    idx = np.argsort(eigenvalues)[:k]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Step 5: Canonicalize eigenvectors\n",
    "    eigenvectors = canonicalize_eigenvectors(torch.tensor(eigenvectors, dtype=torch.float))\n",
    "\n",
    "    # Step 6: Add the eigenvectors as new node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], eigenvectors], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = eigenvectors\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def Graph_encoding(dataset, k=3):\n",
    "    \"\"\"\n",
    "    Apply canonicalized Laplacian positional encoding to a list of DGL graphs.\n",
    "\n",
    "    Args:\n",
    "        dgl_graphs: List of DGL graphs.\n",
    "        k: Number of Laplacian eigenvectors to compute.\n",
    "\n",
    "    Returns:\n",
    "        GE_EXP_dgl: List of DGL graphs with Laplacian PE added.\n",
    "    \"\"\"\n",
    "    GE_EXP_dgl = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        graph_pe = add_canonicalized_laplacian_pe(data_copy, k=k)\n",
    "        GE_EXP_dgl.append(graph_pe)\n",
    "    return GE_EXP_dgl\n",
    "\n",
    "# Sub\n",
    "def extract_local_subgraph_features(dgl_graph, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Initialize a list to store subgraph features for each node\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        # Extract the ego graph (subgraph) around the node\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "\n",
    "        # Example feature 1: Size of the subgraph (number of nodes)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "\n",
    "        # Example feature 2: Average degree of the subgraph\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "\n",
    "    # Convert the features to tensors and add them as node features\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], subgraph_sizes_tensor, subgraph_degrees_tensor], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = torch.cat([subgraph_sizes_tensor, subgraph_degrees_tensor], dim=1)\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def subgraph_dataset(dataset, radius=3):\n",
    "    # Compute centrality and add it as an additional feature\n",
    "    Graph_data_sub = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        data_copy = extract_local_subgraph_features(data_copy, radius=radius)\n",
    "        Graph_data_sub.append(data_copy)\n",
    "    return Graph_data_sub\n",
    "\n",
    "# ExN\n",
    "def add_extra_node_on_each_edge(dgl_graph):\n",
    "    # Collect new edges (source, destination) and the new node features\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_node_features = []\n",
    "\n",
    "    # Original number of nodes\n",
    "    num_original_nodes = dgl_graph.num_nodes()\n",
    "\n",
    "    # Use a set to track edges we have already processed (to avoid duplicates)\n",
    "    processed_edges = set()\n",
    "\n",
    "    # Iterate over all edges\n",
    "    for i in range(dgl_graph.num_edges()):\n",
    "        u, v = dgl_graph.edges()[0][i].item(), dgl_graph.edges()[1][i].item()\n",
    "\n",
    "        # Avoid processing reverse edges (v, u) if (u, v) is already processed\n",
    "        if (u, v) in processed_edges or (v, u) in processed_edges:\n",
    "            continue\n",
    "\n",
    "        # Mark the edge as processed\n",
    "        processed_edges.add((u, v))\n",
    "        processed_edges.add((v, u))  # In case there is a reverse edge\n",
    "\n",
    "        # Add a new node\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        mean_feature = (dgl_graph.ndata['x'][u] + dgl_graph.ndata['x'][v]) / 2\n",
    "        new_node_features.append(mean_feature)\n",
    "\n",
    "        # Add new edges connecting the new node to the original nodes\n",
    "        new_edges_src.append(u)\n",
    "        new_edges_dst.append(new_node_id)\n",
    "\n",
    "        new_edges_src.append(new_node_id)\n",
    "        new_edges_dst.append(v)\n",
    "\n",
    "    # Add new nodes to the DGL graph\n",
    "    dgl_graph.add_nodes(len(new_node_features), {'x': torch.stack(new_node_features)})\n",
    "\n",
    "    # Remove the original edges\n",
    "    dgl_graph.remove_edges(torch.arange(dgl_graph.num_edges()))\n",
    "\n",
    "    # Add new edges to the DGL graph\n",
    "    dgl_graph.add_edges(new_edges_src, new_edges_dst)\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def extra_node_dataset(dataset):\n",
    "    Graph_data_exN = []\n",
    "    for data in dataset:\n",
    "        data_copy = copy.deepcopy(data)\n",
    "        dgl_graph = add_extra_node_on_each_edge(data_copy)\n",
    "        Graph_data_exN.append(dgl_graph)\n",
    "    return Graph_data_exN\n",
    "\n",
    "def count_3_star(G):\n",
    "    \"\"\"Count 3-star graphlets for each node.\"\"\"\n",
    "    # A 3-star is a node with at least three neighbors\n",
    "    star_counts = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degree = len(neighbors)\n",
    "        # Count the number of 3-combinations of neighbors\n",
    "        star_counts[node] = max(0, (degree * (degree - 1) * (degree - 2)) // 6)\n",
    "    return star_counts\n",
    "\n",
    "def count_tailed_triangle(G):\n",
    "    \"\"\"Count tailed triangle graphlets for each node.\"\"\"\n",
    "    tail_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for neighbor in neighbors:\n",
    "            # For each pair of neighbors, check if there's a triangle\n",
    "            for other in neighbors:\n",
    "                if neighbor != other and G.has_edge(neighbor, other):\n",
    "                    # Found a triangle, check for a tail\n",
    "                    for extra in G.neighbors(node):\n",
    "                        if extra not in {neighbor, other}:\n",
    "                            tail_counts[node] += 1\n",
    "    return tail_counts\n",
    "\n",
    "def count_4_cycle(G):\n",
    "    \"\"\"Count 4-cycle graphlets for each node.\"\"\"\n",
    "    cycle_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for i, neighbor1 in enumerate(neighbors):\n",
    "            for neighbor2 in neighbors[i + 1:]:\n",
    "                # Check for shared neighbors forming a 4-cycle\n",
    "                for shared_neighbor in G.neighbors(neighbor1):\n",
    "                    if shared_neighbor in G.neighbors(neighbor2):\n",
    "                        cycle_counts[node] += 1\n",
    "    return cycle_counts\n",
    "\n",
    "def graphlet_based_encoding(dgl_graph):\n",
    "    \"\"\"\n",
    "    Add graphlet-based features (3-star, triangle, tailed triangle, 4-cycle) to node features.\n",
    "\n",
    "    Args:\n",
    "        dgl_graph: Input DGL graph.\n",
    "\n",
    "    Returns:\n",
    "        dgl_graph: DGL graph with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    # Convert DGL graph to NetworkX\n",
    "    G = dgl_graph.to_networkx()\n",
    "    G = nx.Graph(G)\n",
    "\n",
    "    # Count graphlets\n",
    "    triangle_counts = nx.triangles(G)  # Triangle counts\n",
    "    star_counts = count_3_star(G)  # 3-star graphlets\n",
    "    tail_counts = count_tailed_triangle(G)  # Tailed triangles\n",
    "    cycle_counts = count_4_cycle(G)  # 4-cycles\n",
    "\n",
    "    # Combine features into tensors\n",
    "    num_nodes = dgl_graph.num_nodes()\n",
    "    triangle_tensor = torch.tensor([triangle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    star_tensor = torch.tensor([star_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    tail_tensor = torch.tensor([tail_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    cycle_tensor = torch.tensor([cycle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Concatenate all graphlet features\n",
    "    graphlet_features = torch.cat([triangle_tensor, star_tensor, tail_tensor, cycle_tensor], dim=1)\n",
    "\n",
    "    # Add to node features\n",
    "    if 'x' in dgl_graph.ndata:\n",
    "        dgl_graph.ndata['x'] = torch.cat([dgl_graph.ndata['x'], graphlet_features], dim=1)\n",
    "    else:\n",
    "        dgl_graph.ndata['x'] = graphlet_features\n",
    "\n",
    "    return dgl_graph\n",
    "\n",
    "def graphlet_encoding_dataset(dgl_dataset):\n",
    "    \"\"\"\n",
    "    Apply graphlet-based encoding to a list of DGL graphs.\n",
    "\n",
    "    Args:\n",
    "        dgl_dataset: List of DGL graphs.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset: List of DGL graphs with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for dgl_graph in dgl_dataset:\n",
    "        graph_copy = copy.deepcopy(dgl_graph)\n",
    "        graph_encoded = graphlet_based_encoding(graph_copy)\n",
    "        encoded_dataset.append(graph_encoded)\n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2396216-5894-4406-aa8b-836b6c7af93c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Add Isomorphic Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61320dff-c013-4982-9a8e-6667852889f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_isomorphic_pairs_dgl(dataset, num_pairs=5):\n",
    "    isomorphic_pair = []\n",
    "    original_indices = []\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        # Pick a random graph from the dataset\n",
    "        original_graph = random.choice(dataset)\n",
    "        original_idx = dataset.index(original_graph)\n",
    "\n",
    "        # Convert to NetworkX and create isomorphic graphs\n",
    "        G = dgl.to_networkx(original_graph)\n",
    "        nodes = list(G.nodes())\n",
    "        random.shuffle(nodes)  # Shuffle to get isomorphic graph\n",
    "        mapping = {node: nodes[i] for i, node in enumerate(nodes)}\n",
    "        isomorphic_G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        # Convert back to DGL\n",
    "        isomorphic_dgl_graph = dgl.from_networkx(isomorphic_G)\n",
    "        isomorphic_dgl_graph.ndata['x'] = original_graph.ndata['x']  # Copy node features\n",
    "\n",
    "        isomorphic_pair.append(isomorphic_dgl_graph)\n",
    "        original_indices.append(original_idx)\n",
    "\n",
    "    return dataset, isomorphic_pair, original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2e16b-29cc-4e62-a1ea-3e55d18d1163",
   "metadata": {},
   "source": [
    "# Organize Graphs in Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "508c99e0-9dbf-4244-91d5-a3296cd0ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_pairs(dummy_dataset, original_indices):\n",
    "    original_size = int(len(dummy_dataset)*2/3)\n",
    "    non_isomorphic_pairs = []\n",
    "    isomorphic_pairs = []\n",
    "\n",
    "    # Group original graphs into non-isomorphic pairs (assuming they're already paired)\n",
    "    for i in range(0, original_size, 2):\n",
    "      non_isomorphic_pairs.append((dummy_dataset[i], dummy_dataset[i+1]))\n",
    "\n",
    "\n",
    "    for i in range(0, len(original_indices)):\n",
    "      indice = original_indices[i]\n",
    "      isomorphic_pairs.append((dummy_dataset[indice], dummy_dataset[i+original_size]))\n",
    "\n",
    "    return non_isomorphic_pairs, isomorphic_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20c08d-aecb-48d0-82f3-82ff8cb285c0",
   "metadata": {},
   "source": [
    "# Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ef66c8a-176a-4255-b062-b0eb63ff91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss2(embedding1, embedding2, label, margin=1.0):\n",
    "    \"\"\"Optimized contrastive loss: Pull embeddings together if label == 1, push them apart if label == 0.\"\"\"\n",
    "    euclidean_distance = F.pairwise_distance(embedding1, embedding2)\n",
    "    euclidean_distance_squared = torch.pow(euclidean_distance, 2)\n",
    "    \n",
    "    # Ensure that label is a tensor, convert to float tensor\n",
    "    if isinstance(label, int):  # Check if label is a scalar integer\n",
    "        label = torch.tensor(label).float().to(embedding1.device)\n",
    "    else:\n",
    "        label = label.float()\n",
    "\n",
    "    # Compute positive and negative losses\n",
    "    loss_positive = euclidean_distance_squared  # For label == 1\n",
    "    loss_negative = torch.pow(F.relu(margin - euclidean_distance), 2)  # For label == 0\n",
    "    \n",
    "    # Use torch.where with tensor inputs\n",
    "    loss = torch.where(label == 1, loss_positive, loss_negative)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53e44e81-7ee5-403f-b3c9-d6e0a8d9c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, model, input_dim):\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    import dgl\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Custom collate function to handle DGLGraphs\n",
    "    def collate_fn(batch):\n",
    "        # Unpack the batch of pairs\n",
    "        g1_batch, g2_batch = zip(*batch)\n",
    "    \n",
    "        # Batch the graphs using DGL's batch function\n",
    "        batched_g1 = dgl.batch(g1_batch)\n",
    "        batched_g2 = dgl.batch(g2_batch)\n",
    "        \n",
    "        return batched_g1, batched_g2\n",
    "    \n",
    "    # Custom Dataset for graph pairs (as you already have)\n",
    "    class GraphPairDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, pairs):\n",
    "            self.pairs = pairs\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.pairs)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            return self.pairs[idx]\n",
    "    \n",
    "    # Assuming you already have non_isomorphic_pairs and isomorphic_pairs\n",
    "    non_isomorphic_dataset = GraphPairDataset(non_isomorphic_pairs)\n",
    "    isomorphic_dataset = GraphPairDataset(isomorphic_pairs)\n",
    "    \n",
    "    # Define DataLoader with custom collate_fn\n",
    "    non_isomorphic_loader = DataLoader(non_isomorphic_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    isomorphic_loader = DataLoader(isomorphic_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "    \n",
    "        # Iterate over non-isomorphic pairs in batches\n",
    "        for batched_g1, batched_g2 in non_isomorphic_loader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            batched_g1 = batched_g1.to(device)\n",
    "            batched_g2 = batched_g2.to(device)\n",
    "    \n",
    "            embedding1 = model(batched_g1, batched_g1.ndata['x'])\n",
    "            embedding2 = model(batched_g2, batched_g2.ndata['x'])\n",
    "    \n",
    "            # Label is 0 for non-isomorphic pairs\n",
    "            loss = contrastive_loss2(embedding1, embedding2, label=torch.zeros(batched_g1.batch_size).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        # Iterate over isomorphic pairs in batches\n",
    "        for batched_g1, batched_g2 in isomorphic_loader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            batched_g1 = batched_g1.to(device)\n",
    "            batched_g2 = batched_g2.to(device)\n",
    "    \n",
    "            embedding1 = model(batched_g1, batched_g1.ndata['x'])\n",
    "            embedding2 = model(batched_g2, batched_g2.ndata['x'])\n",
    "    \n",
    "            # Label is 1 for isomorphic pairs\n",
    "            loss = contrastive_loss2(embedding1, embedding2, label=torch.ones(batched_g1.batch_size).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{100}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    non_isomorphic_different_count = 0\n",
    "    isomorphic_same_count = 0\n",
    "    isomorphic_different_count = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        # Compare embeddings of non-isomorphic pairs\n",
    "        for g1, g2 in non_isomorphic_pairs:\n",
    "            g1 = g1.to(device)\n",
    "            g2 = g2.to(device)\n",
    "            embedding1 = model(g1, g1.ndata['x']).to(device)\n",
    "            embedding2 = model(g2, g2.ndata['x']).to(device)\n",
    "            cosine_sim = F.cosine_similarity(embedding1, embedding2).item()\n",
    "            if cosine_sim < 0.99:\n",
    "                non_isomorphic_different_count += 1  # Correctly classified as different\n",
    "            #final_embedding1 = calculate_integer_embedding(embedding1)\n",
    "            #final_embedding2 = calculate_integer_embedding(embedding2)\n",
    "            #if final_embedding1 != final_embedding2:\n",
    "            #    non_isomorphic_different_count += 1\n",
    "                \n",
    "\n",
    "        # Compare embeddings of isomorphic pairs\n",
    "        for g1, g2 in isomorphic_pairs:\n",
    "            g1 = g1.to(device)\n",
    "            g2 = g2.to(device)\n",
    "            embedding1 = model(g1, g1.ndata['x']).to(device)\n",
    "            embedding2 = model(g2, g2.ndata['x']).to(device)\n",
    "            cosine_sim = F.cosine_similarity(embedding1, embedding2).item()\n",
    "            if cosine_sim > 0.99:\n",
    "                isomorphic_same_count += 1  # Correctly classified as the same\n",
    "            else:\n",
    "                isomorphic_different_count += 1  # Incorrectly classified as different\n",
    "\n",
    "            #final_embedding1 = calculate_integer_embedding(embedding1)\n",
    "            #final_embedding2 = calculate_integer_embedding(embedding2)\n",
    "\n",
    "            #if final_embedding1 == final_embedding2:\n",
    "            #   isomorphic_same_count += 1\n",
    "            #else:\n",
    "            #    isomorphic_different_count += 1\n",
    "\n",
    "    print(f\"Correctly classified non-isomorphic pairs: {non_isomorphic_different_count}\")\n",
    "    print(f\"Correctly classified isomorphic pairs: {isomorphic_same_count}\")\n",
    "    print(f\"Incorrectly classified isomorphic pairs: {isomorphic_different_count}\")\n",
    "\n",
    "    return non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af45cbd-797a-48e3-acb5-d7e552e2320c",
   "metadata": {},
   "source": [
    "# BREC-Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b999d9c-47ce-4479-93d0-bf1b783cb9be",
   "metadata": {},
   "source": [
    "## Original Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a435a377-c13e-4617-a81d-3670cce5af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/G_Basics_dataset.pkl.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_Basics_dgl_graphs = []\n",
    "for G in G_Basics:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_Basics_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/Basics/G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Basics_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_Basics_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd166d8-8ada-4111-94f8-d0b63f6888bd",
   "metadata": {},
   "source": [
    "## Basics dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5eae61bf-8136-467c-9483-ad9fd8e7e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "G_Basics_dgl_graphs, G_Basics_isomorphic_pair, G_Basics_original_indices = add_isomorphic_pairs_dgl(G_Basics_dgl_graphs, num_pairs=60)\n",
    "G_Basics_dummy_dgl = G_Basics_dgl_graphs + G_Basics_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/Basics/G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19386283-f2f4-4cf6-86cb-ad4f1f20c034",
   "metadata": {},
   "source": [
    "## VN on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f347b3b9-a11a-4c05-9ef2-914238571aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/vn_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/vn_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "vn_G_Basics_dgl = apply_vn(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/vn_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_Basics_dummy_dgl = apply_vn(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/vn_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b3a75-5b1e-459a-86a1-29c55dc20f1a",
   "metadata": {},
   "source": [
    "## Degree Centrality on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4d403c9f-8f86-45cf-8e57-e809f9413465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/deg_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/deg_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "deg_G_Basics_dgl = degree_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/deg_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_Basics_dummy_dgl = degree_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/deg_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31515362-3b18-4000-9686-d3f706608cfe",
   "metadata": {},
   "source": [
    "## Closeness Centrality on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7925fda0-7418-40c0-a04b-d81a9f6c23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/clo_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/clo_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_Basics_dgl = closeness_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/clo_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_Basics_dummy_dgl = closeness_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/clo_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa3886-7cc5-4b17-af0b-9b6d7b022f98",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0e9cc7fd-af0a-4b42-9fb5-15984f736f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/bet_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/bet_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "bet_G_Basics_dgl = betweenness_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/bet_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_Basics_dummy_dgl = betweenness_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/bet_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd1c18-cf4b-4eaa-a4c8-4b4675a2fbc0",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9cc2a233-3366-4f48-a310-b1e027c51bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/eig_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/eig_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "eig_G_Basics_dgl = eigenvector_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/eig_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_Basics_dummy_dgl = eigenvector_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/eig_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73490d51-292b-4fab-81d9-8ff74426d60c",
   "metadata": {},
   "source": [
    "## Distance Encoding on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5454eae8-caf0-4a61-ab5f-84468755f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/DE_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/DE_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "DE_G_Basics_dgl = distance_encoding(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/DE_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_Basics_dummy_dgl = distance_encoding(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/DE_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb5331-b36e-4465-9c4b-d01df027403b",
   "metadata": {},
   "source": [
    "## Graph Encoding on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2500d07f-3f53-44ab-bcc8-5b5c1afd7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/GE_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/GE_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "GE_G_Basics_dgl = Graph_encoding(G_Basics_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/Basics/GE_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_Basics_dummy_dgl = Graph_encoding(G_Basics_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/Basics/GE_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9d4da-6442-4fd6-a55c-275e5556e037",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "40ebb617-a132-44d9-9527-0b25bd84fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/SE_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/SE_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SE_G_Basics_dgl = subgraph_dataset(G_Basics_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/Basics/SE_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_Basics_dummy_dgl = subgraph_dataset(G_Basics_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/Basics/SE_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e435803-4646-46b5-9a9a-5d66021912e1",
   "metadata": {},
   "source": [
    "## Extra Node on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "de001bfe-ef24-41c3-ba83-ef7158f0d64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/exN_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/exN_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "exN_G_Basics_dgl = extra_node_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/exN_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_Basics_dummy_dgl = extra_node_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/exN_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11926aba-361c-4b91-9d63-f229826b36f0",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on Basics original and Basics dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "216b5fe2-c398-4d83-b43e-024750a86f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 120 graphs to DGL format and saved to ../data/BREC/Basics/gle_G_Basics_dataset.pkl.\n",
      "Converted 180 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Basics/gle_G_Basics_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "gle_G_Basics_dgl = graphlet_encoding_dataset(G_Basics_dgl_graphs)\n",
    "output_file = '../data/BREC/Basics/gle_G_Basics_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Basics_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Basics_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_Basics_dummy_dgl = graphlet_encoding_dataset(G_Basics_dummy_dgl)\n",
    "output_file = '../data/BREC/Basics/gle_G_Basics_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Basics_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Basics_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd589c2-7f43-4cb9-90bc-917805eadf01",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9abdfca7-64af-486e-be82-58336a6c9bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Basics/deg_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {98049, 519041, 84611, 41092, 74500, 1290503, 40969, 222218, 79114, 1522316, 1194510, 49041, 1845651, 1375637, 1375638, 28695, 92057, 65945, 371229, 814751, 23328, 83748, 119848, 38188, 57132, 4014255, 13384501, 61365, 84535, 105655, 18305981, 27582, 18305984, 1100995, 1078985, 1474635, 14176843, 14176844, 87758, 58702, 41805, 90705, 3388620, 131545, 185818, 35018459, 59228, 119386, 72670, 78684, 3259616, 999143, 97512, 1435625, 4358124, 94190, 2251631, 73070, 63348, 96116, 58106, 124028, 103423}\n",
      "Number of unique embeddings with GIN: 63\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {10881, 12931, 12676, 14274, 10635, 7950, 12558, 10256, 34577, 12562, 43668, 15637, 6805, 5781, 6932, 17686, 6170, 17687, 19989, 9758, 6948, 11942, 12584, 13355, 14636, 15921, 15285, 13752, 12221, 6589, 14399, 7102, 12993, 14018, 9922, 12867, 7362, 16708, 6596, 16200, 6344, 91338, 16074, 19013, 11470, 14671, 35027, 11604, 14803, 11348, 9175, 12761, 7396, 14569, 8937, 8811, 15606, 14710, 18552, 6653, 16766}\n",
      "Number of unique embeddings with PNA: 61\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/DE_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2098690, 263190, 206874, 181789, 261668, 91684, 123431, 123432, 101415, 108077, 185903, 163386, 239169, 377926, 1152079, 23499860, 1096276, 2863194, 305252, 1752181, 123005, 1228929, 58006, 138391, 208550, 1436843, 1493168, 242355, 126145, 40645, 170701, 68814, 147677, 1268451, 427244, 177902, 112372, 351477, 173312, 530189, 110353, 810772, 185119, 689959, 176424, 352554, 322347, 8396074, 8396075, 86833, 49478, 106321, 960346, 742243, 37220, 958310, 295785, 184683, 1342829, 409458, 529781, 148877, 86926, 1006991, 5724053, 496533, 145309, 344994, 831397, 77745, 77746, 410038, 93626, 1645511, 148952, 207333, 5338086, 5338088, 74217, 195057, 195058}\n",
      "Number of unique embeddings with GIN: 81\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2692, 25481, 10762, 4745, 1038, 4113, 3731, 2836, 12949, 37143, 2328, 2713, 13466, 1305, 1308, 2075, 929, 1059, 140068, 32165, 6822, 8483, 1699, 15401, 26538, 37931, 38572, 47399, 2221, 14895, 6191, 1201, 2554, 1330, 18741, 13110, 99769, 8378, 39355, 2876, 5561, 7868, 1599, 78012, 1345, 1220, 4037, 36427, 1995, 1618, 180435, 3924, 1877, 2135, 1112, 4699, 11868, 2143, 1251, 1892, 1510, 2536, 2793, 5225, 1261, 7791, 1392, 12913, 5359, 1141, 758, 2679, 1013, 2426, 12283, 3068}\n",
      "Number of unique embeddings with PNA: 76\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/eig_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {71554, 1004419, 40706, 102661, 54150, 102662, 480779, 102285, 121621, 3476246, 3259164, 3259165, 99614, 116637, 448161, 173091, 63526, 426279, 650279, 44713, 72235, 611884, 1062573, 501421, 70446, 82098, 41906, 41267, 414010, 145216, 72129, 119360, 56260, 9332168, 629321, 40265, 27468, 4577230, 91087, 91088, 211538, 100052, 227798, 132823, 79192, 52957, 644319, 54753, 483938, 259297, 132452, 214372, 36201, 1265257, 425578, 128879, 339313, 128629, 400885, 181365, 28408, 1664121, 167037}\n",
      "Number of unique embeddings with GIN: 63\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {9216, 8961, 7170, 11139, 48515, 8709, 5760, 3848, 10505, 113419, 8460, 6030, 4495, 12562, 7442, 10390, 28570, 28571, 7962, 10010, 10911, 8097, 6562, 9641, 12714, 5803, 12204, 61097, 25389, 22319, 48048, 7855, 5545, 8107, 6581, 9017, 30266, 21436, 13629, 9278, 9405, 13119, 15549, 8263, 4424, 19657, 5065, 6863, 48852, 7765, 8025, 10585, 15708, 15709, 9692, 12383, 6505, 10226, 12022, 16759, 6518, 9338}\n",
      "Number of unique embeddings with PNA: 62\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/exN_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {45058, 37893, 131597, 362003, 116256, 49184, 59943, 213055, 89155, 35917, 216143, 1056856, 177245, 72292, 26221, 53360, 311924, 84596, 177789, 44684, 21654, 137878, 67734, 184477, 62111, 53927, 161456, 41139, 150205, 201408, 39617, 1117894, 125135, 113360, 131796, 129236, 77020, 119521, 148707, 23779, 137967, 150257, 42226, 79096, 43775, 124677, 56584, 118024, 233736, 75016, 389900, 20753, 28440, 86301, 56608, 59682, 217892, 166693, 91438, 277807, 109877, 76090, 139579, 84800, 181072, 376669, 143198, 156528, 38776, 41855, 42375, 212872, 59273, 455049, 179610, 492954, 683423, 50593, 307112, 105388, 151469, 162736, 37809, 35256, 126397, 176574, 67519, 253890, 100296, 51658, 47564, 20430, 104911, 36818, 682963, 133081, 177635, 75757, 55790, 190963, 56308, 58361, 164350}\n",
      "Number of unique embeddings with GIN: 103\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {17415, 23568, 34834, 23580, 19999, 29757, 18495, 12355, 12358, 48206, 31312, 18518, 17499, 29788, 12381, 20574, 31336, 12907, 29804, 16493, 64140, 94350, 22670, 18574, 65174, 93848, 74392, 27800, 96409, 11933, 20132, 27814, 28839, 28849, 17586, 14003, 12469, 28856, 12477, 12991, 21703, 20169, 58062, 20175, 32465, 18652, 22754, 77539, 18153, 65262, 28918, 14097, 33043, 77591, 13080, 64794, 27941, 18732, 12082, 20275, 34621, 74050, 34631, 24393, 57674, 18766, 65359, 34131, 12125, 18783, 34145, 12641, 14181, 18795, 19308, 29550, 20334, 16752, 18289, 18815, 33666, 16790, 20374, 12188, 12197, 11707, 96701, 19904, 13761, 33221, 19412, 20437, 11741, 11744, 12257, 12774, 11755, 16880, 48113, 13811, 17396, 20469, 17399, 22015}\n",
      "Number of unique embeddings with PNA: 104\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/gle_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {4131855, 2582032, 600593, 3313174, 1045532, 1926696, 21271597, 308282, 768059, 392764, 774719, 16669256, 90699, 396887, 412247, 8044642, 1098850, 373864, 1625200, 42613, 753790, 2248835, 294029, 34958, 42640, 2303123, 4699797, 378011, 359077, 332454, 1258664, 267945, 591018, 20151, 377020, 37569, 223943, 3615945, 568522, 62156, 16701647, 1063123, 208089, 2033892, 8537317, 33606373, 18666, 281835, 355563, 2023661, 433912, 145657, 100603, 4506877, 35070, 705283, 335116, 1431826, 2796307, 1512731, 35612, 991534, 43158843, 445251, 3651397, 568649, 235337, 22354, 696673, 6646628, 561516, 37752, 36734, 1456004, 187278, 521614, 216996, 3587493, 1705382, 976806, 38314, 21951915, 3974572, 4441522, 1429940, 1611195, 5773244, 37822, 35265, 1341382, 192456, 648653, 1032654, 2429393, 7346648, 19791323, 4779488, 4531681, 39397, 4962283, 1754096, 858098, 150015}\n",
      "Number of unique embeddings with GIN: 103\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {12291, 183301, 12295, 8204, 11278, 12304, 37932, 48687, 13872, 72251, 46654, 22084, 12869, 8781, 26712, 8286, 38498, 14968, 11900, 12417, 10377, 252047, 9873, 10898, 10392, 10905, 22171, 1801886, 16034, 10917, 22695, 747687, 11947, 10411, 9902, 9398, 19640, 8890, 14526, 10430, 24256, 726208, 9410, 37062, 7305929, 84169, 68299, 19145, 22734, 11477, 11995, 91360, 9959, 13044, 9465, 12030, 32514, 126215, 42247, 8970, 11531, 12556, 13584, 10513, 11027, 9491, 8473, 8986, 10014, 9504, 14628, 13604, 12068, 10023, 15156, 9016, 9535, 8521, 12618, 15177, 7356748, 10062, 116047, 9057, 9572, 909673, 9069, 1765235, 12152, 20857, 9594, 9082, 8062, 10628, 16774, 35727, 8596, 64919, 12194, 10153, 8617, 23471, 11188, 40373, 185276, 894403, 26051, 69065, 9164, 19923, 15317, 10200, 246749, 31201, 25572, 13807, 9200, 11763, 9212, 20991}\n",
      "Number of unique embeddings with PNA: 120\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/vn_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {144436865, 272257, 2237059, 90244, 360453, 693255, 348681, 1545737, 315020, 64199310, 128506, 2090516, 357653, 297238, 43375643, 313248, 85540, 130089, 516394, 568363, 68139, 449836, 450730, 257328, 16279605, 1521848, 1054392, 5426747, 158012, 565693, 126272, 2165446, 148551, 3189448, 4721999, 2336977, 160083, 181718, 176218, 2203259, 1269083, 13661788, 1188319, 185696, 122337, 4249062, 72167, 592872, 132585, 518250, 127608, 67053, 2656366, 2648438, 46342903, 46342904, 107129, 9667322, 9667323, 275965, 4097534, 9850879}\n",
      "Number of unique embeddings with GIN: 62\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {13952, 16769, 15746, 13568, 16515, 19462, 11786, 13709, 13198, 13074, 14098, 26772, 14613, 13055, 18584, 12186, 12443, 16666, 12062, 25504, 20385, 14881, 12836, 12581, 12598, 13622, 12217, 12218, 25147, 12475, 16706, 12228, 13765, 13766, 14789, 14021, 11339, 13132, 16974, 16596, 19543, 13018, 13147, 12380, 34653, 14686, 15069, 19295, 12002, 14821, 12646, 17514, 15338, 12269, 16622, 12530, 13302, 13816, 17278, 14463}\n",
      "Number of unique embeddings with PNA: 60\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/deg_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {5175040, 5175041, 5147138, 5147137, 134404, 108677, 371845, 60935, 208772616, 45067, 139021, 5014158, 120080, 996497, 936085, 4374805, 4374806, 593686, 83605, 1006745, 1092886, 1092885, 77083, 6062110, 6062111, 859299, 2543912, 1300649, 473465, 221739, 4244267, 78213421, 1863348, 16593531, 750645, 5633845, 3223734, 78213433, 14284727, 14284728, 45846970, 866229, 1691712, 1221063, 1221064, 55752, 1186250, 1186251, 1212024, 7074892, 34119, 339786, 4912592, 358479, 709842, 27219, 4473942, 13964887, 105814, 99938, 5484136, 1087850, 65514, 52139633, 1261938, 524662, 12365175, 12365174, 1212025, 7622650, 7622651, 16593532, 424317}\n",
      "Number of unique embeddings with GIN: 73\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {23552, 5890, 10120, 11916, 4108, 6161, 15634, 29075, 31252, 40983, 509181, 29089, 7970, 4515, 8868, 23462, 252071, 77623, 12344, 10553, 74427, 15163, 3644, 63165, 11711, 30914, 21187, 20932, 38980, 6851, 9415, 3270, 358090, 20428, 9804, 107471, 34898, 16598, 11351, 127324, 15583, 35424, 93537, 36834, 46055, 47719, 50281, 20842, 215787, 7400, 29293, 89588, 7668, 79222, 31735, 44152, 6651, 24188, 20093, 32638}\n",
      "Number of unique embeddings with PNA: 60\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/bet_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2018309, 330757, 76295, 51213, 721934, 40975, 112143, 624668, 227868, 162339, 250406, 198695, 252974, 815153, 1729074, 4519487, 4723780, 917066, 1947215, 138329, 223848, 88174, 259700, 89722, 128637, 13360768, 81536, 207490, 413828, 282757, 4715660, 797329, 174741, 521886, 234668, 116913, 354483, 1571508, 973497, 303805, 163518, 6261951, 369870, 970447, 443601, 192730, 339167, 705759, 77024, 125672, 199917, 79599, 366327, 883452, 322813, 764671, 13146880, 82689, 256260, 261389, 1646863, 243473, 92948, 83740, 666397, 117024, 218403, 1814314, 954675, 288055, 286522, 928582, 367433, 680269, 87894, 48986, 493915, 78172, 706909, 38750, 6203240, 253294, 213872, 795507, 795508, 45941, 1917816, 4501375, 41858, 147344, 120726, 204183, 124311, 499101, 328095, 380831, 180641, 157608, 198056, 60847, 182712, 250304, 344010, 1189837, 213967, 917968, 183759, 213455, 2089433, 249317, 880614, 639977, 1172970, 953327, 531440, 195567, 219641, 208378}\n",
      "Number of unique embeddings with GIN: 118\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {9216, 6147, 8709, 9229, 17423, 13839, 11799, 6168, 6681, 6179, 7204, 36920, 69695, 5189, 7253, 7765, 10325, 36954, 5723, 4190, 5225, 6252, 37484, 41069, 5230, 13428, 5751, 6775, 35453, 5770, 6135, 14484, 5785, 6816, 55456, 28836, 7335, 6317, 23731, 4278, 16567, 8888, 5303, 10422, 8904, 8906, 154315, 6862, 7378, 6879, 51423, 407789, 9460, 7413, 218870, 6391, 21240, 7929, 5366, 153340, 163069, 6910, 4865, 6405, 9991, 8967, 14089, 37670, 8500, 4408, 69953, 14149, 219463, 5447, 4440, 15192, 7522, 5476, 9573, 10602, 7531, 11119, 15741, 6528, 7557, 161161, 7050, 5006, 6544, 5523, 5016, 9113, 6049, 13220, 5549, 3503, 5040, 12209, 11187, 9144, 7613, 6592, 9667, 10198, 14811, 5085, 8669, 28645, 24552, 3564, 14832, 5616, 21489, 406005, 9207, 11768}\n",
      "Number of unique embeddings with PNA: 116\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/bet_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {672257, 550403, 190468, 634375, 1690636, 2068498, 64105494, 1214998, 10547229, 9804829, 7627809, 1541667, 2046499, 7858725, 62369318, 9676339, 6242873, 8321081, 1475641, 8570949, 68168, 6389834, 2865233, 4207702, 936544, 6210145, 237155, 965735, 288873, 555122, 129650, 1806453, 20609, 308872, 85069962, 146058, 23181, 7040657, 7552152, 1676964, 2206373, 198317, 194500271, 537263, 8157873, 294068, 1757369, 17791164, 1045185, 166596, 9756870, 113863, 3868360, 3868361, 9547977, 337607, 218832, 6136545, 6136546, 974564, 751852, 1886959, 7024371, 19666170, 17731336, 22286, 20239, 1192210, 6742808, 984857, 84341554, 336690, 1226040, 164671, 23015745, 1161538, 1901895, 925516, 613724, 1612636, 1711965, 23913, 2464621, 2935665, 21618034, 399731, 1621364, 242038, 2351485, 1777541, 217990, 250257, 64058777, 904602, 20889, 2779549, 97193, 1258409, 2134953, 2012076, 19228589, 23833520, 216780209, 61572532, 47544, 209336, 6693826, 21741002, 2069963, 3952590, 10592207, 7591383, 1747419, 7968220, 1412572, 2964963, 776165, 84459, 863228}\n",
      "Number of unique embeddings with GIN: 119\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {11280, 12824, 9753, 17947, 7197, 72738, 11299, 7203, 31785, 466986, 10282, 5678, 12853, 6736, 29283, 8813, 9838, 31349, 39037, 8325, 11934, 445605, 10409, 9898, 71857, 15030, 16057, 42685, 14535, 14540, 7372, 33487, 7383, 7384, 7898, 20699, 221915, 31964, 18151, 32488, 10482, 6389, 21240, 17146, 11516, 61692, 6399, 14080, 30979, 17156, 33539, 9479, 71943, 72457, 14090, 11033, 5916, 36135, 8999, 14637, 44342, 30007, 14136, 28474, 35646, 8514, 6467, 32070, 34131, 15708, 9568, 11107, 63336, 31080, 23917, 14701, 174960, 10608, 13171, 6516, 18815, 29055, 13695, 40843, 18831, 8082, 173977, 167833, 12187, 6556, 11168, 34723, 66468, 9125, 17327, 74163, 10677, 9653, 167351, 13240, 6587, 32191, 19394, 11204, 32248, 13772, 13266, 13267, 8661, 15831, 227804, 40927, 10209, 15352, 19449, 13818, 40444}\n",
      "Number of unique embeddings with PNA: 117\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/clo_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {1394177, 208391, 109067, 2401297, 441874, 99355, 1088028, 1550374, 357927, 9147444, 261700, 1643823177, 244812, 51279, 114257, 146520, 2162270, 8647263, 86623, 2714724, 44647, 414332, 324231, 305288, 184461, 109710, 155281, 106657, 1639075, 238262, 652472, 652473, 23349433, 306272954, 306272955, 38601, 23349458, 1427673, 10884825, 1538784, 1538785, 795878, 192233, 1950967, 3482368, 878855, 360535310, 360535311, 8095507, 8095510, 12620054, 12620055, 1643822875, 9318693, 39705894, 100137, 123699, 609924403, 1186104, 543545, 94523, 161084, 62852935, 198987, 37388626, 564052, 9862495, 9862496, 1699171, 12548464, 37388659, 52110710, 1018770, 19128216, 9838488, 9838490, 19128219, 609924504, 1194406, 119719, 76335026, 198072, 7646658, 18401736, 201174, 1257435, 1519082, 258539, 73708}\n",
      "Number of unique embeddings with GIN: 89\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {19458, 3077, 4624, 19473, 5137, 7710, 7200, 11306, 37931, 12336, 3635, 5700, 25670, 4167, 23122, 8283, 3164, 5730, 21603, 212089, 8827, 112251, 12924, 3732, 12437, 5275, 4764, 4765, 7324, 21155, 110762, 10420, 682680, 3773, 31436, 63693, 6860, 1743, 7377, 8924, 13030, 4840, 8959, 4880, 6932, 13593, 5417, 4906, 52526, 11059, 4413, 22860, 38743, 223576, 8025, 10588, 9058, 9059, 65890, 25452, 17773, 17774, 2929, 24971, 2970, 38299, 65438, 22434, 19877, 11716, 5061, 37333, 10716, 3037, 8671, 5106, 5112, 12795, 296956, 296957}\n",
      "Number of unique embeddings with PNA: 80\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/SE_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {4676737, 280705, 144134, 281608, 4472206, 4472207, 809999, 809998, 4813839, 4415374, 651412, 2896020, 199570, 50713, 174234, 295834, 156522524, 312217, 114243744, 63648, 705060, 6967336, 229546, 797355, 114243757, 318132, 156522550, 305078, 2608312, 207546, 170171, 313659, 190141, 371904, 367297, 157951, 84164, 325831, 256200, 554825, 52939, 119371, 131280, 27822291, 27822292, 215125, 2827092, 108815449, 2622684, 9194461, 187229, 373546464, 23431648, 3466469, 19025766, 4823014, 476135, 131563, 59245, 19025774, 319342, 529134, 217457, 25613298, 95862, 1559419, 327164, 2527103}\n",
      "Number of unique embeddings with GIN: 68\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {51969, 51970, 29315, 3843, 9719, 8588, 16399, 10512, 6415, 34322, 11284, 3477, 7829, 7318, 18072, 66586, 8219, 4636, 518812, 14747, 256375, 1189, 12069, 7207, 7847, 8107, 7212, 9648, 2868, 20021, 8632, 9147, 9148, 17725, 3192, 11072, 13252, 6213, 7243, 186574, 13779, 28116, 10452, 8793, 28762, 13148, 5980, 71518, 4192, 5729, 18528, 18403, 4577, 4453, 197222, 5990, 62957, 12910, 12911, 19568, 19566, 5743, 4343, 9208, 9598}\n",
      "Number of unique embeddings with PNA: 65\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/DE_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {29952, 29953, 25605, 90122, 91275, 13327, 217233, 37012, 140181, 27029, 76824, 148376, 27422, 97694, 28064, 108964, 71463, 24615, 103338, 59820, 35373, 125815, 26289, 190129, 1391416, 127032, 26811, 1711935, 28023, 223686, 333639, 70344, 100168, 89292, 94540, 153420, 81488, 156624, 62674, 28882, 34263, 408664, 121051, 835036, 360568, 117083, 153310, 56542, 55014, 39785, 155243, 181611, 139501, 779888, 51570, 74739, 55796, 55797, 175605, 175606, 161141, 179961, 206586, 124669, 163582, 62719}\n",
      "Number of unique embeddings with GIN: 66\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {9352, 5002, 22027, 53644, 25613, 43407, 12945, 26642, 14484, 7700, 21782, 16150, 24342, 38303, 19999, 7841, 7329, 101407, 24228, 11426, 10662, 20135, 28584, 18087, 46378, 9131, 10410, 58541, 18347, 8879, 30000, 19500, 63538, 8755, 22709, 7861, 7739, 12860, 6333, 6204, 102719, 24252, 29243, 15170, 129091, 10556, 20293, 51141, 31687, 10566, 9804, 21964, 7503, 16848, 29009, 8786, 11603, 17492, 20948, 22868, 13651, 7386, 5978, 7263, 9056, 6370, 40163, 26211, 13285, 5566, 21353, 6890, 201455, 23668, 52469, 21368}\n",
      "Number of unique embeddings with PNA: 76\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/gle_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {251397, 5619718, 330248, 321546, 688140, 395789, 4440591, 3257364, 2276376, 1163289, 1642521, 42523, 60953, 3649573, 3523625, 2673194, 2704939, 111940652, 603699, 2196533, 2272316, 2367558, 617552, 385617, 4079190, 42072, 17772123, 571491, 1614950, 349290, 4642418, 414836, 8060021, 2632829, 2069119, 4445314, 402563, 885900, 379548, 2703521, 3571874, 4925612, 293556, 48621753, 4128961, 2835655, 65743, 4286162, 4165855, 4017889, 1090277, 172782, 44066543, 3571950, 324848, 253686, 821495, 880391, 1627918, 18049809, 500498, 2034966, 2833178, 935707, 3902746, 33569, 1898792, 213804, 2938158, 2920751, 1966897, 2597681, 2050870, 1255742, 103848255, 1979724, 91470, 968015, 51025, 2690392, 90969, 3283291, 232289, 1099108, 4683111, 472424, 175463, 989549, 2318708, 1919864, 136570, 452991, 1285509, 147333, 22625176, 2534296, 2532772, 24874917, 5391789, 3110848, 3295684, 5365190, 4044231, 262601, 1222092, 1868749, 156624, 2028501, 1712089, 1562588, 1565149, 564703, 1629154, 1131495, 3247594, 3031018, 3189741, 2337778, 499699, 2900470}\n",
      "Number of unique embeddings with GIN: 120\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {11267, 11269, 9758, 9760, 10274, 27175, 9767, 11314, 10807, 31290, 26687, 8767, 12357, 10829, 9807, 54352, 10842, 11375, 9328, 10863, 75379, 10356, 12916, 9335, 10879, 53377, 11394, 9858, 21124, 11393, 17031, 21128, 10887, 10379, 24719, 78484, 11428, 11429, 10416, 9907, 10932, 54966, 271542, 18617, 52924, 32447, 15047, 54488, 394970, 13024, 10471, 21242, 9983, 15104, 17153, 9987, 12036, 409861, 275206, 11523, 12043, 102158, 114960, 307990, 11543, 11550, 117023, 10529, 13608, 9514, 11050, 33071, 9522, 11570, 10548, 20796, 10044, 9544, 9561, 9574, 1232234, 11116, 10106, 10112, 11140, 9095, 9099, 302478, 10648, 14232, 11168, 13730, 10659, 9639, 13742, 10159, 13746, 12213, 11190, 101815, 31159, 12215, 9660, 11198, 31679, 32191, 55748, 11210, 18895, 12239, 10193, 25052, 11745, 10722, 14306, 18404, 11241, 1199084, 15354, 11261}\n",
      "Number of unique embeddings with PNA: 120\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {136578, 55428, 166661, 65030, 1169674, 121229, 18082447, 17040, 18082449, 37649, 173435, 1156115, 165625, 1035924, 13298334, 503454, 13298335, 58782, 189470, 357923, 395681, 22437, 105502, 36263, 370600, 193064, 812076, 3433905, 1040051, 304307, 379321, 543424, 21953, 100673, 331330, 41543, 43208, 17867, 81996, 221901, 1938386, 1938387, 1036756, 49625, 1637728, 1637729, 5172066, 1476067, 5172067, 477281, 316897, 2019938, 62056, 250729, 41833, 38881387, 110699, 12452330, 12452332, 23655, 1006450, 189299, 1103478, 3380729, 1688187, 4590844}\n",
      "Number of unique embeddings with GIN: 66\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {47488, 18561, 54916, 26505, 8202, 15371, 47116, 18187, 39179, 16274, 45330, 11283, 39829, 27158, 312981, 8850, 52381, 25761, 8737, 28197, 23590, 12071, 24873, 9648, 10293, 11962, 27712, 40002, 11459, 100164, 12483, 47300, 64325, 91210, 6731, 487116, 13134, 40782, 11728, 15697, 9809, 97237, 59352, 21976, 16474, 136154, 15326, 10976, 925921, 14321, 345842, 12530, 18036, 79605, 11637, 9079, 7928, 129017, 9212, 14463}\n",
      "Number of unique embeddings with PNA: 60\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/GE_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {402946, 1657347, 22018, 330247, 64522, 624651, 156172, 6835745, 7509029, 31783, 693808, 1690685, 57407, 36421, 1889351, 173641, 185424, 683089, 14680149, 36952, 151641, 123486, 1940574, 966250, 186988, 176748, 126063, 31358, 5363332, 43653, 41093, 2346119, 270474, 4907666, 158360, 209564, 194723, 743592, 320689, 39090, 434370, 81091, 132295, 17611, 1137870, 41678, 63696, 833745, 38607, 281815, 26333, 44772, 53479, 34541, 189688, 442619, 75519, 47872, 53510, 107783, 157452, 829200, 136978, 39186, 90417, 627505, 120628, 140084, 73527, 476987, 687426, 215363, 576836, 39242, 395596, 67923, 149334, 264039, 192362, 42869, 53623, 213880, 645497, 120188, 52094, 754051, 79754, 598934, 689053, 1794462, 14135712, 68512, 609698, 731045, 461222, 534950, 249257, 44974, 1089967, 802739, 96180, 734645, 791994, 5030344, 188872, 24523, 5704652, 29132, 152531, 164308, 839123, 541139, 189399, 714712, 660442, 39898, 108004, 2389993}\n",
      "Number of unique embeddings with GIN: 118\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {43537, 12306, 2067, 23573, 32789, 238615, 164890, 28703, 130097, 3123, 71220, 81978, 505408, 80960, 72265, 13390, 2648, 12378, 11354, 311900, 57951, 55394, 22629, 76904, 29805, 11885, 164982, 71289, 56443, 18557, 54397, 20094, 10370, 49295, 51857, 70290, 14481, 17557, 128154, 19100, 8865, 5795, 148654, 32943, 54451, 9912, 42171, 319676, 31420, 502465, 115905, 107724, 260316, 33503, 232683, 44268, 33006, 67313, 12535, 46840, 23288, 26366, 259328, 36101, 76040, 7945, 84745, 118028, 147213, 25362, 5906, 5920, 15655, 54058, 12589, 57649, 30002, 18742, 36159, 22851, 27461, 27466, 36173, 86356, 15193, 81251, 9066, 16242, 108918, 87415, 53628, 24453, 112006, 17292, 25489, 15772, 13213, 31647, 35747, 6058, 31666, 9141, 48567, 3002, 63423, 13247, 33228, 7630, 19408, 84433, 10711, 17886, 9711, 49648, 6130, 20985, 28666, 86014}\n",
      "Number of unique embeddings with PNA: 118\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {151425, 1020674, 94084, 89867, 301195, 1976078, 2374032, 258833, 232339, 252566, 363034, 131103, 312480, 69151, 433571, 11442595, 8404645, 310439, 37159, 210857, 707754, 24403883, 494382, 535727, 3141295, 465969, 422704, 657076, 130741, 130233, 386105, 218939, 642748, 142526, 118079, 185663, 539970, 171337, 726346, 81099, 248908, 439374, 809550, 7779922, 7779923, 951892, 173144, 118372, 459110, 151016, 135147, 306029, 1158003, 540276, 263159, 2073846, 236279, 590712, 148857, 390899, 97918}\n",
      "Number of unique embeddings with GIN: 61\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {6915, 8326, 8968, 7050, 7051, 6666, 91661, 15250, 9362, 5650, 19864, 7071, 8864, 219938, 35108, 48676, 10022, 13222, 24745, 34346, 20522, 12202, 29229, 5933, 7217, 15921, 18099, 6334, 6847, 14150, 5319, 7879, 6729, 84809, 17743, 8019, 9947, 40412, 7261, 18782, 16480, 13280, 5472, 4960, 4708, 7394, 9830, 6500, 7274, 8172, 6637, 27374, 4846, 4845, 13427, 7796, 6901, 22646, 16246, 12026, 119163, 25213}\n",
      "Number of unique embeddings with PNA: 62\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/vn_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {11991808, 11991810, 88706, 352898, 623238, 414343, 2471686, 584588, 961804, 69518, 216847, 75022, 1692669, 325005, 276371, 276372, 1139607, 60057, 1437595, 518174, 518175, 79648, 638628, 45096, 40618, 313259, 395820, 22549293, 356017, 1108018, 48051, 49335, 1560509, 939709, 939710, 3947072, 370627, 474820, 9517896, 9517897, 125515, 62927, 1289552, 3757905, 1250131, 652631, 226520, 1028443, 1191260, 5139164, 864481, 622945, 9129064, 296169, 9129068, 55662, 57711, 506736, 999026, 265331, 741119, 81781, 856696, 53371, 286589, 741118, 1883007}\n",
      "Number of unique embeddings with GIN: 67\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {5632, 9472, 7301, 8829, 7817, 7178, 11019, 18828, 8205, 8591, 7953, 6162, 10005, 7957, 8091, 6814, 6817, 11426, 8100, 10789, 9765, 5553, 7221, 7606, 6454, 10683, 14651, 9405, 7230, 6977, 6337, 6595, 5196, 19789, 8398, 25293, 6733, 8017, 8654, 9168, 15707, 8926, 6624, 9057, 7648, 7650, 16102, 7403, 8685, 6895, 9456, 7153, 6386, 8052, 9591, 10872, 27385, 9979, 5628, 8445}\n",
      "Number of unique embeddings with PNA: 60\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/eig_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {134528, 715650, 715651, 327172, 151559, 929159, 735882, 181514, 3697804, 350605, 124305, 875283, 875284, 141078, 903191, 3369240, 204955, 267677, 44703, 145439, 161571, 230564, 140073, 1524394, 140074, 103215, 126258, 198325, 97589, 988730, 264637, 110270, 795839, 17786944, 135746, 161352, 17786952, 12762184, 206539, 1120843, 216907, 12762186, 39183060, 37848, 2887133, 83295, 92896, 91105, 109283, 239973, 269414, 160103, 13397482, 13397485, 779246, 252783, 410487, 111857, 3437298, 509300, 121844, 1512183, 206968, 895097, 54906, 250237, 250238}\n",
      "Number of unique embeddings with GIN: 67\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {20096, 10626, 193157, 12798, 146055, 8199, 27526, 27525, 6414, 6288, 66962, 6035, 346900, 7060, 5526, 7447, 40856, 18839, 7834, 5430, 18337, 72355, 8611, 14378, 6572, 33197, 11183, 6576, 14901, 5814, 58165, 6840, 7353, 43578, 39355, 7993, 5437, 11448, 5439, 24768, 5953, 21062, 10954, 8906, 5836, 9933, 131792, 24145, 5714, 6225, 7506, 57053, 16611, 14436, 6629, 6504, 6765, 6639, 13555, 18932, 10494}\n",
      "Number of unique embeddings with PNA: 61\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/SE_G_Basics_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {69249, 73477, 1152518, 46199431, 69512, 65544, 3261453, 2544397, 625039, 4473494, 113943, 4724247, 1721881, 194841, 86171, 60192, 165925, 194085, 462631, 58534, 58535, 1840940, 1187116, 170158, 1786544, 87861, 281399, 281400, 121145, 301626, 43067, 176700, 2545084, 111166, 17054783, 2436343, 78908, 134211, 188875, 53452, 2438479, 2438480, 35919, 407506, 378707, 377684, 377685, 58967, 528345, 131548, 5102045, 16369628, 5421023, 16369631, 52444, 2805730, 2104803, 215909, 151014, 234471, 68200, 22381932, 1704814, 22381936, 409079, 403576, 197497, 67834}\n",
      "Number of unique embeddings with GIN: 68\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {89346, 237704, 112521, 7176, 20104, 154764, 9353, 38541, 27276, 6152, 66845, 831262, 90147, 13219, 11685, 86949, 26662, 84515, 12841, 31146, 19881, 73772, 73773, 9388, 6444, 5421, 101554, 915123, 20281, 102330, 25403, 31676, 17853, 8254, 1354682, 20645, 5185, 69569, 6590, 7748, 26704, 5975, 9944, 5976, 11479, 4443, 19548, 207585, 8674, 36067, 6116, 10722, 24554, 14445, 28656, 249842, 73078, 11511, 3216632, 286073, 41086, 140924, 41085, 28158}\n",
      "Number of unique embeddings with PNA: 64\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/GE_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {61965, 922639, 36880, 38425, 32799, 344615, 199208, 54311, 45611, 1175084, 92720, 51253, 291899, 7941709, 46165, 132696, 7663196, 54365, 114273, 50273, 115812, 50280, 45167, 56433, 34930, 57970, 43638, 123511, 3318904, 27265, 200325, 323212, 108172, 104079, 82575, 40591, 94354, 82064, 65175, 63130, 70819, 83109, 3184301, 87727, 887987, 127667, 193719, 198855, 51401, 618704, 136918, 2043096, 1167582, 1999591, 167145, 156915, 1911038, 160518, 170763, 103184, 69396, 44314, 56093, 43293, 56095, 164136, 134446, 39214, 3116338, 98611, 106293, 296759, 135998, 1434943, 180544, 137541, 194888, 1509708, 52045, 1076562, 94550, 63319, 258908, 96604, 76133, 600424, 88425, 103275, 105838, 30063, 164727, 61305, 92041, 1939855, 1205664, 64930, 47526, 157608, 85420, 110508, 79281, 78774, 36802, 79303, 42440, 102347, 127951, 46036, 69080, 194523, 80867, 114152, 41960, 55274, 31212, 103416, 126973, 58367}\n",
      "Number of unique embeddings with GIN: 118\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {18951, 14358, 29209, 8742, 33320, 22060, 30252, 7212, 185401, 11322, 22588, 20541, 24130, 15429, 9802, 12878, 9307, 10337, 9318, 9835, 9842, 9331, 23675, 40574, 9863, 12951, 105624, 16536, 12954, 7834, 6808, 44192, 94886, 16039, 13480, 8877, 23728, 22705, 9907, 65204, 12469, 9908, 22710, 11960, 10425, 10936, 9404, 10943, 10946, 44226, 14539, 9931, 9429, 22746, 9947, 24289, 9955, 14058, 12530, 12534, 13561, 9466, 9471, 14601, 21771, 24343, 15640, 9498, 25375, 10019, 13603, 22310, 26415, 13616, 19247, 8517, 22855, 8525, 9038, 64334, 17753, 14173, 39266, 9572, 23398, 61800, 182633, 13674, 11115, 12141, 10099, 106871, 11647, 9604, 30085, 23948, 17805, 8597, 16285, 8607, 30624, 39331, 83363, 10665, 9131, 13744, 14772, 14775, 32708, 11229, 17890, 12774, 12779, 9198, 12789, 38902, 83960, 11775}\n",
      "Number of unique embeddings with PNA: 118\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/exN_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {125443, 76292, 402438, 115718, 90633, 69134, 74255, 79375, 94753, 86056, 134188, 57395, 86581, 59958, 79434, 65618, 54872, 70746, 58976, 96353, 122978, 106596, 128633, 82563, 56965, 110214, 75927, 55962, 64162, 277672, 83626, 89258, 81584, 113329, 62132, 114869, 71864, 137410, 225990, 97482, 94922, 114380, 116434, 160980, 82646, 111320, 53978, 72924, 110817, 64737, 51950, 171248, 403187, 90358, 66299, 80637, 80142, 53524, 213802, 74543, 61766, 64840, 70482, 47453, 85347, 111972, 74086, 53098, 128369, 108923, 105341, 149374, 75659, 81292, 67469, 77208, 99228, 62365, 64934, 145833, 120745, 107948, 96176, 116660, 128437, 132534, 75701, 79291, 98751, 109510, 88524, 187853, 120270, 82382, 124880, 282074, 160731, 90079, 74208, 75244, 56303, 89589, 111096}\n",
      "Number of unique embeddings with GIN: 103\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {19973, 23560, 9737, 9738, 21528, 10267, 12833, 9260, 14388, 10316, 11343, 10833, 12884, 9312, 9836, 13427, 9856, 10368, 21122, 12420, 14471, 10381, 14486, 11417, 19617, 10413, 12469, 16056, 16061, 12496, 12502, 9945, 10459, 10975, 13027, 15078, 16103, 15080, 19181, 9972, 17653, 11509, 10998, 10486, 12539, 11518, 9983, 12544, 13571, 11017, 15631, 10004, 24341, 16153, 10522, 11035, 11038, 13090, 10032, 11059, 11064, 23869, 12606, 10047, 11073, 10053, 10062, 16209, 21845, 11095, 9579, 9580, 10102, 14204, 10110, 14220, 20878, 13199, 11670, 17817, 11680, 11684, 13220, 10671, 10672, 23995, 12240, 12243, 12760, 11226, 10720, 10212, 11753, 10219, 11244, 10220, 18414, 18413, 9716, 15352, 11771}\n",
      "Number of unique embeddings with PNA: 101\n",
      "\n",
      "------------- Processing ../data/BREC/Basics/clo_G_Basics_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {10080769, 605192, 718350, 686098, 1169448, 1122347, 8354355, 1132085, 128056, 40587321, 635963, 1599039, 281272398, 498271, 128096, 1139300, 8383078, 890984, 1430633, 281272425, 1741817, 280706, 14501508, 1602180, 475787, 2643083, 1116302, 15226516, 15226517, 7483542, 1854114, 1624234, 1624237, 6890163, 6890164, 723123, 762552, 843449, 551111, 150732, 8343758, 2475220, 594645, 1007318, 599772, 710314719, 553701, 784108, 552178, 8685823, 47680787, 531731, 28856601, 958749, 176267049, 205103, 1459506, 28856628, 1459508, 1030462, 9410371, 40892229, 40892232, 29989202, 410962, 1312605, 1312607, 1448303, 1448304, 1455471, 522113, 1395596, 1490830, 10103183, 6622110, 6622112, 18474915, 18474925, 1052600, 582075, 426962, 154958804, 999898, 154958817, 2163685, 310760, 162283, 2937330, 1741816, 2937337}\n",
      "Number of unique embeddings with GIN: 90\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {17922, 11266, 10244, 7690, 11788, 7186, 7702, 21020, 13853, 8737, 10274, 132143, 11825, 57394, 24114, 11330, 8771, 25669, 12871, 15957, 10334, 12383, 14958, 175221, 7322, 22177, 19119, 13488, 37045, 9402, 33979, 57034, 8909, 13529, 8936, 10985, 16622, 10998, 11522, 7951, 10011, 11079, 13144, 10603, 10604, 12793, 17264, 12658, 10101, 11134, 38784, 9610, 12183, 11672, 13726, 10155, 121259, 9644, 14254, 8629, 14266, 36796, 305090, 13254, 12745, 10708, 10711, 11742, 10719, 10721, 12772, 7141, 15851, 14828, 21999, 57331, 11257, 11258}\n",
      "Number of unique embeddings with PNA: 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/Basics/*G_Basics_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71351b-4144-4a51-aa0c-93b1f9297d80",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8fd76936-12c8-4d64-824f-12beac8279e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Basics/deg_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 0.9999\n",
      "Epoch 4/100, Loss: 0.9999\n",
      "Epoch 5/100, Loss: 0.9999\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 0.9999\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 0.9999\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9999\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 0.9999\n",
      "Epoch 28/100, Loss: 0.9999\n",
      "Epoch 29/100, Loss: 0.9998\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9998\n",
      "Epoch 32/100, Loss: 0.9999\n",
      "Epoch 33/100, Loss: 0.9998\n",
      "Epoch 34/100, Loss: 0.9998\n",
      "Epoch 35/100, Loss: 0.9998\n",
      "Epoch 36/100, Loss: 0.9998\n",
      "Epoch 37/100, Loss: 0.9998\n",
      "Epoch 38/100, Loss: 0.9997\n",
      "Epoch 39/100, Loss: 0.9998\n",
      "Epoch 40/100, Loss: 0.9998\n",
      "Epoch 41/100, Loss: 0.9998\n",
      "Epoch 42/100, Loss: 0.9998\n",
      "Epoch 43/100, Loss: 0.9998\n",
      "Epoch 44/100, Loss: 0.9998\n",
      "Epoch 45/100, Loss: 0.9998\n",
      "Epoch 46/100, Loss: 0.9998\n",
      "Epoch 47/100, Loss: 0.9997\n",
      "Epoch 48/100, Loss: 0.9998\n",
      "Epoch 49/100, Loss: 0.9998\n",
      "Epoch 50/100, Loss: 0.9997\n",
      "Epoch 51/100, Loss: 0.9998\n",
      "Epoch 52/100, Loss: 0.9997\n",
      "Epoch 53/100, Loss: 0.9997\n",
      "Epoch 54/100, Loss: 0.9997\n",
      "Epoch 55/100, Loss: 0.9997\n",
      "Epoch 56/100, Loss: 0.9997\n",
      "Epoch 57/100, Loss: 0.9997\n",
      "Epoch 58/100, Loss: 0.9997\n",
      "Epoch 59/100, Loss: 0.9997\n",
      "Epoch 60/100, Loss: 0.9997\n",
      "Epoch 61/100, Loss: 0.9997\n",
      "Epoch 62/100, Loss: 0.9995\n",
      "Epoch 63/100, Loss: 0.9996\n",
      "Epoch 64/100, Loss: 0.9996\n",
      "Epoch 65/100, Loss: 0.9995\n",
      "Epoch 66/100, Loss: 0.9997\n",
      "Epoch 67/100, Loss: 0.9996\n",
      "Epoch 68/100, Loss: 0.9996\n",
      "Epoch 69/100, Loss: 0.9997\n",
      "Epoch 70/100, Loss: 0.9996\n",
      "Epoch 71/100, Loss: 0.9996\n",
      "Epoch 72/100, Loss: 0.9997\n",
      "Epoch 73/100, Loss: 0.9996\n",
      "Epoch 74/100, Loss: 0.9996\n",
      "Epoch 75/100, Loss: 0.9996\n",
      "Epoch 76/100, Loss: 0.9996\n",
      "Epoch 77/100, Loss: 0.9996\n",
      "Epoch 78/100, Loss: 0.9994\n",
      "Epoch 79/100, Loss: 0.9995\n",
      "Epoch 80/100, Loss: 0.9995\n",
      "Epoch 81/100, Loss: 0.9995\n",
      "Epoch 82/100, Loss: 0.9996\n",
      "Epoch 83/100, Loss: 0.9996\n",
      "Epoch 84/100, Loss: 0.9996\n",
      "Epoch 85/100, Loss: 0.9993\n",
      "Epoch 86/100, Loss: 0.9996\n",
      "Epoch 87/100, Loss: 0.9995\n",
      "Epoch 88/100, Loss: 0.9995\n",
      "Epoch 89/100, Loss: 0.9997\n",
      "Epoch 90/100, Loss: 0.9997\n",
      "Epoch 91/100, Loss: 0.9997\n",
      "Epoch 92/100, Loss: 0.9998\n",
      "Epoch 93/100, Loss: 0.9998\n",
      "Epoch 94/100, Loss: 0.9998\n",
      "Epoch 95/100, Loss: 0.9999\n",
      "Epoch 96/100, Loss: 0.9998\n",
      "Epoch 97/100, Loss: 0.9999\n",
      "Epoch 98/100, Loss: 0.9998\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9998\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 1.0000\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 1.0000\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 1.0000\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 1.0000\n",
      "Epoch 25/100, Loss: 1.0000\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 1.0000\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 0.9999\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 1.0000\n",
      "Epoch 33/100, Loss: 1.0000\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 1.0000\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 0.9999\n",
      "Epoch 40/100, Loss: 0.9999\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9999\n",
      "Epoch 45/100, Loss: 1.0000\n",
      "Epoch 46/100, Loss: 0.9999\n",
      "Epoch 47/100, Loss: 0.9999\n",
      "Epoch 48/100, Loss: 0.9999\n",
      "Epoch 49/100, Loss: 0.9999\n",
      "Epoch 50/100, Loss: 0.9999\n",
      "Epoch 51/100, Loss: 0.9999\n",
      "Epoch 52/100, Loss: 0.9999\n",
      "Epoch 53/100, Loss: 1.0000\n",
      "Epoch 54/100, Loss: 0.9999\n",
      "Epoch 55/100, Loss: 0.9999\n",
      "Epoch 56/100, Loss: 0.9999\n",
      "Epoch 57/100, Loss: 0.9999\n",
      "Epoch 58/100, Loss: 0.9999\n",
      "Epoch 59/100, Loss: 0.9999\n",
      "Epoch 60/100, Loss: 0.9999\n",
      "Epoch 61/100, Loss: 0.9999\n",
      "Epoch 62/100, Loss: 0.9999\n",
      "Epoch 63/100, Loss: 0.9999\n",
      "Epoch 64/100, Loss: 0.9999\n",
      "Epoch 65/100, Loss: 0.9999\n",
      "Epoch 66/100, Loss: 0.9999\n",
      "Epoch 67/100, Loss: 0.9999\n",
      "Epoch 68/100, Loss: 0.9999\n",
      "Epoch 69/100, Loss: 0.9999\n",
      "Epoch 70/100, Loss: 0.9999\n",
      "Epoch 71/100, Loss: 0.9999\n",
      "Epoch 72/100, Loss: 0.9999\n",
      "Epoch 73/100, Loss: 0.9999\n",
      "Epoch 74/100, Loss: 0.9999\n",
      "Epoch 75/100, Loss: 0.9999\n",
      "Epoch 76/100, Loss: 0.9999\n",
      "Epoch 77/100, Loss: 0.9999\n",
      "Epoch 78/100, Loss: 0.9999\n",
      "Epoch 79/100, Loss: 0.9999\n",
      "Epoch 80/100, Loss: 0.9998\n",
      "Epoch 81/100, Loss: 0.9999\n",
      "Epoch 82/100, Loss: 0.9998\n",
      "Epoch 83/100, Loss: 0.9999\n",
      "Epoch 84/100, Loss: 0.9998\n",
      "Epoch 85/100, Loss: 0.9999\n",
      "Epoch 86/100, Loss: 1.0000\n",
      "Epoch 87/100, Loss: 0.9999\n",
      "Epoch 88/100, Loss: 0.9999\n",
      "Epoch 89/100, Loss: 0.9999\n",
      "Epoch 90/100, Loss: 0.9998\n",
      "Epoch 91/100, Loss: 0.9999\n",
      "Epoch 92/100, Loss: 0.9999\n",
      "Epoch 93/100, Loss: 0.9998\n",
      "Epoch 94/100, Loss: 0.9999\n",
      "Epoch 95/100, Loss: 0.9998\n",
      "Epoch 96/100, Loss: 0.9999\n",
      "Epoch 97/100, Loss: 0.9998\n",
      "Epoch 98/100, Loss: 0.9999\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9998\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/DE_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.7259\n",
      "Epoch 2/100, Loss: 0.6667\n",
      "Epoch 3/100, Loss: 0.6382\n",
      "Epoch 4/100, Loss: 0.6068\n",
      "Epoch 5/100, Loss: 0.5803\n",
      "Epoch 6/100, Loss: 0.5514\n",
      "Epoch 7/100, Loss: 0.5276\n",
      "Epoch 8/100, Loss: 0.5033\n",
      "Epoch 9/100, Loss: 0.4754\n",
      "Epoch 10/100, Loss: 0.4511\n",
      "Epoch 11/100, Loss: 0.4230\n",
      "Epoch 12/100, Loss: 0.3999\n",
      "Epoch 13/100, Loss: 0.3766\n",
      "Epoch 14/100, Loss: 0.3578\n",
      "Epoch 15/100, Loss: 0.3325\n",
      "Epoch 16/100, Loss: 0.3095\n",
      "Epoch 17/100, Loss: 0.2923\n",
      "Epoch 18/100, Loss: 0.2700\n",
      "Epoch 19/100, Loss: 0.2482\n",
      "Epoch 20/100, Loss: 0.2299\n",
      "Epoch 21/100, Loss: 0.2080\n",
      "Epoch 22/100, Loss: 0.1794\n",
      "Epoch 23/100, Loss: 0.1617\n",
      "Epoch 24/100, Loss: 0.1388\n",
      "Epoch 25/100, Loss: 0.1223\n",
      "Epoch 26/100, Loss: 0.1050\n",
      "Epoch 27/100, Loss: 0.0907\n",
      "Epoch 28/100, Loss: 0.0761\n",
      "Epoch 29/100, Loss: 0.0633\n",
      "Epoch 30/100, Loss: 0.0527\n",
      "Epoch 31/100, Loss: 0.0441\n",
      "Epoch 32/100, Loss: 0.0377\n",
      "Epoch 33/100, Loss: 0.0322\n",
      "Epoch 34/100, Loss: 0.0279\n",
      "Epoch 35/100, Loss: 0.0249\n",
      "Epoch 36/100, Loss: 0.0217\n",
      "Epoch 37/100, Loss: 0.0188\n",
      "Epoch 38/100, Loss: 0.0161\n",
      "Epoch 39/100, Loss: 0.0141\n",
      "Epoch 40/100, Loss: 0.0124\n",
      "Epoch 41/100, Loss: 0.0102\n",
      "Epoch 42/100, Loss: 0.0088\n",
      "Epoch 43/100, Loss: 0.0072\n",
      "Epoch 44/100, Loss: 0.0059\n",
      "Epoch 45/100, Loss: 0.0048\n",
      "Epoch 46/100, Loss: 0.0039\n",
      "Epoch 47/100, Loss: 0.0033\n",
      "Epoch 48/100, Loss: 0.0027\n",
      "Epoch 49/100, Loss: 0.0023\n",
      "Epoch 50/100, Loss: 0.0021\n",
      "Epoch 51/100, Loss: 0.0018\n",
      "Epoch 52/100, Loss: 0.0017\n",
      "Epoch 53/100, Loss: 0.0015\n",
      "Epoch 54/100, Loss: 0.0014\n",
      "Epoch 55/100, Loss: 0.0013\n",
      "Epoch 56/100, Loss: 0.0012\n",
      "Epoch 57/100, Loss: 0.0011\n",
      "Epoch 58/100, Loss: 0.0010\n",
      "Epoch 59/100, Loss: 0.0009\n",
      "Epoch 60/100, Loss: 0.0008\n",
      "Epoch 61/100, Loss: 0.0008\n",
      "Epoch 62/100, Loss: 0.0007\n",
      "Epoch 63/100, Loss: 0.0007\n",
      "Epoch 64/100, Loss: 0.0006\n",
      "Epoch 65/100, Loss: 0.0006\n",
      "Epoch 66/100, Loss: 0.0005\n",
      "Epoch 67/100, Loss: 0.0005\n",
      "Epoch 68/100, Loss: 0.0005\n",
      "Epoch 69/100, Loss: 0.0004\n",
      "Epoch 70/100, Loss: 0.0004\n",
      "Epoch 71/100, Loss: 0.0004\n",
      "Epoch 72/100, Loss: 0.0003\n",
      "Epoch 73/100, Loss: 0.0003\n",
      "Epoch 74/100, Loss: 0.0003\n",
      "Epoch 75/100, Loss: 0.0003\n",
      "Epoch 76/100, Loss: 0.0002\n",
      "Epoch 77/100, Loss: 0.0002\n",
      "Epoch 78/100, Loss: 0.0002\n",
      "Epoch 79/100, Loss: 0.0002\n",
      "Epoch 80/100, Loss: 0.0002\n",
      "Epoch 81/100, Loss: 0.0001\n",
      "Epoch 82/100, Loss: 0.0001\n",
      "Epoch 83/100, Loss: 0.0001\n",
      "Epoch 84/100, Loss: 0.0001\n",
      "Epoch 85/100, Loss: 0.0001\n",
      "Epoch 86/100, Loss: 0.0001\n",
      "Epoch 87/100, Loss: 0.0001\n",
      "Epoch 88/100, Loss: 0.0001\n",
      "Epoch 89/100, Loss: 0.0001\n",
      "Epoch 90/100, Loss: 0.0001\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 9\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0120\n",
      "Epoch 2/100, Loss: 0.0121\n",
      "Epoch 3/100, Loss: 0.0193\n",
      "Epoch 4/100, Loss: 0.0075\n",
      "Epoch 5/100, Loss: 0.0055\n",
      "Epoch 6/100, Loss: 0.0071\n",
      "Epoch 7/100, Loss: 0.0070\n",
      "Epoch 8/100, Loss: 0.0066\n",
      "Epoch 9/100, Loss: 0.0060\n",
      "Epoch 10/100, Loss: 0.0054\n",
      "Epoch 11/100, Loss: 0.0051\n",
      "Epoch 12/100, Loss: 0.0041\n",
      "Epoch 13/100, Loss: 0.0036\n",
      "Epoch 14/100, Loss: 0.0031\n",
      "Epoch 15/100, Loss: 0.0028\n",
      "Epoch 16/100, Loss: 0.0023\n",
      "Epoch 17/100, Loss: 0.0019\n",
      "Epoch 18/100, Loss: 0.0016\n",
      "Epoch 19/100, Loss: 0.0012\n",
      "Epoch 20/100, Loss: 0.0008\n",
      "Epoch 21/100, Loss: 0.0007\n",
      "Epoch 22/100, Loss: 0.0005\n",
      "Epoch 23/100, Loss: 0.0003\n",
      "Epoch 24/100, Loss: 0.0002\n",
      "Epoch 25/100, Loss: 0.0001\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 16\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/bet_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.5719\n",
      "Epoch 2/100, Loss: 0.5362\n",
      "Epoch 3/100, Loss: 0.5009\n",
      "Epoch 4/100, Loss: 0.4698\n",
      "Epoch 5/100, Loss: 0.4370\n",
      "Epoch 6/100, Loss: 0.4083\n",
      "Epoch 7/100, Loss: 0.3738\n",
      "Epoch 8/100, Loss: 0.3414\n",
      "Epoch 9/100, Loss: 0.3148\n",
      "Epoch 10/100, Loss: 0.2860\n",
      "Epoch 11/100, Loss: 0.2585\n",
      "Epoch 12/100, Loss: 0.2325\n",
      "Epoch 13/100, Loss: 0.2029\n",
      "Epoch 14/100, Loss: 0.1744\n",
      "Epoch 15/100, Loss: 0.1493\n",
      "Epoch 16/100, Loss: 0.1283\n",
      "Epoch 17/100, Loss: 0.1116\n",
      "Epoch 18/100, Loss: 0.1001\n",
      "Epoch 19/100, Loss: 0.0866\n",
      "Epoch 20/100, Loss: 0.0725\n",
      "Epoch 21/100, Loss: 0.0594\n",
      "Epoch 22/100, Loss: 0.0529\n",
      "Epoch 23/100, Loss: 0.0418\n",
      "Epoch 24/100, Loss: 0.0360\n",
      "Epoch 25/100, Loss: 0.0283\n",
      "Epoch 26/100, Loss: 0.0229\n",
      "Epoch 27/100, Loss: 0.0194\n",
      "Epoch 28/100, Loss: 0.0161\n",
      "Epoch 29/100, Loss: 0.0145\n",
      "Epoch 30/100, Loss: 0.0113\n",
      "Epoch 31/100, Loss: 0.0095\n",
      "Epoch 32/100, Loss: 0.0076\n",
      "Epoch 33/100, Loss: 0.0062\n",
      "Epoch 34/100, Loss: 0.0052\n",
      "Epoch 35/100, Loss: 0.0044\n",
      "Epoch 36/100, Loss: 0.0037\n",
      "Epoch 37/100, Loss: 0.0030\n",
      "Epoch 38/100, Loss: 0.0023\n",
      "Epoch 39/100, Loss: 0.0016\n",
      "Epoch 40/100, Loss: 0.0012\n",
      "Epoch 41/100, Loss: 0.0010\n",
      "Epoch 42/100, Loss: 0.0008\n",
      "Epoch 43/100, Loss: 0.0006\n",
      "Epoch 44/100, Loss: 0.0004\n",
      "Epoch 45/100, Loss: 0.0003\n",
      "Epoch 46/100, Loss: 0.0002\n",
      "Epoch 47/100, Loss: 0.0002\n",
      "Epoch 48/100, Loss: 0.0001\n",
      "Epoch 49/100, Loss: 0.0001\n",
      "Epoch 50/100, Loss: 0.0001\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 26\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0491\n",
      "Epoch 2/100, Loss: 0.0369\n",
      "Epoch 3/100, Loss: 0.0312\n",
      "Epoch 4/100, Loss: 0.0247\n",
      "Epoch 5/100, Loss: 0.0208\n",
      "Epoch 6/100, Loss: 0.0177\n",
      "Epoch 7/100, Loss: 0.0142\n",
      "Epoch 8/100, Loss: 0.0113\n",
      "Epoch 9/100, Loss: 0.0097\n",
      "Epoch 10/100, Loss: 0.0080\n",
      "Epoch 11/100, Loss: 0.0064\n",
      "Epoch 12/100, Loss: 0.0051\n",
      "Epoch 13/100, Loss: 0.0041\n",
      "Epoch 14/100, Loss: 0.0034\n",
      "Epoch 15/100, Loss: 0.0030\n",
      "Epoch 16/100, Loss: 0.0026\n",
      "Epoch 17/100, Loss: 0.0021\n",
      "Epoch 18/100, Loss: 0.0018\n",
      "Epoch 19/100, Loss: 0.0015\n",
      "Epoch 20/100, Loss: 0.0013\n",
      "Epoch 21/100, Loss: 0.0010\n",
      "Epoch 22/100, Loss: 0.0008\n",
      "Epoch 23/100, Loss: 0.0006\n",
      "Epoch 24/100, Loss: 0.0005\n",
      "Epoch 25/100, Loss: 0.0004\n",
      "Epoch 26/100, Loss: 0.0003\n",
      "Epoch 27/100, Loss: 0.0003\n",
      "Epoch 28/100, Loss: 0.0002\n",
      "Epoch 29/100, Loss: 0.0002\n",
      "Epoch 30/100, Loss: 0.0001\n",
      "Epoch 31/100, Loss: 0.0001\n",
      "Epoch 32/100, Loss: 0.0001\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 22\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/SE_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.9496\n",
      "Epoch 2/100, Loss: 0.9380\n",
      "Epoch 3/100, Loss: 0.9277\n",
      "Epoch 4/100, Loss: 0.9158\n",
      "Epoch 5/100, Loss: 0.9039\n",
      "Epoch 6/100, Loss: 0.8879\n",
      "Epoch 7/100, Loss: 0.8648\n",
      "Epoch 8/100, Loss: 0.8512\n",
      "Epoch 9/100, Loss: 0.8324\n",
      "Epoch 10/100, Loss: 0.8081\n",
      "Epoch 11/100, Loss: 0.7724\n",
      "Epoch 12/100, Loss: 0.7206\n",
      "Epoch 13/100, Loss: 0.6799\n",
      "Epoch 14/100, Loss: 0.6293\n",
      "Epoch 15/100, Loss: 0.5698\n",
      "Epoch 16/100, Loss: 0.5102\n",
      "Epoch 17/100, Loss: 0.4787\n",
      "Epoch 18/100, Loss: 0.4123\n",
      "Epoch 19/100, Loss: 0.3601\n",
      "Epoch 20/100, Loss: 0.3018\n",
      "Epoch 21/100, Loss: 0.2463\n",
      "Epoch 22/100, Loss: 0.1949\n",
      "Epoch 23/100, Loss: 0.1576\n",
      "Epoch 24/100, Loss: 0.1150\n",
      "Epoch 25/100, Loss: 0.0921\n",
      "Epoch 26/100, Loss: 0.0633\n",
      "Epoch 27/100, Loss: 0.0398\n",
      "Epoch 28/100, Loss: 0.0310\n",
      "Epoch 29/100, Loss: 0.0249\n",
      "Epoch 30/100, Loss: 0.0151\n",
      "Epoch 31/100, Loss: 0.0127\n",
      "Epoch 32/100, Loss: 0.0146\n",
      "Epoch 33/100, Loss: 0.0076\n",
      "Epoch 34/100, Loss: 0.0040\n",
      "Epoch 35/100, Loss: 0.0027\n",
      "Epoch 36/100, Loss: 0.0017\n",
      "Epoch 37/100, Loss: 0.0014\n",
      "Epoch 38/100, Loss: 0.0014\n",
      "Epoch 39/100, Loss: 0.0015\n",
      "Epoch 40/100, Loss: 0.0010\n",
      "Epoch 41/100, Loss: 0.0007\n",
      "Epoch 42/100, Loss: 0.0004\n",
      "Epoch 43/100, Loss: 0.0003\n",
      "Epoch 44/100, Loss: 0.0002\n",
      "Epoch 45/100, Loss: 0.0001\n",
      "Epoch 46/100, Loss: 0.0001\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 2\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.8321\n",
      "Epoch 2/100, Loss: 0.8208\n",
      "Epoch 3/100, Loss: 0.8163\n",
      "Epoch 4/100, Loss: 0.8065\n",
      "Epoch 5/100, Loss: 0.8014\n",
      "Epoch 6/100, Loss: 0.7960\n",
      "Epoch 7/100, Loss: 0.7894\n",
      "Epoch 8/100, Loss: 0.7804\n",
      "Epoch 9/100, Loss: 0.7733\n",
      "Epoch 10/100, Loss: 0.7675\n",
      "Epoch 11/100, Loss: 0.7592\n",
      "Epoch 12/100, Loss: 0.7517\n",
      "Epoch 13/100, Loss: 0.7439\n",
      "Epoch 14/100, Loss: 0.7356\n",
      "Epoch 15/100, Loss: 0.7268\n",
      "Epoch 16/100, Loss: 0.7183\n",
      "Epoch 17/100, Loss: 0.7107\n",
      "Epoch 18/100, Loss: 0.7028\n",
      "Epoch 19/100, Loss: 0.6948\n",
      "Epoch 20/100, Loss: 0.6883\n",
      "Epoch 21/100, Loss: 0.6810\n",
      "Epoch 22/100, Loss: 0.6745\n",
      "Epoch 23/100, Loss: 0.6666\n",
      "Epoch 24/100, Loss: 0.6592\n",
      "Epoch 25/100, Loss: 0.6497\n",
      "Epoch 26/100, Loss: 0.6408\n",
      "Epoch 27/100, Loss: 0.6326\n",
      "Epoch 28/100, Loss: 0.6262\n",
      "Epoch 29/100, Loss: 0.6191\n",
      "Epoch 30/100, Loss: 0.6113\n",
      "Epoch 31/100, Loss: 0.6042\n",
      "Epoch 32/100, Loss: 0.5971\n",
      "Epoch 33/100, Loss: 0.5899\n",
      "Epoch 34/100, Loss: 0.5834\n",
      "Epoch 35/100, Loss: 0.5760\n",
      "Epoch 36/100, Loss: 0.5693\n",
      "Epoch 37/100, Loss: 0.5637\n",
      "Epoch 38/100, Loss: 0.5580\n",
      "Epoch 39/100, Loss: 0.5523\n",
      "Epoch 40/100, Loss: 0.5464\n",
      "Epoch 41/100, Loss: 0.5417\n",
      "Epoch 42/100, Loss: 0.5351\n",
      "Epoch 43/100, Loss: 0.5278\n",
      "Epoch 44/100, Loss: 0.5228\n",
      "Epoch 45/100, Loss: 0.5164\n",
      "Epoch 46/100, Loss: 0.5101\n",
      "Epoch 47/100, Loss: 0.5051\n",
      "Epoch 48/100, Loss: 0.4993\n",
      "Epoch 49/100, Loss: 0.4939\n",
      "Epoch 50/100, Loss: 0.4892\n",
      "Epoch 51/100, Loss: 0.4823\n",
      "Epoch 52/100, Loss: 0.4749\n",
      "Epoch 53/100, Loss: 0.4690\n",
      "Epoch 54/100, Loss: 0.4648\n",
      "Epoch 55/100, Loss: 0.4599\n",
      "Epoch 56/100, Loss: 0.4520\n",
      "Epoch 57/100, Loss: 0.4473\n",
      "Epoch 58/100, Loss: 0.4389\n",
      "Epoch 59/100, Loss: 0.4337\n",
      "Epoch 60/100, Loss: 0.4288\n",
      "Epoch 61/100, Loss: 0.4222\n",
      "Epoch 62/100, Loss: 0.4179\n",
      "Epoch 63/100, Loss: 0.4131\n",
      "Epoch 64/100, Loss: 0.4088\n",
      "Epoch 65/100, Loss: 0.4043\n",
      "Epoch 66/100, Loss: 0.4006\n",
      "Epoch 67/100, Loss: 0.3962\n",
      "Epoch 68/100, Loss: 0.3922\n",
      "Epoch 69/100, Loss: 0.3880\n",
      "Epoch 70/100, Loss: 0.3838\n",
      "Epoch 71/100, Loss: 0.3797\n",
      "Epoch 72/100, Loss: 0.3756\n",
      "Epoch 73/100, Loss: 0.3711\n",
      "Epoch 74/100, Loss: 0.3674\n",
      "Epoch 75/100, Loss: 0.3622\n",
      "Epoch 76/100, Loss: 0.3584\n",
      "Epoch 77/100, Loss: 0.3546\n",
      "Epoch 78/100, Loss: 0.3517\n",
      "Epoch 79/100, Loss: 0.3477\n",
      "Epoch 80/100, Loss: 0.3447\n",
      "Epoch 81/100, Loss: 0.3407\n",
      "Epoch 82/100, Loss: 0.3367\n",
      "Epoch 83/100, Loss: 0.3345\n",
      "Epoch 84/100, Loss: 0.3308\n",
      "Epoch 85/100, Loss: 0.3269\n",
      "Epoch 86/100, Loss: 0.3233\n",
      "Epoch 87/100, Loss: 0.3198\n",
      "Epoch 88/100, Loss: 0.3160\n",
      "Epoch 89/100, Loss: 0.3127\n",
      "Epoch 90/100, Loss: 0.3063\n",
      "Epoch 91/100, Loss: 0.3029\n",
      "Epoch 92/100, Loss: 0.2997\n",
      "Epoch 93/100, Loss: 0.2944\n",
      "Epoch 94/100, Loss: 0.2913\n",
      "Epoch 95/100, Loss: 0.2872\n",
      "Epoch 96/100, Loss: 0.2841\n",
      "Epoch 97/100, Loss: 0.2797\n",
      "Epoch 98/100, Loss: 0.2766\n",
      "Epoch 99/100, Loss: 0.2729\n",
      "Epoch 100/100, Loss: 0.2704\n",
      "Correctly classified non-isomorphic pairs: 2\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/gle_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.3937\n",
      "Epoch 2/100, Loss: 0.3203\n",
      "Epoch 3/100, Loss: 0.2396\n",
      "Epoch 4/100, Loss: 0.1862\n",
      "Epoch 5/100, Loss: 0.1708\n",
      "Epoch 6/100, Loss: 0.1426\n",
      "Epoch 7/100, Loss: 0.0989\n",
      "Epoch 8/100, Loss: 0.0723\n",
      "Epoch 9/100, Loss: 0.0601\n",
      "Epoch 10/100, Loss: 0.0442\n",
      "Epoch 11/100, Loss: 0.0327\n",
      "Epoch 12/100, Loss: 0.0269\n",
      "Epoch 13/100, Loss: 0.0221\n",
      "Epoch 14/100, Loss: 0.0169\n",
      "Epoch 15/100, Loss: 0.0132\n",
      "Epoch 16/100, Loss: 0.0107\n",
      "Epoch 17/100, Loss: 0.0090\n",
      "Epoch 18/100, Loss: 0.0076\n",
      "Epoch 19/100, Loss: 0.0058\n",
      "Epoch 20/100, Loss: 0.0040\n",
      "Epoch 21/100, Loss: 0.0028\n",
      "Epoch 22/100, Loss: 0.0023\n",
      "Epoch 23/100, Loss: 0.0018\n",
      "Epoch 24/100, Loss: 0.0013\n",
      "Epoch 25/100, Loss: 0.0009\n",
      "Epoch 26/100, Loss: 0.0005\n",
      "Epoch 27/100, Loss: 0.0003\n",
      "Epoch 28/100, Loss: 0.0002\n",
      "Epoch 29/100, Loss: 0.0001\n",
      "Epoch 30/100, Loss: 0.0001\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 29\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.1340\n",
      "Epoch 2/100, Loss: 0.1066\n",
      "Epoch 3/100, Loss: 0.0932\n",
      "Epoch 4/100, Loss: 0.0845\n",
      "Epoch 5/100, Loss: 0.0783\n",
      "Epoch 6/100, Loss: 0.0704\n",
      "Epoch 7/100, Loss: 0.0632\n",
      "Epoch 8/100, Loss: 0.0569\n",
      "Epoch 9/100, Loss: 0.0506\n",
      "Epoch 10/100, Loss: 0.0436\n",
      "Epoch 11/100, Loss: 0.0370\n",
      "Epoch 12/100, Loss: 0.0326\n",
      "Epoch 13/100, Loss: 0.0292\n",
      "Epoch 14/100, Loss: 0.0270\n",
      "Epoch 15/100, Loss: 0.0246\n",
      "Epoch 16/100, Loss: 0.0221\n",
      "Epoch 17/100, Loss: 0.0202\n",
      "Epoch 18/100, Loss: 0.0186\n",
      "Epoch 19/100, Loss: 0.0170\n",
      "Epoch 20/100, Loss: 0.0158\n",
      "Epoch 21/100, Loss: 0.0146\n",
      "Epoch 22/100, Loss: 0.0136\n",
      "Epoch 23/100, Loss: 0.0127\n",
      "Epoch 24/100, Loss: 0.0118\n",
      "Epoch 25/100, Loss: 0.0110\n",
      "Epoch 26/100, Loss: 0.0103\n",
      "Epoch 27/100, Loss: 0.0096\n",
      "Epoch 28/100, Loss: 0.0090\n",
      "Epoch 29/100, Loss: 0.0084\n",
      "Epoch 30/100, Loss: 0.0078\n",
      "Epoch 31/100, Loss: 0.0072\n",
      "Epoch 32/100, Loss: 0.0065\n",
      "Epoch 33/100, Loss: 0.0059\n",
      "Epoch 34/100, Loss: 0.0054\n",
      "Epoch 35/100, Loss: 0.0048\n",
      "Epoch 36/100, Loss: 0.0043\n",
      "Epoch 37/100, Loss: 0.0038\n",
      "Epoch 38/100, Loss: 0.0033\n",
      "Epoch 39/100, Loss: 0.0029\n",
      "Epoch 40/100, Loss: 0.0025\n",
      "Epoch 41/100, Loss: 0.0022\n",
      "Epoch 42/100, Loss: 0.0018\n",
      "Epoch 43/100, Loss: 0.0016\n",
      "Epoch 44/100, Loss: 0.0013\n",
      "Epoch 45/100, Loss: 0.0011\n",
      "Epoch 46/100, Loss: 0.0010\n",
      "Epoch 47/100, Loss: 0.0008\n",
      "Epoch 48/100, Loss: 0.0007\n",
      "Epoch 49/100, Loss: 0.0006\n",
      "Epoch 50/100, Loss: 0.0005\n",
      "Epoch 51/100, Loss: 0.0005\n",
      "Epoch 52/100, Loss: 0.0005\n",
      "Epoch 53/100, Loss: 0.0005\n",
      "Epoch 54/100, Loss: 0.0003\n",
      "Epoch 55/100, Loss: 0.0004\n",
      "Epoch 56/100, Loss: 0.0003\n",
      "Epoch 57/100, Loss: 0.0003\n",
      "Epoch 58/100, Loss: 0.0002\n",
      "Epoch 59/100, Loss: 0.0002\n",
      "Epoch 60/100, Loss: 0.0002\n",
      "Epoch 61/100, Loss: 0.0001\n",
      "Epoch 62/100, Loss: 0.0001\n",
      "Epoch 63/100, Loss: 0.0001\n",
      "Epoch 64/100, Loss: 0.0001\n",
      "Epoch 65/100, Loss: 0.0001\n",
      "Epoch 66/100, Loss: 0.0001\n",
      "Epoch 67/100, Loss: 0.0001\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 34\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 0.9999\n",
      "Epoch 5/100, Loss: 0.9999\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 0.9999\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 0.9999\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9999\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9998\n",
      "Epoch 26/100, Loss: 0.9998\n",
      "Epoch 27/100, Loss: 0.9998\n",
      "Epoch 28/100, Loss: 0.9998\n",
      "Epoch 29/100, Loss: 0.9998\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 0.9998\n",
      "Epoch 33/100, Loss: 0.9998\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 0.9998\n",
      "Epoch 37/100, Loss: 0.9998\n",
      "Epoch 38/100, Loss: 0.9998\n",
      "Epoch 39/100, Loss: 0.9999\n",
      "Epoch 40/100, Loss: 0.9998\n",
      "Epoch 41/100, Loss: 0.9998\n",
      "Epoch 42/100, Loss: 0.9998\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9998\n",
      "Epoch 45/100, Loss: 0.9998\n",
      "Epoch 46/100, Loss: 0.9998\n",
      "Epoch 47/100, Loss: 0.9998\n",
      "Epoch 48/100, Loss: 0.9998\n",
      "Epoch 49/100, Loss: 0.9998\n",
      "Epoch 50/100, Loss: 0.9998\n",
      "Epoch 51/100, Loss: 0.9998\n",
      "Epoch 52/100, Loss: 0.9998\n",
      "Epoch 53/100, Loss: 0.9998\n",
      "Epoch 54/100, Loss: 0.9998\n",
      "Epoch 55/100, Loss: 0.9998\n",
      "Epoch 56/100, Loss: 0.9998\n",
      "Epoch 57/100, Loss: 0.9998\n",
      "Epoch 58/100, Loss: 0.9997\n",
      "Epoch 59/100, Loss: 0.9998\n",
      "Epoch 60/100, Loss: 0.9998\n",
      "Epoch 61/100, Loss: 0.9998\n",
      "Epoch 62/100, Loss: 0.9998\n",
      "Epoch 63/100, Loss: 0.9998\n",
      "Epoch 64/100, Loss: 0.9998\n",
      "Epoch 65/100, Loss: 0.9997\n",
      "Epoch 66/100, Loss: 0.9997\n",
      "Epoch 67/100, Loss: 0.9997\n",
      "Epoch 68/100, Loss: 0.9996\n",
      "Epoch 69/100, Loss: 0.9997\n",
      "Epoch 70/100, Loss: 0.9998\n",
      "Epoch 71/100, Loss: 0.9996\n",
      "Epoch 72/100, Loss: 0.9998\n",
      "Epoch 73/100, Loss: 0.9996\n",
      "Epoch 74/100, Loss: 0.9997\n",
      "Epoch 75/100, Loss: 0.9997\n",
      "Epoch 76/100, Loss: 0.9997\n",
      "Epoch 77/100, Loss: 0.9997\n",
      "Epoch 78/100, Loss: 0.9997\n",
      "Epoch 79/100, Loss: 0.9996\n",
      "Epoch 80/100, Loss: 0.9996\n",
      "Epoch 81/100, Loss: 0.9997\n",
      "Epoch 82/100, Loss: 0.9997\n",
      "Epoch 83/100, Loss: 0.9997\n",
      "Epoch 84/100, Loss: 0.9997\n",
      "Epoch 85/100, Loss: 0.9997\n",
      "Epoch 86/100, Loss: 0.9997\n",
      "Epoch 87/100, Loss: 0.9995\n",
      "Epoch 88/100, Loss: 0.9996\n",
      "Epoch 89/100, Loss: 0.9995\n",
      "Epoch 90/100, Loss: 0.9996\n",
      "Epoch 91/100, Loss: 0.9994\n",
      "Epoch 92/100, Loss: 0.9996\n",
      "Epoch 93/100, Loss: 0.9995\n",
      "Epoch 94/100, Loss: 0.9996\n",
      "Epoch 95/100, Loss: 0.9996\n",
      "Epoch 96/100, Loss: 0.9995\n",
      "Epoch 97/100, Loss: 0.9993\n",
      "Epoch 98/100, Loss: 0.9994\n",
      "Epoch 99/100, Loss: 0.9996\n",
      "Epoch 100/100, Loss: 0.9997\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 0.9999\n",
      "Epoch 6/100, Loss: 0.9999\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 0.9999\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 1.0000\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 1.0000\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 1.0000\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 1.0000\n",
      "Epoch 27/100, Loss: 1.0000\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 1.0000\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 1.0000\n",
      "Epoch 32/100, Loss: 1.0000\n",
      "Epoch 33/100, Loss: 0.9999\n",
      "Epoch 34/100, Loss: 1.0000\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 1.0000\n",
      "Epoch 40/100, Loss: 1.0000\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9999\n",
      "Epoch 45/100, Loss: 0.9999\n",
      "Epoch 46/100, Loss: 1.0000\n",
      "Epoch 47/100, Loss: 0.9999\n",
      "Epoch 48/100, Loss: 1.0000\n",
      "Epoch 49/100, Loss: 1.0000\n",
      "Epoch 50/100, Loss: 1.0000\n",
      "Epoch 51/100, Loss: 1.0000\n",
      "Epoch 52/100, Loss: 1.0000\n",
      "Epoch 53/100, Loss: 1.0000\n",
      "Epoch 54/100, Loss: 1.0000\n",
      "Epoch 55/100, Loss: 1.0000\n",
      "Epoch 56/100, Loss: 0.9999\n",
      "Epoch 57/100, Loss: 1.0000\n",
      "Epoch 58/100, Loss: 1.0000\n",
      "Epoch 59/100, Loss: 1.0000\n",
      "Epoch 60/100, Loss: 0.9999\n",
      "Epoch 61/100, Loss: 1.0000\n",
      "Epoch 62/100, Loss: 0.9999\n",
      "Epoch 63/100, Loss: 0.9999\n",
      "Epoch 64/100, Loss: 0.9999\n",
      "Epoch 65/100, Loss: 0.9999\n",
      "Epoch 66/100, Loss: 1.0000\n",
      "Epoch 67/100, Loss: 1.0000\n",
      "Epoch 68/100, Loss: 0.9999\n",
      "Epoch 69/100, Loss: 1.0000\n",
      "Epoch 70/100, Loss: 0.9999\n",
      "Epoch 71/100, Loss: 0.9999\n",
      "Epoch 72/100, Loss: 1.0000\n",
      "Epoch 73/100, Loss: 0.9999\n",
      "Epoch 74/100, Loss: 1.0000\n",
      "Epoch 75/100, Loss: 0.9999\n",
      "Epoch 76/100, Loss: 0.9999\n",
      "Epoch 77/100, Loss: 0.9999\n",
      "Epoch 78/100, Loss: 0.9999\n",
      "Epoch 79/100, Loss: 0.9999\n",
      "Epoch 80/100, Loss: 1.0000\n",
      "Epoch 81/100, Loss: 1.0000\n",
      "Epoch 82/100, Loss: 1.0000\n",
      "Epoch 83/100, Loss: 0.9999\n",
      "Epoch 84/100, Loss: 0.9999\n",
      "Epoch 85/100, Loss: 1.0000\n",
      "Epoch 86/100, Loss: 1.0000\n",
      "Epoch 87/100, Loss: 0.9999\n",
      "Epoch 88/100, Loss: 0.9999\n",
      "Epoch 89/100, Loss: 0.9999\n",
      "Epoch 90/100, Loss: 0.9999\n",
      "Epoch 91/100, Loss: 0.9999\n",
      "Epoch 92/100, Loss: 1.0000\n",
      "Epoch 93/100, Loss: 1.0000\n",
      "Epoch 94/100, Loss: 0.9999\n",
      "Epoch 95/100, Loss: 1.0000\n",
      "Epoch 96/100, Loss: 1.0000\n",
      "Epoch 97/100, Loss: 0.9999\n",
      "Epoch 98/100, Loss: 0.9999\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9999\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/vn_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 0.9999\n",
      "Epoch 5/100, Loss: 0.9999\n",
      "Epoch 6/100, Loss: 0.9999\n",
      "Epoch 7/100, Loss: 0.9999\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 0.9998\n",
      "Epoch 13/100, Loss: 0.9998\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9998\n",
      "Epoch 17/100, Loss: 0.9997\n",
      "Epoch 18/100, Loss: 0.9998\n",
      "Epoch 19/100, Loss: 0.9998\n",
      "Epoch 20/100, Loss: 0.9998\n",
      "Epoch 21/100, Loss: 0.9998\n",
      "Epoch 22/100, Loss: 0.9998\n",
      "Epoch 23/100, Loss: 0.9998\n",
      "Epoch 24/100, Loss: 0.9995\n",
      "Epoch 25/100, Loss: 0.9997\n",
      "Epoch 26/100, Loss: 0.9994\n",
      "Epoch 27/100, Loss: 0.9992\n",
      "Epoch 28/100, Loss: 0.9995\n",
      "Epoch 29/100, Loss: 0.9990\n",
      "Epoch 30/100, Loss: 0.9994\n",
      "Epoch 31/100, Loss: 0.9995\n",
      "Epoch 32/100, Loss: 0.9988\n",
      "Epoch 33/100, Loss: 0.9992\n",
      "Epoch 34/100, Loss: 0.9988\n",
      "Epoch 35/100, Loss: 0.9986\n",
      "Epoch 36/100, Loss: 0.9972\n",
      "Epoch 37/100, Loss: 0.9989\n",
      "Epoch 38/100, Loss: 0.9987\n",
      "Epoch 39/100, Loss: 0.9981\n",
      "Epoch 40/100, Loss: 0.9985\n",
      "Epoch 41/100, Loss: 0.9980\n",
      "Epoch 42/100, Loss: 0.9968\n",
      "Epoch 43/100, Loss: 0.9933\n",
      "Epoch 44/100, Loss: 0.9959\n",
      "Epoch 45/100, Loss: 0.9977\n",
      "Epoch 46/100, Loss: 0.9967\n",
      "Epoch 47/100, Loss: 0.9968\n",
      "Epoch 48/100, Loss: 0.9968\n",
      "Epoch 49/100, Loss: 0.9985\n",
      "Epoch 50/100, Loss: 0.9959\n",
      "Epoch 51/100, Loss: 0.9963\n",
      "Epoch 52/100, Loss: 0.9931\n",
      "Epoch 53/100, Loss: 0.9961\n",
      "Epoch 54/100, Loss: 0.9918\n",
      "Epoch 55/100, Loss: 0.9915\n",
      "Epoch 56/100, Loss: 0.9930\n",
      "Epoch 57/100, Loss: 0.9941\n",
      "Epoch 58/100, Loss: 0.9955\n",
      "Epoch 59/100, Loss: 0.9975\n",
      "Epoch 60/100, Loss: 0.9979\n",
      "Epoch 61/100, Loss: 0.9992\n",
      "Epoch 62/100, Loss: 0.9989\n",
      "Epoch 63/100, Loss: 0.9992\n",
      "Epoch 64/100, Loss: 0.9991\n",
      "Epoch 65/100, Loss: 0.9995\n",
      "Epoch 66/100, Loss: 0.9991\n",
      "Epoch 67/100, Loss: 0.9989\n",
      "Epoch 68/100, Loss: 0.9991\n",
      "Epoch 69/100, Loss: 0.9987\n",
      "Epoch 70/100, Loss: 0.9989\n",
      "Epoch 71/100, Loss: 0.9990\n",
      "Epoch 72/100, Loss: 0.9989\n",
      "Epoch 73/100, Loss: 0.9983\n",
      "Epoch 74/100, Loss: 0.9995\n",
      "Epoch 75/100, Loss: 0.9991\n",
      "Epoch 76/100, Loss: 0.9987\n",
      "Epoch 77/100, Loss: 0.9990\n",
      "Epoch 78/100, Loss: 0.9982\n",
      "Epoch 79/100, Loss: 0.9979\n",
      "Epoch 80/100, Loss: 0.9986\n",
      "Epoch 81/100, Loss: 0.9974\n",
      "Epoch 82/100, Loss: 0.9981\n",
      "Epoch 83/100, Loss: 0.9974\n",
      "Epoch 84/100, Loss: 0.9988\n",
      "Epoch 85/100, Loss: 0.9986\n",
      "Epoch 86/100, Loss: 0.9985\n",
      "Epoch 87/100, Loss: 0.9982\n",
      "Epoch 88/100, Loss: 0.9985\n",
      "Epoch 89/100, Loss: 0.9983\n",
      "Epoch 90/100, Loss: 0.9987\n",
      "Epoch 91/100, Loss: 0.9991\n",
      "Epoch 92/100, Loss: 0.9991\n",
      "Epoch 93/100, Loss: 0.9995\n",
      "Epoch 94/100, Loss: 0.9992\n",
      "Epoch 95/100, Loss: 0.9990\n",
      "Epoch 96/100, Loss: 0.9977\n",
      "Epoch 97/100, Loss: 0.9974\n",
      "Epoch 98/100, Loss: 0.9984\n",
      "Epoch 99/100, Loss: 0.9986\n",
      "Epoch 100/100, Loss: 0.9983\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.9999\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 0.9999\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 0.9999\n",
      "Epoch 13/100, Loss: 1.0000\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9999\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 1.0000\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 0.9999\n",
      "Epoch 28/100, Loss: 0.9998\n",
      "Epoch 29/100, Loss: 0.9999\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 0.9999\n",
      "Epoch 33/100, Loss: 0.9999\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 0.9999\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 0.9999\n",
      "Epoch 40/100, Loss: 0.9998\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9998\n",
      "Epoch 44/100, Loss: 0.9998\n",
      "Epoch 45/100, Loss: 0.9999\n",
      "Epoch 46/100, Loss: 0.9998\n",
      "Epoch 47/100, Loss: 0.9998\n",
      "Epoch 48/100, Loss: 0.9998\n",
      "Epoch 49/100, Loss: 0.9999\n",
      "Epoch 50/100, Loss: 0.9999\n",
      "Epoch 51/100, Loss: 0.9998\n",
      "Epoch 52/100, Loss: 0.9998\n",
      "Epoch 53/100, Loss: 0.9999\n",
      "Epoch 54/100, Loss: 0.9998\n",
      "Epoch 55/100, Loss: 0.9998\n",
      "Epoch 56/100, Loss: 0.9999\n",
      "Epoch 57/100, Loss: 0.9998\n",
      "Epoch 58/100, Loss: 0.9999\n",
      "Epoch 59/100, Loss: 0.9999\n",
      "Epoch 60/100, Loss: 0.9999\n",
      "Epoch 61/100, Loss: 0.9999\n",
      "Epoch 62/100, Loss: 0.9999\n",
      "Epoch 63/100, Loss: 0.9998\n",
      "Epoch 64/100, Loss: 0.9998\n",
      "Epoch 65/100, Loss: 0.9998\n",
      "Epoch 66/100, Loss: 0.9998\n",
      "Epoch 67/100, Loss: 0.9998\n",
      "Epoch 68/100, Loss: 0.9999\n",
      "Epoch 69/100, Loss: 0.9999\n",
      "Epoch 70/100, Loss: 0.9999\n",
      "Epoch 71/100, Loss: 0.9999\n",
      "Epoch 72/100, Loss: 0.9999\n",
      "Epoch 73/100, Loss: 0.9999\n",
      "Epoch 74/100, Loss: 0.9999\n",
      "Epoch 75/100, Loss: 0.9999\n",
      "Epoch 76/100, Loss: 0.9999\n",
      "Epoch 77/100, Loss: 0.9999\n",
      "Epoch 78/100, Loss: 0.9999\n",
      "Epoch 79/100, Loss: 0.9999\n",
      "Epoch 80/100, Loss: 0.9999\n",
      "Epoch 81/100, Loss: 0.9999\n",
      "Epoch 82/100, Loss: 0.9999\n",
      "Epoch 83/100, Loss: 0.9999\n",
      "Epoch 84/100, Loss: 0.9999\n",
      "Epoch 85/100, Loss: 0.9999\n",
      "Epoch 86/100, Loss: 0.9999\n",
      "Epoch 87/100, Loss: 0.9999\n",
      "Epoch 88/100, Loss: 0.9999\n",
      "Epoch 89/100, Loss: 0.9999\n",
      "Epoch 90/100, Loss: 0.9998\n",
      "Epoch 91/100, Loss: 0.9999\n",
      "Epoch 92/100, Loss: 0.9999\n",
      "Epoch 93/100, Loss: 0.9999\n",
      "Epoch 94/100, Loss: 0.9999\n",
      "Epoch 95/100, Loss: 0.9999\n",
      "Epoch 96/100, Loss: 0.9998\n",
      "Epoch 97/100, Loss: 0.9999\n",
      "Epoch 98/100, Loss: 0.9999\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9999\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/eig_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 0.9999\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 0.9999\n",
      "Epoch 6/100, Loss: 0.9999\n",
      "Epoch 7/100, Loss: 0.9999\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 0.9999\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 0.9999\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9999\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 0.9999\n",
      "Epoch 28/100, Loss: 0.9999\n",
      "Epoch 29/100, Loss: 0.9999\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 0.9999\n",
      "Epoch 33/100, Loss: 0.9999\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 0.9999\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 0.9999\n",
      "Epoch 40/100, Loss: 0.9999\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9998\n",
      "Epoch 45/100, Loss: 0.9998\n",
      "Epoch 46/100, Loss: 0.9998\n",
      "Epoch 47/100, Loss: 0.9999\n",
      "Epoch 48/100, Loss: 0.9998\n",
      "Epoch 49/100, Loss: 0.9999\n",
      "Epoch 50/100, Loss: 0.9998\n",
      "Epoch 51/100, Loss: 0.9998\n",
      "Epoch 52/100, Loss: 0.9997\n",
      "Epoch 53/100, Loss: 0.9998\n",
      "Epoch 54/100, Loss: 0.9997\n",
      "Epoch 55/100, Loss: 0.9997\n",
      "Epoch 56/100, Loss: 0.9997\n",
      "Epoch 57/100, Loss: 0.9997\n",
      "Epoch 58/100, Loss: 0.9997\n",
      "Epoch 59/100, Loss: 0.9998\n",
      "Epoch 60/100, Loss: 0.9997\n",
      "Epoch 61/100, Loss: 0.9997\n",
      "Epoch 62/100, Loss: 0.9996\n",
      "Epoch 63/100, Loss: 0.9997\n",
      "Epoch 64/100, Loss: 0.9996\n",
      "Epoch 65/100, Loss: 0.9996\n",
      "Epoch 66/100, Loss: 0.9996\n",
      "Epoch 67/100, Loss: 0.9996\n",
      "Epoch 68/100, Loss: 0.9997\n",
      "Epoch 69/100, Loss: 0.9996\n",
      "Epoch 70/100, Loss: 0.9995\n",
      "Epoch 71/100, Loss: 0.9996\n",
      "Epoch 72/100, Loss: 0.9994\n",
      "Epoch 73/100, Loss: 0.9994\n",
      "Epoch 74/100, Loss: 0.9994\n",
      "Epoch 75/100, Loss: 0.9994\n",
      "Epoch 76/100, Loss: 0.9994\n",
      "Epoch 77/100, Loss: 0.9995\n",
      "Epoch 78/100, Loss: 0.9994\n",
      "Epoch 79/100, Loss: 0.9991\n",
      "Epoch 80/100, Loss: 0.9994\n",
      "Epoch 81/100, Loss: 0.9989\n",
      "Epoch 82/100, Loss: 0.9992\n",
      "Epoch 83/100, Loss: 0.9990\n",
      "Epoch 84/100, Loss: 0.9990\n",
      "Epoch 85/100, Loss: 0.9990\n",
      "Epoch 86/100, Loss: 0.9992\n",
      "Epoch 87/100, Loss: 0.9985\n",
      "Epoch 88/100, Loss: 0.9988\n",
      "Epoch 89/100, Loss: 0.9989\n",
      "Epoch 90/100, Loss: 0.9990\n",
      "Epoch 91/100, Loss: 0.9991\n",
      "Epoch 92/100, Loss: 0.9989\n",
      "Epoch 93/100, Loss: 0.9994\n",
      "Epoch 94/100, Loss: 0.9996\n",
      "Epoch 95/100, Loss: 0.9990\n",
      "Epoch 96/100, Loss: 0.9988\n",
      "Epoch 97/100, Loss: 0.9990\n",
      "Epoch 98/100, Loss: 0.9989\n",
      "Epoch 99/100, Loss: 0.9988\n",
      "Epoch 100/100, Loss: 0.9991\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 0.9999\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 0.9999\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 0.9999\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 0.9999\n",
      "Epoch 11/100, Loss: 0.9999\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 0.9999\n",
      "Epoch 16/100, Loss: 0.9999\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 0.9999\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 0.9999\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 0.9998\n",
      "Epoch 33/100, Loss: 0.9999\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 0.9999\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 0.9999\n",
      "Epoch 40/100, Loss: 0.9999\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9999\n",
      "Epoch 45/100, Loss: 1.0000\n",
      "Epoch 46/100, Loss: 0.9999\n",
      "Epoch 47/100, Loss: 0.9999\n",
      "Epoch 48/100, Loss: 0.9999\n",
      "Epoch 49/100, Loss: 0.9999\n",
      "Epoch 50/100, Loss: 0.9999\n",
      "Epoch 51/100, Loss: 0.9999\n",
      "Epoch 52/100, Loss: 1.0000\n",
      "Epoch 53/100, Loss: 0.9999\n",
      "Epoch 54/100, Loss: 0.9999\n",
      "Epoch 55/100, Loss: 0.9999\n",
      "Epoch 56/100, Loss: 0.9999\n",
      "Epoch 57/100, Loss: 0.9999\n",
      "Epoch 58/100, Loss: 0.9999\n",
      "Epoch 59/100, Loss: 0.9999\n",
      "Epoch 60/100, Loss: 0.9999\n",
      "Epoch 61/100, Loss: 0.9999\n",
      "Epoch 62/100, Loss: 1.0000\n",
      "Epoch 63/100, Loss: 0.9999\n",
      "Epoch 64/100, Loss: 0.9999\n",
      "Epoch 65/100, Loss: 0.9999\n",
      "Epoch 66/100, Loss: 0.9999\n",
      "Epoch 67/100, Loss: 0.9999\n",
      "Epoch 68/100, Loss: 0.9999\n",
      "Epoch 69/100, Loss: 0.9999\n",
      "Epoch 70/100, Loss: 0.9999\n",
      "Epoch 71/100, Loss: 0.9999\n",
      "Epoch 72/100, Loss: 0.9999\n",
      "Epoch 73/100, Loss: 0.9999\n",
      "Epoch 74/100, Loss: 0.9999\n",
      "Epoch 75/100, Loss: 0.9998\n",
      "Epoch 76/100, Loss: 0.9999\n",
      "Epoch 77/100, Loss: 0.9999\n",
      "Epoch 78/100, Loss: 0.9999\n",
      "Epoch 79/100, Loss: 0.9999\n",
      "Epoch 80/100, Loss: 0.9999\n",
      "Epoch 81/100, Loss: 0.9999\n",
      "Epoch 82/100, Loss: 0.9999\n",
      "Epoch 83/100, Loss: 0.9999\n",
      "Epoch 84/100, Loss: 0.9999\n",
      "Epoch 85/100, Loss: 0.9999\n",
      "Epoch 86/100, Loss: 0.9999\n",
      "Epoch 87/100, Loss: 0.9999\n",
      "Epoch 88/100, Loss: 0.9999\n",
      "Epoch 89/100, Loss: 0.9999\n",
      "Epoch 90/100, Loss: 0.9999\n",
      "Epoch 91/100, Loss: 0.9999\n",
      "Epoch 92/100, Loss: 0.9999\n",
      "Epoch 93/100, Loss: 0.9999\n",
      "Epoch 94/100, Loss: 0.9999\n",
      "Epoch 95/100, Loss: 0.9999\n",
      "Epoch 96/100, Loss: 0.9999\n",
      "Epoch 97/100, Loss: 0.9999\n",
      "Epoch 98/100, Loss: 0.9999\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9999\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/GE_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.2598\n",
      "Epoch 2/100, Loss: 0.1958\n",
      "Epoch 3/100, Loss: 0.1447\n",
      "Epoch 4/100, Loss: 0.1126\n",
      "Epoch 5/100, Loss: 0.0912\n",
      "Epoch 6/100, Loss: 0.0750\n",
      "Epoch 7/100, Loss: 0.0574\n",
      "Epoch 8/100, Loss: 0.0401\n",
      "Epoch 9/100, Loss: 0.0276\n",
      "Epoch 10/100, Loss: 0.0190\n",
      "Epoch 11/100, Loss: 0.0129\n",
      "Epoch 12/100, Loss: 0.0087\n",
      "Epoch 13/100, Loss: 0.0058\n",
      "Epoch 14/100, Loss: 0.0038\n",
      "Epoch 15/100, Loss: 0.0024\n",
      "Epoch 16/100, Loss: 0.0017\n",
      "Epoch 17/100, Loss: 0.0013\n",
      "Epoch 18/100, Loss: 0.0010\n",
      "Epoch 19/100, Loss: 0.0008\n",
      "Epoch 20/100, Loss: 0.0005\n",
      "Epoch 21/100, Loss: 0.0004\n",
      "Epoch 22/100, Loss: 0.0002\n",
      "Epoch 23/100, Loss: 0.0002\n",
      "Epoch 24/100, Loss: 0.0001\n",
      "Epoch 25/100, Loss: 0.0001\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 37\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0000\n",
      "Epoch 2/100, Loss: 0.0000\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0000\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0000\n",
      "Epoch 13/100, Loss: 0.0000\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 45\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/exN_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.5431\n",
      "Epoch 2/100, Loss: 0.3370\n",
      "Epoch 3/100, Loss: 0.3417\n",
      "Epoch 4/100, Loss: 0.2884\n",
      "Epoch 5/100, Loss: 0.2091\n",
      "Epoch 6/100, Loss: 0.1855\n",
      "Epoch 7/100, Loss: 0.1552\n",
      "Epoch 8/100, Loss: 0.1248\n",
      "Epoch 9/100, Loss: 0.1042\n",
      "Epoch 10/100, Loss: 0.0915\n",
      "Epoch 11/100, Loss: 0.0722\n",
      "Epoch 12/100, Loss: 0.0633\n",
      "Epoch 13/100, Loss: 0.0555\n",
      "Epoch 14/100, Loss: 0.0445\n",
      "Epoch 15/100, Loss: 0.0396\n",
      "Epoch 16/100, Loss: 0.0344\n",
      "Epoch 17/100, Loss: 0.0276\n",
      "Epoch 18/100, Loss: 0.0218\n",
      "Epoch 19/100, Loss: 0.0178\n",
      "Epoch 20/100, Loss: 0.0168\n",
      "Epoch 21/100, Loss: 0.0149\n",
      "Epoch 22/100, Loss: 0.0128\n",
      "Epoch 23/100, Loss: 0.0097\n",
      "Epoch 24/100, Loss: 0.0099\n",
      "Epoch 25/100, Loss: 0.0076\n",
      "Epoch 26/100, Loss: 0.0084\n",
      "Epoch 27/100, Loss: 0.0065\n",
      "Epoch 28/100, Loss: 0.0065\n",
      "Epoch 29/100, Loss: 0.0056\n",
      "Epoch 30/100, Loss: 0.0043\n",
      "Epoch 31/100, Loss: 0.0044\n",
      "Epoch 32/100, Loss: 0.0033\n",
      "Epoch 33/100, Loss: 0.0035\n",
      "Epoch 34/100, Loss: 0.0025\n",
      "Epoch 35/100, Loss: 0.0026\n",
      "Epoch 36/100, Loss: 0.0021\n",
      "Epoch 37/100, Loss: 0.0018\n",
      "Epoch 38/100, Loss: 0.0017\n",
      "Epoch 39/100, Loss: 0.0015\n",
      "Epoch 40/100, Loss: 0.0014\n",
      "Epoch 41/100, Loss: 0.0013\n",
      "Epoch 42/100, Loss: 0.0011\n",
      "Epoch 43/100, Loss: 0.0010\n",
      "Epoch 44/100, Loss: 0.0009\n",
      "Epoch 45/100, Loss: 0.0008\n",
      "Epoch 46/100, Loss: 0.0007\n",
      "Epoch 47/100, Loss: 0.0006\n",
      "Epoch 48/100, Loss: 0.0006\n",
      "Epoch 49/100, Loss: 0.0005\n",
      "Epoch 50/100, Loss: 0.0005\n",
      "Epoch 51/100, Loss: 0.0005\n",
      "Epoch 52/100, Loss: 0.0004\n",
      "Epoch 53/100, Loss: 0.0004\n",
      "Epoch 54/100, Loss: 0.0003\n",
      "Epoch 55/100, Loss: 0.0003\n",
      "Epoch 56/100, Loss: 0.0003\n",
      "Epoch 57/100, Loss: 0.0003\n",
      "Epoch 58/100, Loss: 0.0002\n",
      "Epoch 59/100, Loss: 0.0002\n",
      "Epoch 60/100, Loss: 0.0002\n",
      "Epoch 61/100, Loss: 0.0002\n",
      "Epoch 62/100, Loss: 0.0002\n",
      "Epoch 63/100, Loss: 0.0001\n",
      "Epoch 64/100, Loss: 0.0001\n",
      "Epoch 65/100, Loss: 0.0001\n",
      "Epoch 66/100, Loss: 0.0001\n",
      "Epoch 67/100, Loss: 0.0001\n",
      "Epoch 68/100, Loss: 0.0001\n",
      "Epoch 69/100, Loss: 0.0001\n",
      "Epoch 70/100, Loss: 0.0001\n",
      "Epoch 71/100, Loss: 0.0001\n",
      "Epoch 72/100, Loss: 0.0001\n",
      "Epoch 73/100, Loss: 0.0001\n",
      "Epoch 74/100, Loss: 0.0001\n",
      "Epoch 75/100, Loss: 0.0001\n",
      "Epoch 76/100, Loss: 0.0001\n",
      "Epoch 77/100, Loss: 0.0001\n",
      "Epoch 78/100, Loss: 0.0001\n",
      "Epoch 79/100, Loss: 0.0001\n",
      "Epoch 80/100, Loss: 0.0001\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0001\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 13\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.0008\n",
      "Epoch 2/100, Loss: 0.0001\n",
      "Epoch 3/100, Loss: 0.0000\n",
      "Epoch 4/100, Loss: 0.0000\n",
      "Epoch 5/100, Loss: 0.0000\n",
      "Epoch 6/100, Loss: 0.0000\n",
      "Epoch 7/100, Loss: 0.0000\n",
      "Epoch 8/100, Loss: 0.0000\n",
      "Epoch 9/100, Loss: 0.0000\n",
      "Epoch 10/100, Loss: 0.0000\n",
      "Epoch 11/100, Loss: 0.0000\n",
      "Epoch 12/100, Loss: 0.0000\n",
      "Epoch 13/100, Loss: 0.0000\n",
      "Epoch 14/100, Loss: 0.0000\n",
      "Epoch 15/100, Loss: 0.0000\n",
      "Epoch 16/100, Loss: 0.0000\n",
      "Epoch 17/100, Loss: 0.0000\n",
      "Epoch 18/100, Loss: 0.0000\n",
      "Epoch 19/100, Loss: 0.0000\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0000\n",
      "Epoch 22/100, Loss: 0.0000\n",
      "Epoch 23/100, Loss: 0.0000\n",
      "Epoch 24/100, Loss: 0.0000\n",
      "Epoch 25/100, Loss: 0.0000\n",
      "Epoch 26/100, Loss: 0.0000\n",
      "Epoch 27/100, Loss: 0.0000\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0000\n",
      "Epoch 30/100, Loss: 0.0000\n",
      "Epoch 31/100, Loss: 0.0000\n",
      "Epoch 32/100, Loss: 0.0000\n",
      "Epoch 33/100, Loss: 0.0000\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 32\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "------------- Processing ../data/BREC/Basics/clo_G_Basics_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 0.7451\n",
      "Epoch 2/100, Loss: 0.7062\n",
      "Epoch 3/100, Loss: 0.6792\n",
      "Epoch 4/100, Loss: 0.6537\n",
      "Epoch 5/100, Loss: 0.6315\n",
      "Epoch 6/100, Loss: 0.6160\n",
      "Epoch 7/100, Loss: 0.6052\n",
      "Epoch 8/100, Loss: 0.5758\n",
      "Epoch 9/100, Loss: 0.5618\n",
      "Epoch 10/100, Loss: 0.5495\n",
      "Epoch 11/100, Loss: 0.5309\n",
      "Epoch 12/100, Loss: 0.5069\n",
      "Epoch 13/100, Loss: 0.4897\n",
      "Epoch 14/100, Loss: 0.4676\n",
      "Epoch 15/100, Loss: 0.4415\n",
      "Epoch 16/100, Loss: 0.4065\n",
      "Epoch 17/100, Loss: 0.3722\n",
      "Epoch 18/100, Loss: 0.3433\n",
      "Epoch 19/100, Loss: 0.3070\n",
      "Epoch 20/100, Loss: 0.2712\n",
      "Epoch 21/100, Loss: 0.2361\n",
      "Epoch 22/100, Loss: 0.2012\n",
      "Epoch 23/100, Loss: 0.1763\n",
      "Epoch 24/100, Loss: 0.1431\n",
      "Epoch 25/100, Loss: 0.1219\n",
      "Epoch 26/100, Loss: 0.0930\n",
      "Epoch 27/100, Loss: 0.0757\n",
      "Epoch 28/100, Loss: 0.0566\n",
      "Epoch 29/100, Loss: 0.0487\n",
      "Epoch 30/100, Loss: 0.0395\n",
      "Epoch 31/100, Loss: 0.0338\n",
      "Epoch 32/100, Loss: 0.0282\n",
      "Epoch 33/100, Loss: 0.0256\n",
      "Epoch 34/100, Loss: 0.0205\n",
      "Epoch 35/100, Loss: 0.0178\n",
      "Epoch 36/100, Loss: 0.0149\n",
      "Epoch 37/100, Loss: 0.0128\n",
      "Epoch 38/100, Loss: 0.0108\n",
      "Epoch 39/100, Loss: 0.0090\n",
      "Epoch 40/100, Loss: 0.0071\n",
      "Epoch 41/100, Loss: 0.0059\n",
      "Epoch 42/100, Loss: 0.0047\n",
      "Epoch 43/100, Loss: 0.0036\n",
      "Epoch 44/100, Loss: 0.0026\n",
      "Epoch 45/100, Loss: 0.0020\n",
      "Epoch 46/100, Loss: 0.0014\n",
      "Epoch 47/100, Loss: 0.0011\n",
      "Epoch 48/100, Loss: 0.0007\n",
      "Epoch 49/100, Loss: 0.0005\n",
      "Epoch 50/100, Loss: 0.0004\n",
      "Epoch 51/100, Loss: 0.0002\n",
      "Epoch 52/100, Loss: 0.0002\n",
      "Epoch 53/100, Loss: 0.0001\n",
      "Epoch 54/100, Loss: 0.0001\n",
      "Epoch 55/100, Loss: 0.0001\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n",
      "Correctly classified non-isomorphic pairs: 9\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.4467\n",
      "Epoch 2/100, Loss: 0.4192\n",
      "Epoch 3/100, Loss: 0.3954\n",
      "Epoch 4/100, Loss: 0.3805\n",
      "Epoch 5/100, Loss: 0.3696\n",
      "Epoch 6/100, Loss: 0.3629\n",
      "Epoch 7/100, Loss: 0.3533\n",
      "Epoch 8/100, Loss: 0.3395\n",
      "Epoch 9/100, Loss: 0.3343\n",
      "Epoch 10/100, Loss: 0.3251\n",
      "Epoch 11/100, Loss: 0.3262\n",
      "Epoch 12/100, Loss: 0.3233\n",
      "Epoch 13/100, Loss: 0.3177\n",
      "Epoch 14/100, Loss: 0.3112\n",
      "Epoch 15/100, Loss: 0.3030\n",
      "Epoch 16/100, Loss: 0.2973\n",
      "Epoch 17/100, Loss: 0.2909\n",
      "Epoch 18/100, Loss: 0.2835\n",
      "Epoch 19/100, Loss: 0.2776\n",
      "Epoch 20/100, Loss: 0.2738\n",
      "Epoch 21/100, Loss: 0.2689\n",
      "Epoch 22/100, Loss: 0.2634\n",
      "Epoch 23/100, Loss: 0.2588\n",
      "Epoch 24/100, Loss: 0.2536\n",
      "Epoch 25/100, Loss: 0.2484\n",
      "Epoch 26/100, Loss: 0.2447\n",
      "Epoch 27/100, Loss: 0.2404\n",
      "Epoch 28/100, Loss: 0.2359\n",
      "Epoch 29/100, Loss: 0.2301\n",
      "Epoch 30/100, Loss: 0.2258\n",
      "Epoch 31/100, Loss: 0.2203\n",
      "Epoch 32/100, Loss: 0.2175\n",
      "Epoch 33/100, Loss: 0.2129\n",
      "Epoch 34/100, Loss: 0.2089\n",
      "Epoch 35/100, Loss: 0.2066\n",
      "Epoch 36/100, Loss: 0.2023\n",
      "Epoch 37/100, Loss: 0.1987\n",
      "Epoch 38/100, Loss: 0.1954\n",
      "Epoch 39/100, Loss: 0.1913\n",
      "Epoch 40/100, Loss: 0.1876\n",
      "Epoch 41/100, Loss: 0.1845\n",
      "Epoch 42/100, Loss: 0.1815\n",
      "Epoch 43/100, Loss: 0.1783\n",
      "Epoch 44/100, Loss: 0.1750\n",
      "Epoch 45/100, Loss: 0.1714\n",
      "Epoch 46/100, Loss: 0.1683\n",
      "Epoch 47/100, Loss: 0.1656\n",
      "Epoch 48/100, Loss: 0.1593\n",
      "Epoch 49/100, Loss: 0.1583\n",
      "Epoch 50/100, Loss: 0.1594\n",
      "Epoch 51/100, Loss: 0.1581\n",
      "Epoch 52/100, Loss: 0.1547\n",
      "Epoch 53/100, Loss: 0.1497\n",
      "Epoch 54/100, Loss: 0.1456\n",
      "Epoch 55/100, Loss: 0.1425\n",
      "Epoch 56/100, Loss: 0.1384\n",
      "Epoch 57/100, Loss: 0.1385\n",
      "Epoch 58/100, Loss: 0.1352\n",
      "Epoch 59/100, Loss: 0.1315\n",
      "Epoch 60/100, Loss: 0.1280\n",
      "Epoch 61/100, Loss: 0.1255\n",
      "Epoch 62/100, Loss: 0.1219\n",
      "Epoch 63/100, Loss: 0.1167\n",
      "Epoch 64/100, Loss: 0.1139\n",
      "Epoch 65/100, Loss: 0.1120\n",
      "Epoch 66/100, Loss: 0.1100\n",
      "Epoch 67/100, Loss: 0.1079\n",
      "Epoch 68/100, Loss: 0.1059\n",
      "Epoch 69/100, Loss: 0.1044\n",
      "Epoch 70/100, Loss: 0.1032\n",
      "Epoch 71/100, Loss: 0.1013\n",
      "Epoch 72/100, Loss: 0.0992\n",
      "Epoch 73/100, Loss: 0.0975\n",
      "Epoch 74/100, Loss: 0.0955\n",
      "Epoch 75/100, Loss: 0.0937\n",
      "Epoch 76/100, Loss: 0.0918\n",
      "Epoch 77/100, Loss: 0.0906\n",
      "Epoch 78/100, Loss: 0.0891\n",
      "Epoch 79/100, Loss: 0.0880\n",
      "Epoch 80/100, Loss: 0.0864\n",
      "Epoch 81/100, Loss: 0.0855\n",
      "Epoch 82/100, Loss: 0.0835\n",
      "Epoch 83/100, Loss: 0.0826\n",
      "Epoch 84/100, Loss: 0.0807\n",
      "Epoch 85/100, Loss: 0.0800\n",
      "Epoch 86/100, Loss: 0.0783\n",
      "Epoch 87/100, Loss: 0.0771\n",
      "Epoch 88/100, Loss: 0.0763\n",
      "Epoch 89/100, Loss: 0.0748\n",
      "Epoch 90/100, Loss: 0.0735\n",
      "Epoch 91/100, Loss: 0.0723\n",
      "Epoch 92/100, Loss: 0.0710\n",
      "Epoch 93/100, Loss: 0.0711\n",
      "Epoch 94/100, Loss: 0.0691\n",
      "Epoch 95/100, Loss: 0.0687\n",
      "Epoch 96/100, Loss: 0.0674\n",
      "Epoch 97/100, Loss: 0.0661\n",
      "Epoch 98/100, Loss: 0.0650\n",
      "Epoch 99/100, Loss: 0.0637\n",
      "Epoch 100/100, Loss: 0.0632\n",
      "Correctly classified non-isomorphic pairs: 13\n",
      "Correctly classified isomorphic pairs: 60\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/Basics/*G_Basics_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_Basics_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593f6e9-35b7-46ab-8683-3ec437bb249b",
   "metadata": {},
   "source": [
    "# BREC-Regular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56997f2-4bcc-4e82-9d77-78dba2caf692",
   "metadata": {},
   "source": [
    "## Original Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9f741f7-4f12-4a40-b5d1-de9ffeeb6288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/G_Regular_dataset.pkl.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_Regular_dgl_graphs = []\n",
    "for G in G_Regular:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_Regular_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/Regular/G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Regular_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_Regular_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29627715-6e67-485a-ba47-16fdccf1df8f",
   "metadata": {},
   "source": [
    "## Regular dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "98de0e9a-bcec-4ba2-bedb-267d9a0bdf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "G_Regular_dgl_graphs, G_Regular_isomorphic_pair, G_Regular_original_indices = add_isomorphic_pairs_dgl(G_Regular_dgl_graphs, num_pairs=100)\n",
    "G_Regular_dummy_dgl = G_Regular_dgl_graphs + G_Regular_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/Regular/G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06b9ee-5999-4304-990d-17acb3238138",
   "metadata": {},
   "source": [
    "## VN on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "091bb5a2-25a5-48a3-a496-ac1058edf212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/vn_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/vn_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "vn_G_Regular_dgl = apply_vn(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/vn_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_Regular_dummy_dgl = apply_vn(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/vn_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909c153-eec0-48b1-a8af-8e52d47203b3",
   "metadata": {},
   "source": [
    "## Degree Centrality on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2785301-c6b6-4ccf-aa81-a2cb54e94c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/deg_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/deg_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "deg_G_Regular_dgl = degree_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/deg_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_Regular_dummy_dgl = degree_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/deg_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913c33c-f2f4-43c4-974e-0732d49faf86",
   "metadata": {},
   "source": [
    "## Closeness Centrality on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "463e0f90-0149-4833-8665-9fc82e298a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/clo_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/clo_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_Regular_dgl = closeness_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/clo_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_Regular_dummy_dgl = closeness_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/clo_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19b1a7-1051-4159-ae0a-61477bc88a92",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "69820049-f939-42ea-9249-9553a204b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/bet_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/bet_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "bet_G_Regular_dgl = betweenness_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/bet_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_Regular_dummy_dgl = betweenness_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/bet_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0e7db-b8ad-4982-8847-1f347fd4bc98",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a76dcce-3e09-43ab-abdc-b838369fb662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/eig_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/eig_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "eig_G_Regular_dgl = eigenvector_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/eig_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_Regular_dummy_dgl = eigenvector_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/eig_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e759d9a-3caa-485a-9915-9ad572dc73af",
   "metadata": {},
   "source": [
    "## Distance Encoding on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e830b8b5-b127-45b1-add3-5001d46dde4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/DE_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/DE_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "DE_G_Regular_dgl = distance_encoding(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/DE_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_Regular_dummy_dgl = distance_encoding(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/DE_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19351a86-3042-4a66-a900-7b6aa79be874",
   "metadata": {},
   "source": [
    "## Graph Encoding on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1adf4f7-25cc-4f0a-9fe0-facfe3e569e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/GE_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/GE_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "GE_G_Regular_dgl = Graph_encoding(G_Regular_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/Regular/GE_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_Regular_dummy_dgl = Graph_encoding(G_Regular_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/Regular/GE_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9663c8b-dffc-478e-8a10-ced27340b05a",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f9bcf10f-b35b-4437-b4d0-92e97cd1618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/SE_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/SE_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "SE_G_Regular_dgl = subgraph_dataset(G_Regular_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/Regular/SE_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_Regular_dummy_dgl = subgraph_dataset(G_Regular_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/Regular/SE_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504e564-3952-4ba4-adaf-4a355760ab39",
   "metadata": {},
   "source": [
    "## Extra Node on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "da4a2764-54ce-41b0-bd60-9a9829c71be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/exN_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/exN_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "exN_G_Regular_dgl = extra_node_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/exN_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_Regular_dummy_dgl = extra_node_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/exN_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a7d0e-52ee-4b6f-99e0-aee5b3136461",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on Regular original and Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7f720468-9f8f-467a-a6dd-d84316569331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Regular/gle_G_Regular_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Regular/gle_G_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "gle_G_Regular_dgl = graphlet_encoding_dataset(G_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Regular/gle_G_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_Regular_dummy_dgl = graphlet_encoding_dataset(G_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Regular/gle_G_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1fd872-2b4a-4832-b7b7-e200756d1094",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "071062d4-07b4-40d2-b9e4-1d8d4559a3b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Regular/clo_G_Regular_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {546506273467430400, 546506219780339200, 540371822420748800, 1367498087771328000, 540371929794995200, 540371822420684800, 1367498195145574400, 1367498517268185600, 1367498409893939200, 1367498517268121600, 1069076997260595200, 269808283683795200, 4059200909006336000, 27824205796147916800, 244518801976729600, 1808664590115020800, 31402125088682496000, 269601212234764800, 265790822618432000, 40126186917251276800, 29609508717778944000, 927094773181158400, 927094665806976000, 549422665023820800, 167660496609817600, 163671837837747200, 555292760665625600, 1069077212009088000, 267350004044518400, 543591116708697600, 265017754887001600, 927094451058547200, 1033411244425984000, 1069077212009024000, 927094558432729600, 428064593520704000, 540371876107840000, 555292599604288000, 552351565672716800, 543591277770099200, 428064620364249600, 265790634713516800, 543591116708761600, 543591170395852800, 546506273467366400, 1808664375366528000, 263357910087859200, 269688507726208000, 266149988384448000, 927094665807040000, 264889254757158400, 927094773181222400, 543591224082944000}\n",
      "Number of unique embeddings with GIN: 53\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {2625945500675, 1096618411525, 2625975185925, 1107843887112, 418020284937, 6519070833675, 40398411084300, 1039978983950, 6519021266450, 2625971924500, 1193528583700, 6519074109975, 393711978525, 1082240250400, 17818037289000, 2625944879150, 433986482743, 2625905755725, 6519059382875, 1107837437537, 6519071659625, 1107836000875, 1082240562287, 2625952049775, 1096606019187, 1082227446900, 7864107320450, 1082224786562, 1082204405900, 7863926287500, 2625918660750, 2625967623825, 417522155162, 6519102393500, 1082223764125, 1054001980062, 1107846039200, 409682739375, 44098748698800, 1096585124025, 7863898432700, 1096613903550, 2625978885825, 1022779439300, 2625956966600, 1096607042250, 408632158925, 1096620154575, 2625974578900, 1096619332312, 2625949388000, 1096640544487, 1066612207850, 1065990883587, 6519053247750, 2625963312400, 1096627532562, 2625972738325, 1107837127962, 1107828221725, 2625906776350, 412352053537, 2625933415725, 1096621794100, 6519028659000, 2625980508475, 2625987692350, 2625957996350, 1193517215550, 6518914390850, 5064864829250, 34663186051400, 2625971297100, 421019774287, 1096634398550, 17818432162650, 26546110385500, 1024690050400, 2625986666350, 1067423837050, 424709387650, 1107830879625, 6519075764625, 419414780312, 1096631528350, 2625949794725, 2625964744625, 392609697212, 2625959828925, 1107843477950, 2625896540100, 1082227141075, 2625962905050, 1096620153825, 6519048323050, 414117512175, 410419443700, 1082199380987, 1107842556412}\n",
      "Number of unique embeddings with PNA: 99\n",
      "\n",
      "------------- Processing ../data/BREC/Regular/clo_G_Regular_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {7829640121600, 6872666652800, 2306744776153600, 2222683007100800, 12466406687305600, 138384038901363200, 8070233391500, 7829640121100, 7829639301900, 8070232572050, 9766465635350, 8070232571800, 4067425991450, 9766464815900, 9766465635100, 8080249172650, 8080247534250, 22954536166700, 6851568069550, 6740527776050, 6740528185650, 6716569487800, 9738619953600, 9738620772800, 9815862806850, 9738621592000, 4235080758600, 241357989385800, 11108825000400, 6673168347000, 11108823361750, 11108824180950, 11108822542550, 6719986658650, 11108823361500, 11108821723100, 6781874612450, 6068542325475, 7069992096100, 9854983836900, 6068541915750, 6068542734950, 6844553503850, 4603240268275, 4603239858675, 9793181895800, 9899725437950}\n",
      "Number of unique embeddings with GIN: 47\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {272691707912, 276432654862, 349031611918, 281731206675, 68763577368, 52236607512, 271606628387, 349031815718, 6853427715625, 271607346218, 271604016687, 349031405118, 272692884543, 248625435200, 248623822400, 349032995912, 349030459981, 106927486037, 349032124512, 168548220000, 271606705250, 277248800868, 9088826481, 111934940787, 113043919987, 102378593915, 272689401475, 248624026262, 168549474462, 113587567775, 271605809312, 271606475943, 271605578412, 272688018606, 248621977262, 248626896056, 113036234425, 272688379068, 272686407868, 68763180225, 273595201225, 248624692956, 68763167456, 68764135137, 277206807275, 68760899309, 349033914606, 4033251088625, 9088825587, 349029484275, 112520338675, 349031356668, 272690297600, 248623617287, 248622054156, 272684563731, 68764115734, 68762841384, 349027105068, 32955302041900, 349031611700, 349033299768, 272688608062, 349034964800, 271890812737, 349034582862, 349029101400, 68761680734, 68763897696, 349032124262, 271606781800, 248622771050, 349035170162, 349030101362, 68763321212, 112749626243, 113342089093, 272690760587, 349034784662, 349031996825, 349026079131, 116300628906, 272689352618, 349033197493, 104741999543, 271864558525, 349030409156, 272691065287, 864007865806, 272684256218, 349030843356, 349030331356, 52237414365, 362963468818400, 113971261931, 349031509487, 9088752112, 112659866618, 68762866687}\n",
      "Number of unique embeddings with PNA: 99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/Regular/clo_G_Regular_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899e835-0012-47c9-a435-088dda5627e6",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b07a3eb7-18ad-4d7a-9ba6-66cbc47cd9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Regular/clo_G_Regular_dataset_dummy.pkl...\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.4149\n",
      "Epoch 2/100, Loss: 1.3382\n",
      "Epoch 3/100, Loss: 1.3016\n",
      "Epoch 4/100, Loss: 1.0251\n",
      "Epoch 5/100, Loss: 0.8888\n",
      "Epoch 6/100, Loss: 0.9436\n",
      "Epoch 7/100, Loss: 0.7803\n",
      "Epoch 8/100, Loss: 0.9934\n",
      "Epoch 9/100, Loss: 1.3079\n",
      "Epoch 10/100, Loss: 1.3109\n",
      "Epoch 11/100, Loss: 1.2611\n",
      "Epoch 12/100, Loss: 1.2082\n",
      "Epoch 13/100, Loss: 1.1617\n",
      "Epoch 14/100, Loss: 1.1616\n",
      "Epoch 15/100, Loss: 1.2800\n",
      "Epoch 16/100, Loss: 1.0518\n",
      "Epoch 17/100, Loss: 1.1451\n",
      "Epoch 18/100, Loss: 1.2377\n",
      "Epoch 19/100, Loss: 1.1764\n",
      "Epoch 20/100, Loss: 1.0332\n",
      "Epoch 21/100, Loss: 1.0327\n",
      "Epoch 22/100, Loss: 1.2446\n",
      "Epoch 23/100, Loss: 0.9698\n",
      "Epoch 24/100, Loss: 0.9159\n",
      "Epoch 25/100, Loss: 0.9373\n",
      "Epoch 26/100, Loss: 0.9605\n",
      "Epoch 27/100, Loss: 1.0332\n",
      "Epoch 28/100, Loss: 1.3650\n",
      "Epoch 29/100, Loss: 1.3216\n",
      "Epoch 30/100, Loss: 1.3620\n",
      "Epoch 31/100, Loss: 1.1598\n",
      "Epoch 32/100, Loss: 1.1615\n",
      "Epoch 33/100, Loss: 1.3513\n",
      "Epoch 34/100, Loss: 1.1795\n",
      "Epoch 35/100, Loss: 1.0831\n",
      "Epoch 36/100, Loss: 1.2464\n",
      "Epoch 37/100, Loss: 1.0309\n",
      "Epoch 38/100, Loss: 0.9710\n",
      "Epoch 39/100, Loss: 0.9875\n",
      "Epoch 40/100, Loss: 0.7905\n",
      "Epoch 41/100, Loss: 0.9328\n",
      "Epoch 42/100, Loss: 0.9728\n",
      "Epoch 43/100, Loss: 0.8830\n",
      "Epoch 44/100, Loss: 0.8766\n",
      "Epoch 45/100, Loss: 0.6783\n",
      "Epoch 46/100, Loss: 0.6835\n",
      "Epoch 47/100, Loss: 0.8143\n",
      "Epoch 48/100, Loss: 0.9534\n",
      "Epoch 49/100, Loss: 0.5087\n",
      "Epoch 50/100, Loss: 0.7348\n",
      "Epoch 51/100, Loss: 0.6615\n",
      "Epoch 52/100, Loss: 0.6534\n",
      "Epoch 53/100, Loss: 0.6398\n",
      "Epoch 54/100, Loss: 0.5651\n",
      "Epoch 55/100, Loss: 0.6343\n",
      "Epoch 56/100, Loss: 0.7398\n",
      "Epoch 57/100, Loss: 0.4986\n",
      "Epoch 58/100, Loss: 0.5069\n",
      "Epoch 59/100, Loss: 0.4452\n",
      "Epoch 60/100, Loss: 0.4036\n",
      "Epoch 61/100, Loss: 0.5693\n",
      "Epoch 62/100, Loss: 0.9296\n",
      "Epoch 63/100, Loss: 0.7205\n",
      "Epoch 64/100, Loss: 0.6120\n",
      "Epoch 65/100, Loss: 0.6284\n",
      "Epoch 66/100, Loss: 0.4878\n",
      "Epoch 67/100, Loss: 0.8540\n",
      "Epoch 68/100, Loss: 0.4192\n",
      "Epoch 69/100, Loss: 0.3954\n",
      "Epoch 70/100, Loss: 0.3747\n",
      "Epoch 71/100, Loss: 0.8622\n",
      "Epoch 72/100, Loss: 0.7126\n",
      "Epoch 73/100, Loss: 0.5935\n",
      "Epoch 74/100, Loss: 0.5388\n",
      "Epoch 75/100, Loss: 0.7014\n",
      "Epoch 76/100, Loss: 0.5644\n",
      "Epoch 77/100, Loss: 0.7212\n",
      "Epoch 78/100, Loss: 0.6845\n",
      "Epoch 79/100, Loss: 0.5909\n",
      "Epoch 80/100, Loss: 0.5523\n",
      "Epoch 81/100, Loss: 0.7214\n",
      "Epoch 82/100, Loss: 0.4863\n",
      "Epoch 83/100, Loss: 0.5260\n",
      "Epoch 84/100, Loss: 0.4580\n",
      "Epoch 85/100, Loss: 0.3019\n",
      "Epoch 86/100, Loss: 0.3495\n",
      "Epoch 87/100, Loss: 0.3554\n",
      "Epoch 88/100, Loss: 0.7577\n",
      "Epoch 89/100, Loss: 0.4879\n",
      "Epoch 90/100, Loss: 0.4531\n",
      "Epoch 91/100, Loss: 0.3451\n",
      "Epoch 92/100, Loss: 0.5919\n",
      "Epoch 93/100, Loss: 0.8480\n",
      "Epoch 94/100, Loss: 0.1683\n",
      "Epoch 95/100, Loss: 0.3288\n",
      "Epoch 96/100, Loss: 0.4892\n",
      "Epoch 97/100, Loss: 0.6996\n",
      "Epoch 98/100, Loss: 0.3542\n",
      "Epoch 99/100, Loss: 0.3623\n",
      "Epoch 100/100, Loss: 0.7838\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.3115\n",
      "Epoch 2/100, Loss: 0.3164\n",
      "Epoch 3/100, Loss: 0.6729\n",
      "Epoch 4/100, Loss: 0.3057\n",
      "Epoch 5/100, Loss: 1.0292\n",
      "Epoch 6/100, Loss: 0.2178\n",
      "Epoch 7/100, Loss: 0.0218\n",
      "Epoch 8/100, Loss: 0.1626\n",
      "Epoch 9/100, Loss: 0.1667\n",
      "Epoch 10/100, Loss: 0.3912\n",
      "Epoch 11/100, Loss: 0.5803\n",
      "Epoch 12/100, Loss: 0.5249\n",
      "Epoch 13/100, Loss: 0.5654\n",
      "Epoch 14/100, Loss: 0.3727\n",
      "Epoch 15/100, Loss: 0.2988\n",
      "Epoch 16/100, Loss: 0.3429\n",
      "Epoch 17/100, Loss: 0.5517\n",
      "Epoch 18/100, Loss: 0.4012\n",
      "Epoch 19/100, Loss: 0.6614\n",
      "Epoch 20/100, Loss: 0.3432\n",
      "Epoch 21/100, Loss: 0.6511\n",
      "Epoch 22/100, Loss: 0.7268\n",
      "Epoch 23/100, Loss: 0.4174\n",
      "Epoch 24/100, Loss: 0.3761\n",
      "Epoch 25/100, Loss: 0.1571\n",
      "Epoch 26/100, Loss: 0.0917\n",
      "Epoch 27/100, Loss: 0.0503\n",
      "Epoch 28/100, Loss: 0.9912\n",
      "Epoch 29/100, Loss: 0.3476\n",
      "Epoch 30/100, Loss: 0.1320\n",
      "Epoch 31/100, Loss: 0.8881\n",
      "Epoch 32/100, Loss: 0.0479\n",
      "Epoch 33/100, Loss: 0.4163\n",
      "Epoch 34/100, Loss: 0.1988\n",
      "Epoch 35/100, Loss: 0.3977\n",
      "Epoch 36/100, Loss: 0.9162\n",
      "Epoch 37/100, Loss: 0.4521\n",
      "Epoch 38/100, Loss: 0.8917\n",
      "Epoch 39/100, Loss: 0.0895\n",
      "Epoch 40/100, Loss: 0.6153\n",
      "Epoch 41/100, Loss: 0.1705\n",
      "Epoch 42/100, Loss: 0.1935\n",
      "Epoch 43/100, Loss: 0.7971\n",
      "Epoch 44/100, Loss: 0.4337\n",
      "Epoch 45/100, Loss: 0.0900\n",
      "Epoch 46/100, Loss: 0.3703\n",
      "Epoch 47/100, Loss: 0.6713\n",
      "Epoch 48/100, Loss: 0.1217\n",
      "Epoch 49/100, Loss: 0.1840\n",
      "Epoch 50/100, Loss: 0.0603\n",
      "Epoch 51/100, Loss: 0.3314\n",
      "Epoch 52/100, Loss: 0.2444\n",
      "Epoch 53/100, Loss: 0.3057\n",
      "Epoch 54/100, Loss: 0.1178\n",
      "Epoch 55/100, Loss: 0.5594\n",
      "Epoch 56/100, Loss: 0.0042\n",
      "Epoch 57/100, Loss: 0.3632\n",
      "Epoch 58/100, Loss: 0.1567\n",
      "Epoch 59/100, Loss: 0.0520\n",
      "Epoch 60/100, Loss: 0.2946\n",
      "Epoch 61/100, Loss: 0.0284\n",
      "Epoch 62/100, Loss: 0.0526\n",
      "Epoch 63/100, Loss: 0.9661\n",
      "Epoch 64/100, Loss: 0.2314\n",
      "Epoch 65/100, Loss: 0.6608\n",
      "Epoch 66/100, Loss: 0.6071\n",
      "Epoch 67/100, Loss: 0.4515\n",
      "Epoch 68/100, Loss: 0.3418\n",
      "Epoch 69/100, Loss: 0.2888\n",
      "Epoch 70/100, Loss: 0.0648\n",
      "Epoch 71/100, Loss: 0.2988\n",
      "Epoch 72/100, Loss: 0.2631\n",
      "Epoch 73/100, Loss: 0.6296\n",
      "Epoch 74/100, Loss: 0.8376\n",
      "Epoch 75/100, Loss: 0.2711\n",
      "Epoch 76/100, Loss: 0.6587\n",
      "Epoch 77/100, Loss: 0.2517\n",
      "Epoch 78/100, Loss: 0.4504\n",
      "Epoch 79/100, Loss: 0.4797\n",
      "Epoch 80/100, Loss: 0.2197\n",
      "Epoch 81/100, Loss: 0.2661\n",
      "Epoch 82/100, Loss: 0.3934\n",
      "Epoch 83/100, Loss: 0.1865\n",
      "Epoch 84/100, Loss: 0.8624\n",
      "Epoch 85/100, Loss: 0.5987\n",
      "Epoch 86/100, Loss: 0.4966\n",
      "Epoch 87/100, Loss: 0.1291\n",
      "Epoch 88/100, Loss: 0.4690\n",
      "Epoch 89/100, Loss: 0.2867\n",
      "Epoch 90/100, Loss: 0.3277\n",
      "Epoch 91/100, Loss: 0.0400\n",
      "Epoch 92/100, Loss: 0.2911\n",
      "Epoch 93/100, Loss: 0.0711\n",
      "Epoch 94/100, Loss: 0.5897\n",
      "Epoch 95/100, Loss: 0.1792\n",
      "Epoch 96/100, Loss: 0.0599\n",
      "Epoch 97/100, Loss: 0.1433\n",
      "Epoch 98/100, Loss: 0.3706\n",
      "Epoch 99/100, Loss: 0.1409\n",
      "Epoch 100/100, Loss: 0.1277\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/Regular/clo_G_Regular_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_Regular_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0ab8c-a1d3-421a-92e7-18b5d1841164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BREC-Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e4703-fb72-4a4a-b2d9-7e6419a1b052",
   "metadata": {},
   "source": [
    "## Original Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95353f3-8818-4022-aa2b-0bd05d154e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_Extension_dgl_graphs = []\n",
    "for G in G_Extension:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_Extension_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/Extension/G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Extension_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_Extension_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6192e-264f-43b3-90d6-7b889e2dfb35",
   "metadata": {},
   "source": [
    "## Extension dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea213255-3015-4b3a-a67a-afb4a1d2cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_Extension_dgl_graphs, G_Extension_isomorphic_pair, G_Extension_original_indices = add_isomorphic_pairs_dgl(G_Extension_dgl_graphs, num_pairs=100)\n",
    "G_Extension_dummy_dgl = G_Extension_dgl_graphs + G_Extension_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/Extension/G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0834a-300a-486a-8462-324b59d985ca",
   "metadata": {},
   "source": [
    "## VN on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efc667-8080-40cb-bcad-d2ef1914d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_G_Extension_dgl = apply_vn(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/vn_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_Extension_dummy_dgl = apply_vn(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/vn_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8a098-867d-4f3c-b2b0-aa57d4fa6a53",
   "metadata": {},
   "source": [
    "## Degree Centrality on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094c3a-af34-4188-b24e-2f9461dce037",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_G_Extension_dgl = degree_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/deg_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_Extension_dummy_dgl = degree_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/deg_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a45705-e2a3-436b-a410-534212303e5a",
   "metadata": {},
   "source": [
    "## Closeness Centrality on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "286742f4-0436-4c9e-a6a3-c5f36b3a94a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/Extension/clo_G_Extension_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Extension/clo_G_Extension_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_Extension_dgl = closeness_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/clo_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_Extension_dummy_dgl = closeness_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/clo_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ddbfc3-dde6-46b5-ae67-9d0c66d84e4d",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a830d-00c8-4cb2-af3a-f1d7be7ffa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_G_Extension_dgl = betweenness_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/bet_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_Extension_dummy_dgl = betweenness_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/bet_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67477822-e381-4c35-8c96-a3207b5d4d2d",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcfc16-2db1-43e3-96ca-b51f3ae4ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_G_Extension_dgl = eigenvector_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/eig_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_Extension_dummy_dgl = eigenvector_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/eig_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb86d3-62ce-4d63-8b2e-1ecffe79d874",
   "metadata": {},
   "source": [
    "## Distance Encoding on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757311c7-9f2f-4290-aef0-412c06e17d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_G_Extension_dgl = distance_encoding(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/DE_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_Extension_dummy_dgl = distance_encoding(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/DE_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157c785-f0aa-4e94-8f0c-96aad6b1ed2f",
   "metadata": {},
   "source": [
    "## Graph Encoding on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e1623-1dbc-4e52-8dc4-7b7620249219",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_G_Extension_dgl = Graph_encoding(G_Extension_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/Extension/GE_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_Extension_dummy_dgl = Graph_encoding(G_Extension_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/Extension/GE_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e656d-9124-4faf-8ef8-50f33d04cd0a",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee724e-ac45-469c-9af5-906f49bb8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_G_Extension_dgl = subgraph_dataset(G_Extension_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/Extension/SE_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_Extension_dummy_dgl = subgraph_dataset(G_Extension_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/Extension/SE_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25ee3b-cda0-43c2-beed-c50c149e802f",
   "metadata": {},
   "source": [
    "## Extra Node on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739312fa-fd48-413a-b7c1-4bf8794a59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "exN_G_Extension_dgl = extra_node_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/exN_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_Extension_dummy_dgl = extra_node_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/exN_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36f133-d7c9-4010-8e78-1852144e0bf9",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on Extension original and Extension dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ae27c-f093-4c20-b12c-d79497f66e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gle_G_Extension_dgl = graphlet_encoding_dataset(G_Extension_dgl_graphs)\n",
    "output_file = '../data/BREC/Extension/gle_G_Extension_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Extension_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Extension_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_Extension_dummy_dgl = graphlet_encoding_dataset(G_Extension_dummy_dgl)\n",
    "output_file = '../data/BREC/Extension/gle_G_Extension_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Extension_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Extension_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d46bca-61bf-468c-9f15-3675a52a5998",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8008b870-0630-42ad-91af-9585a1e5ce97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Extension/clo_G_Extension_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {316657155, 3524102, 42123787, 22907411, 115643923, 306744339, 80503833, 190399513, 65968161, 836643, 34625062, 27870250, 389835822, 57462318, 3729966, 543053362, 526702645, 11643958, 3651127, 62354492, 27599425, 109444674, 310498883, 241303620, 62354501, 2294337, 2294338, 428118090, 127931470, 98687058, 29766740, 288953943, 30848603, 113158748, 5520993, 62001761, 25307235, 69026916, 455704682, 69026922, 2989164, 15357037, 15357038, 376102509, 149770349, 402031217, 66415221, 408118391, 6759550072, 4131448, 4131449, 739460, 149747845, 201846923, 45276813, 1379982, 75952274, 35378840, 35378841, 467768475, 148230811, 18454688, 1434273, 134315169, 18454691, 3832493, 10620078, 267394223, 148230837, 246704316, 285914303, 4445377, 745222851, 4683979, 4683983, 26802901, 5152984, 40376541, 1103399648, 1089574640, 114892533, 479650567, 627739911, 12122887, 336151310, 2829583, 4339477, 223526169, 492116252, 470162716, 2580766, 149775649, 449104676, 172252976, 207960369, 223526194, 8125750, 22118200, 14570811, 81095996, 36117319, 35763021, 4635478, 123437398, 6759549272, 88334684, 123437406, 15177057, 15177059, 1896805, 164876651, 212292460, 303636847, 159833972, 2753397, 361099637, 88286581, 6604152, 21281149, 1386881, 444216708, 122962311, 618634121, 502667146, 119034253, 122962322, 122962323, 7169941, 69698971, 348956576, 2355116, 35439534, 621657007, 443901358, 6256056, 276860345, 1089574844, 25632189, 14086590, 324361664, 14086593, 23582658, 247533515, 35439564, 24886732, 408097743, 25280467, 191449046, 13847000, 260697563, 40316382, 15014882, 40316388, 22653418, 304380907, 618634223, 191449072, 384977909, 306744313, 389835773, 425582590, 63555583}\n",
      "Number of unique embeddings with GIN: 162\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {6147, 3075, 3080, 6153, 4106, 3594, 12815, 13328, 5659, 4647, 5673, 7211, 5687, 28729, 6715, 4158, 3653, 5194, 11340, 5708, 4176, 13397, 7261, 4190, 3167, 4707, 4196, 6756, 3685, 3686, 3177, 3692, 5229, 5237, 14966, 3210, 4248, 6303, 6304, 4257, 5798, 4782, 6319, 4784, 4785, 4289, 4807, 4815, 5841, 3794, 7380, 4310, 4314, 4316, 4319, 4322, 5863, 8432, 3825, 2802, 3830, 3832, 3323, 5375, 6913, 14594, 3848, 2825, 6922, 2828, 14610, 6419, 5396, 3354, 4383, 4386, 21284, 4391, 4395, 6959, 3379, 6453, 26941, 5954, 2894, 12110, 6480, 4435, 6996, 3924, 3419, 4960, 4448, 12131, 6507, 21357, 4465, 4466, 29042, 3957, 5498, 7035, 6017, 6531, 2950, 2956, 5004, 2961, 6549, 6037, 3479, 3992, 7063, 5019, 7582, 5538, 5026, 4517, 6054, 8101, 4526, 5552, 3506, 4028, 4030, 4038, 4047, 5588, 14294, 4061, 3042, 85991, 3561, 3050, 4083, 5620, 7670, 5116}\n",
      "Number of unique embeddings with PNA: 138\n",
      "\n",
      "------------- Processing ../data/BREC/Extension/clo_G_Extension_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {314880, 477696, 349709, 1163277, 53264, 46100, 3302424, 630810, 1457183, 1029666, 2627106, 151587, 4899373, 1675823, 11330608, 103476, 1723965, 511559, 377927, 972361, 1011274, 4418640, 2407510, 1348183, 879706, 46172, 390753, 210532, 265320, 604266, 3074667, 656498, 656499, 1650301, 2177151, 214656, 267393, 274559, 2357891, 597642, 71322, 2163357, 4174495, 2923169, 2164902, 720039, 131754, 3333291, 1655475, 3427508, 209081, 324794, 1992381, 418497, 3140293, 890568, 179400, 2939599, 1019601, 806611, 3379411, 864483, 645351, 398058, 497900, 626415, 6823664, 2377973, 995063, 2166520, 888080, 567059, 2972438, 3436824, 162076, 905510, 4224295, 1774887, 939815, 60199, 115507, 461114, 2112828, 501564, 557373, 211783, 2397517, 136014, 542558, 2580321, 48481, 609642, 154479, 3325308, 290684, 5276032, 313218, 278405, 502662, 82825, 2848139, 2848140, 2432908, 1622415, 1262999, 360859, 482717, 315297, 487843, 1993635, 786856, 324524, 3169711, 2715570, 483766, 551353, 505786, 408507, 923070, 488900, 726471, 3059655, 3592656, 2642385, 2589650, 820181, 1348055, 2228188, 772063, 47079, 2430954, 112109, 242158, 3049968, 2171378, 1020915, 377337, 792061}\n",
      "Number of unique embeddings with GIN: 138\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {14852, 4616, 5128, 4108, 4621, 9748, 4125, 6686, 5153, 13355, 5167, 4658, 5177, 11834, 8763, 6722, 5191, 5707, 11856, 8784, 26706, 11872, 24160, 7269, 18533, 16485, 6249, 11885, 5747, 4728, 13433, 5246, 6785, 14978, 122502, 6283, 4748, 16531, 4755, 16025, 8861, 7325, 13473, 5282, 8869, 30890, 6322, 4788, 6325, 8374, 6326, 5304, 7353, 4798, 5314, 20674, 12485, 15048, 8909, 12495, 8400, 5335, 5849, 10461, 7909, 14576, 5873, 5875, 6391, 42744, 6395, 12544, 3842, 9989, 7942, 11535, 12560, 8476, 5920, 5923, 25893, 41256, 11051, 9006, 7471, 4916, 8503, 13623, 5433, 4922, 89913, 12605, 7486, 7488, 9538, 5442, 5453, 13136, 9553, 4944, 5459, 14683, 13671, 9082, 4480, 6019, 6021, 8072, 6027, 62865, 6040, 8608, 12193, 5024, 7076, 6053, 6566, 13739, 4013, 11192, 4537, 14783, 10693, 8135, 5577, 12235, 11728, 5072, 5081, 12764, 26591, 12769, 4068, 5607, 75240, 8699, 8188, 4607}\n",
      "Number of unique embeddings with PNA: 138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/Extension/clo_G_Extension_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75397a96-0e64-4629-8c23-4f9ab097282c",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a48feba-f7ee-4eed-acdf-ce36c8e82000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Extension/clo_G_Extension_dataset_dummy.pkl...\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 0.3370\n",
      "Epoch 2/100, Loss: 0.4768\n",
      "Epoch 3/100, Loss: 0.3297\n",
      "Epoch 4/100, Loss: 0.3003\n",
      "Epoch 5/100, Loss: 0.2898\n",
      "Epoch 6/100, Loss: 0.3188\n",
      "Epoch 7/100, Loss: 0.3116\n",
      "Epoch 8/100, Loss: 0.3287\n",
      "Epoch 9/100, Loss: 0.2343\n",
      "Epoch 10/100, Loss: 0.4425\n",
      "Epoch 11/100, Loss: 0.2793\n",
      "Epoch 12/100, Loss: 0.2280\n",
      "Epoch 13/100, Loss: 0.2268\n",
      "Epoch 14/100, Loss: 0.2006\n",
      "Epoch 15/100, Loss: 0.1600\n",
      "Epoch 16/100, Loss: 0.2388\n",
      "Epoch 17/100, Loss: 0.0951\n",
      "Epoch 18/100, Loss: 0.1597\n",
      "Epoch 19/100, Loss: 0.3098\n",
      "Epoch 20/100, Loss: 0.1525\n",
      "Epoch 21/100, Loss: 0.1315\n",
      "Epoch 22/100, Loss: 0.1272\n",
      "Epoch 23/100, Loss: 0.1365\n",
      "Epoch 24/100, Loss: 0.0771\n",
      "Epoch 25/100, Loss: 0.1878\n",
      "Epoch 26/100, Loss: 0.0998\n",
      "Epoch 27/100, Loss: 0.1096\n",
      "Epoch 28/100, Loss: 0.0872\n",
      "Epoch 29/100, Loss: 0.1406\n",
      "Epoch 30/100, Loss: 0.0898\n",
      "Epoch 31/100, Loss: 0.0723\n",
      "Epoch 32/100, Loss: 0.0944\n",
      "Epoch 33/100, Loss: 0.1563\n",
      "Epoch 34/100, Loss: 0.1233\n",
      "Epoch 35/100, Loss: 0.1015\n",
      "Epoch 36/100, Loss: 0.1121\n",
      "Epoch 37/100, Loss: 0.0935\n",
      "Epoch 38/100, Loss: 0.0614\n",
      "Epoch 39/100, Loss: 0.0953\n",
      "Epoch 40/100, Loss: 0.0909\n",
      "Epoch 41/100, Loss: 0.1189\n",
      "Epoch 42/100, Loss: 0.1391\n",
      "Epoch 43/100, Loss: 0.1628\n",
      "Epoch 44/100, Loss: 0.1324\n",
      "Epoch 45/100, Loss: 0.1208\n",
      "Epoch 46/100, Loss: 0.0540\n",
      "Epoch 47/100, Loss: 0.0953\n",
      "Epoch 48/100, Loss: 0.0471\n",
      "Epoch 49/100, Loss: 0.0617\n",
      "Epoch 50/100, Loss: 0.1376\n",
      "Epoch 51/100, Loss: 0.2472\n",
      "Epoch 52/100, Loss: 0.1091\n",
      "Epoch 53/100, Loss: 0.0540\n",
      "Epoch 54/100, Loss: 0.0611\n",
      "Epoch 55/100, Loss: 0.0523\n",
      "Epoch 56/100, Loss: 0.0231\n",
      "Epoch 57/100, Loss: 0.0244\n",
      "Epoch 58/100, Loss: 0.0425\n",
      "Epoch 59/100, Loss: 0.0336\n",
      "Epoch 60/100, Loss: 0.0532\n",
      "Epoch 61/100, Loss: 0.0274\n",
      "Epoch 62/100, Loss: 0.0075\n",
      "Epoch 63/100, Loss: 0.1585\n",
      "Epoch 64/100, Loss: 0.0708\n",
      "Epoch 65/100, Loss: 0.0369\n",
      "Epoch 66/100, Loss: 0.0189\n",
      "Epoch 67/100, Loss: 0.0387\n",
      "Epoch 68/100, Loss: 0.0907\n",
      "Epoch 69/100, Loss: 0.0166\n",
      "Epoch 70/100, Loss: 0.0818\n",
      "Epoch 71/100, Loss: 0.0940\n",
      "Epoch 72/100, Loss: 0.0456\n",
      "Epoch 73/100, Loss: 0.0575\n",
      "Epoch 74/100, Loss: 0.0588\n",
      "Epoch 75/100, Loss: 0.0199\n",
      "Epoch 76/100, Loss: 0.0179\n",
      "Epoch 77/100, Loss: 0.0236\n",
      "Epoch 78/100, Loss: 0.0380\n",
      "Epoch 79/100, Loss: 0.0479\n",
      "Epoch 80/100, Loss: 0.0460\n",
      "Epoch 81/100, Loss: 0.0297\n",
      "Epoch 82/100, Loss: 0.0250\n",
      "Epoch 83/100, Loss: 0.0059\n",
      "Epoch 84/100, Loss: 0.0270\n",
      "Epoch 85/100, Loss: 0.0347\n",
      "Epoch 86/100, Loss: 0.0969\n",
      "Epoch 87/100, Loss: 0.0504\n",
      "Epoch 88/100, Loss: 0.0406\n",
      "Epoch 89/100, Loss: 0.0487\n",
      "Epoch 90/100, Loss: 0.0294\n",
      "Epoch 91/100, Loss: 0.0237\n",
      "Epoch 92/100, Loss: 0.0225\n",
      "Epoch 93/100, Loss: 0.0174\n",
      "Epoch 94/100, Loss: 0.0149\n",
      "Epoch 95/100, Loss: 0.1047\n",
      "Epoch 96/100, Loss: 0.0672\n",
      "Epoch 97/100, Loss: 0.0296\n",
      "Epoch 98/100, Loss: 0.0236\n",
      "Epoch 99/100, Loss: 0.0360\n",
      "Epoch 100/100, Loss: 0.0344\n",
      "Correctly classified non-isomorphic pairs: 30\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.3901\n",
      "Epoch 2/100, Loss: 1.3010\n",
      "Epoch 3/100, Loss: 1.1213\n",
      "Epoch 4/100, Loss: 1.1756\n",
      "Epoch 5/100, Loss: 1.0235\n",
      "Epoch 6/100, Loss: 0.9023\n",
      "Epoch 7/100, Loss: 0.8074\n",
      "Epoch 8/100, Loss: 0.7688\n",
      "Epoch 9/100, Loss: 0.5383\n",
      "Epoch 10/100, Loss: 0.4940\n",
      "Epoch 11/100, Loss: 0.5103\n",
      "Epoch 12/100, Loss: 0.5673\n",
      "Epoch 13/100, Loss: 0.6263\n",
      "Epoch 14/100, Loss: 0.2886\n",
      "Epoch 15/100, Loss: 0.3942\n",
      "Epoch 16/100, Loss: 0.3112\n",
      "Epoch 17/100, Loss: 0.1935\n",
      "Epoch 18/100, Loss: 0.2202\n",
      "Epoch 19/100, Loss: 0.1838\n",
      "Epoch 20/100, Loss: 0.0815\n",
      "Epoch 21/100, Loss: 0.0605\n",
      "Epoch 22/100, Loss: 0.1042\n",
      "Epoch 23/100, Loss: 0.0850\n",
      "Epoch 24/100, Loss: 0.2313\n",
      "Epoch 25/100, Loss: 0.1708\n",
      "Epoch 26/100, Loss: 0.0293\n",
      "Epoch 27/100, Loss: 0.0468\n",
      "Epoch 28/100, Loss: 0.0534\n",
      "Epoch 29/100, Loss: 0.0359\n",
      "Epoch 30/100, Loss: 0.0428\n",
      "Epoch 31/100, Loss: 0.0235\n",
      "Epoch 32/100, Loss: 0.0536\n",
      "Epoch 33/100, Loss: 0.0422\n",
      "Epoch 34/100, Loss: 0.0290\n",
      "Epoch 35/100, Loss: 0.0329\n",
      "Epoch 36/100, Loss: 0.0509\n",
      "Epoch 37/100, Loss: 0.1389\n",
      "Epoch 38/100, Loss: 0.0476\n",
      "Epoch 39/100, Loss: 0.0392\n",
      "Epoch 40/100, Loss: 0.0054\n",
      "Epoch 41/100, Loss: 0.0198\n",
      "Epoch 42/100, Loss: 0.0359\n",
      "Epoch 43/100, Loss: 0.0605\n",
      "Epoch 44/100, Loss: 0.0067\n",
      "Epoch 45/100, Loss: 0.0075\n",
      "Epoch 46/100, Loss: 0.0398\n",
      "Epoch 47/100, Loss: 0.0924\n",
      "Epoch 48/100, Loss: 0.0919\n",
      "Epoch 49/100, Loss: 0.0438\n",
      "Epoch 50/100, Loss: 0.0056\n",
      "Epoch 51/100, Loss: 0.0526\n",
      "Epoch 52/100, Loss: 0.0180\n",
      "Epoch 53/100, Loss: 0.2100\n",
      "Epoch 54/100, Loss: 0.0486\n",
      "Epoch 55/100, Loss: 0.1017\n",
      "Epoch 56/100, Loss: 0.0309\n",
      "Epoch 57/100, Loss: 0.0005\n",
      "Epoch 58/100, Loss: 0.0070\n",
      "Epoch 59/100, Loss: 0.0010\n",
      "Epoch 60/100, Loss: 0.0005\n",
      "Epoch 61/100, Loss: 0.0659\n",
      "Epoch 62/100, Loss: 0.0273\n",
      "Epoch 63/100, Loss: 0.0372\n",
      "Epoch 64/100, Loss: 0.0162\n",
      "Epoch 65/100, Loss: 0.0094\n",
      "Epoch 66/100, Loss: 0.0014\n",
      "Epoch 67/100, Loss: 0.1251\n",
      "Epoch 68/100, Loss: 0.0385\n",
      "Epoch 69/100, Loss: 0.0239\n",
      "Epoch 70/100, Loss: 0.0144\n",
      "Epoch 71/100, Loss: 0.0805\n",
      "Epoch 72/100, Loss: 0.0008\n",
      "Epoch 73/100, Loss: 0.0017\n",
      "Epoch 74/100, Loss: 0.0042\n",
      "Epoch 75/100, Loss: 0.0040\n",
      "Epoch 76/100, Loss: 0.0015\n",
      "Epoch 77/100, Loss: 0.0184\n",
      "Epoch 78/100, Loss: 0.0003\n",
      "Epoch 79/100, Loss: 0.0097\n",
      "Epoch 80/100, Loss: 0.0057\n",
      "Epoch 81/100, Loss: 0.0116\n",
      "Epoch 82/100, Loss: 0.0015\n",
      "Epoch 83/100, Loss: 0.0078\n",
      "Epoch 84/100, Loss: 0.0004\n",
      "Epoch 85/100, Loss: 0.0583\n",
      "Epoch 86/100, Loss: 0.0170\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0024\n",
      "Epoch 90/100, Loss: 0.0005\n",
      "Epoch 91/100, Loss: 0.0196\n",
      "Epoch 92/100, Loss: 0.0108\n",
      "Epoch 93/100, Loss: 0.0072\n",
      "Epoch 94/100, Loss: 0.0147\n",
      "Epoch 95/100, Loss: 0.0013\n",
      "Epoch 96/100, Loss: 0.0018\n",
      "Epoch 97/100, Loss: 0.0339\n",
      "Epoch 98/100, Loss: 0.0111\n",
      "Epoch 99/100, Loss: 0.0076\n",
      "Epoch 100/100, Loss: 0.0405\n",
      "Correctly classified non-isomorphic pairs: 41\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/Extension/clo_G_Extension_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_Extension_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcd9ef-67da-4df0-90a9-32ba31f4fc11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BREC-CFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a81fec-116b-489a-b14e-3e6c33785718",
   "metadata": {},
   "source": [
    "## Original CFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6a4a6-a48e-48cd-a85f-c198e920abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_CFI_dgl_graphs = []\n",
    "for G in G_CFI:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_CFI_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/CFI/G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_CFI_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_CFI_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d9280-8c6f-43c1-8857-70e29599f3f6",
   "metadata": {},
   "source": [
    "## CFI dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56031fc5-0174-4fba-8616-3f24fddf020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_CFI_dgl_graphs, G_CFI_isomorphic_pair, G_CFI_original_indices = add_isomorphic_pairs_dgl(G_CFI_dgl_graphs, num_pairs=100)\n",
    "G_CFI_dummy_dgl = G_CFI_dgl_graphs + G_CFI_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/CFI/G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9d3d2-2599-4646-ae05-17fe30c23d8c",
   "metadata": {},
   "source": [
    "## VN on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da83f8-ea37-42bc-a572-45f23d65e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_G_CFI_dgl = apply_vn(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/vn_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_CFI_dummy_dgl = apply_vn(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/vn_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d6d76-4fca-4144-a3b8-911767832cf4",
   "metadata": {},
   "source": [
    "## Degree Centrality on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c586595-8211-4057-9669-aaf935b47f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_G_CFI_dgl = degree_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/deg_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_CFI_dummy_dgl = degree_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/deg_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713a06c-8e1f-4d6d-b462-4fd73a2512d4",
   "metadata": {},
   "source": [
    "## Closeness Centrality on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "698f172c-d171-4a7c-b60a-c1f5790f2129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 200 graphs to DGL format and saved to ../data/BREC/CFI/clo_G_CFI_dataset.pkl.\n",
      "Converted 300 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/CFI/clo_G_CFI_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_CFI_dgl = closeness_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/clo_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_CFI_dummy_dgl = closeness_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/clo_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f62cfb-f605-4ac8-98fa-adace22ee347",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6afe9-1773-46cb-90f9-c90bdedf7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_G_CFI_dgl = betweenness_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/bet_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_CFI_dummy_dgl = betweenness_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/bet_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497f8a6-a0ea-4429-92fd-d60f72691705",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96deba-bbc6-47de-9498-c596dae1e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_G_CFI_dgl = eigenvector_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/eig_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_CFI_dummy_dgl = eigenvector_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/eig_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea6c42-d4d4-4397-9ce0-3fe882febb1b",
   "metadata": {},
   "source": [
    "## Distance Encoding on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564363fe-6813-416b-ae36-17422835873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_G_CFI_dgl = distance_encoding(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/DE_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_CFI_dummy_dgl = distance_encoding(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/DE_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e563c5-9565-44bb-86a5-2feed8c9601c",
   "metadata": {},
   "source": [
    "## Graph Encoding on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2a111-cbce-4362-b760-1c09de358e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_G_CFI_dgl = Graph_encoding(G_CFI_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/CFI/GE_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_CFI_dummy_dgl = Graph_encoding(G_CFI_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/CFI/GE_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915607f0-1aa5-4f23-8e31-b7de7aa7dc71",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7652bd-4d52-4ad3-bd2d-bdf6b1f30df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_G_CFI_dgl = subgraph_dataset(G_CFI_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/CFI/SE_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_CFI_dummy_dgl = subgraph_dataset(G_CFI_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/CFI/SE_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984a298-8116-4127-a6c7-26e984c74780",
   "metadata": {},
   "source": [
    "## Extra Node on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057236e-e081-41dd-8313-65deaa473048",
   "metadata": {},
   "outputs": [],
   "source": [
    "exN_G_CFI_dgl = extra_node_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/exN_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_CFI_dummy_dgl = extra_node_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/exN_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e648666-c99b-402b-bda5-7e3c918c7cd5",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on CFI original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494e3d8-a265-4d4e-abb0-596a67bbeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gle_G_CFI_dgl = graphlet_encoding_dataset(G_CFI_dgl_graphs)\n",
    "output_file = '../data/BREC/CFI/gle_G_CFI_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_CFI_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_CFI_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_CFI_dummy_dgl = graphlet_encoding_dataset(G_CFI_dummy_dgl)\n",
    "output_file = '../data/BREC/CFI/gle_G_CFI_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_CFI_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_CFI_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29cfa6-141e-4597-9718-8fa030d859be",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "51fd3cab-df44-4b56-adc2-bb71f1fcc902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/CFI/clo_G_CFI_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {56847363, 52558862, 59414, 549910, 738841, 35756059, 383016, 154153, 180791, 82492, 466503, 84552, 398921, 330826, 67659, 141386, 255569, 262740, 163929, 410714, 79467, 70260, 54430330, 4988539, 32961676, 159373, 90258, 188564, 129694, 276127, 37970590, 85669, 836775, 318122, 127154, 683698, 257218, 209609, 168678, 327910, 324334, 178416, 644339, 103670, 580859, 406270, 278783, 62730, 107671820, 125205, 59163, 76573, 307491, 1844515, 59177, 569131, 265006, 821555, 924472, 63808, 166218, 33661262, 642898, 107862, 34032985, 322907, 42834782, 329057, 123240, 103201652, 25975, 189818, 198010, 34686, 234880, 128896, 785794, 81284, 134799237, 578957, 215440, 145300, 144788, 59806, 32812962, 32680, 44477866, 42412, 317873, 32551353, 312763, 392636, 124864, 115137, 556483, 264647, 266188, 165841, 316882, 226258, 325597, 161246, 369123, 197604, 506854, 756209, 43963893, 327673}\n",
      "Number of unique embeddings with GIN: 108\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {22027, 90136, 21017, 121885, 21539, 107047, 491049, 80427, 491054, 7727, 80434, 17978, 140858, 81477, 32325, 1241159, 21580, 74828, 805966, 118355, 15972, 24169, 13938, 15987, 25735, 18574, 109202, 560276, 37017, 18076, 74399, 30372, 109733, 1237670, 24235, 34989, 91825, 580788, 493752, 16058, 76997, 16584, 108744, 624842, 37070, 109774, 22225, 100563, 16599, 74968, 100575, 91884, 35565, 17657, 98554, 18171, 479996, 15613, 23293, 18177, 38658, 21250, 98562, 1299205, 28934, 16151, 22296, 24358, 36666, 37187, 41814, 94551, 137561, 24412, 98140, 24415, 1006449, 132977, 80753, 20855, 94588, 15746, 119686, 93580, 80782, 28567, 93594, 21920, 143271, 20905, 32174, 25014, 80833, 191434, 37326, 664534, 766423, 21978, 80349, 105438, 28638, 7653, 444392, 88049, 28662, 17910, 840186, 16383}\n",
      "Number of unique embeddings with PNA: 108\n",
      "\n",
      "------------- Processing ../data/BREC/CFI/clo_G_CFI_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {269319, 4140553, 662032, 306707, 187414, 643097, 566297, 246817, 428071, 149032, 256053, 3504188, 230463, 5730881, 877636, 1248325, 382022, 93769, 235082, 302668, 618581, 405079, 183385, 5887596, 5502575, 244336, 291957, 9719927, 1009285, 168589, 450718, 288419, 443044, 561832, 351414, 307387, 79043, 494281, 338128, 250580, 1188060, 500960, 398570, 3505906, 316157, 238846, 726286, 560911, 250129, 628498, 1403671, 95514, 223003, 3233567, 163619, 433957, 842027, 603950, 102705, 358194, 566069, 245059, 3181386, 246096, 4721492, 595290, 490330, 176991, 373087, 13026151, 293735, 333675, 150380, 248686, 257390, 270708, 288629, 234358, 267128, 428921, 291194, 499079, 682382, 97168, 253845, 167321, 220578, 425382, 492969, 507305, 190379, 406445, 685997, 4901819, 15279547, 504768, 474562, 4690372, 3221452, 312275, 3405268, 308703, 66017, 660459, 158188, 208372, 210422, 1713659}\n",
      "Number of unique embeddings with GIN: 108\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {69122, 142345, 177679, 98832, 141344, 122409, 77876, 72248, 158780, 84543, 199237, 142405, 70217, 138830, 122449, 71251, 117758, 72799, 68705, 71778, 70763, 1736818, 70260, 3489912, 1758844, 125054, 70272, 70793, 223881, 133258, 3171470, 125073, 72339, 123029, 67738, 176286, 74919, 1675945, 1736362, 69810, 70834, 110264, 143544, 112825, 70849, 96978, 82143, 170720, 2385124, 144103, 4679401, 91372, 80625, 84209, 6080757, 122614, 125176, 76025, 133376, 73990, 77576, 71946, 168203, 68884, 1244438, 138529, 150310, 3673895, 210731, 2235693, 134449, 77625, 224063, 70465, 140620, 141142, 68954, 6038366, 69988, 72550, 2151275, 73583, 75633, 70002, 72573, 69513, 74122, 1667981, 133014, 74135, 70054, 141239, 78263, 134584, 73149, 141761, 172483, 77768, 73171, 115161, 83418, 70622, 70114, 74732, 6534645, 72698, 78331, 69630}\n",
      "Number of unique embeddings with PNA: 108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/CFI/clo_G_CFI_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6cf2d0-a8ee-4152-8b68-1595dbd9d63e",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "85d0de91-e379-4bab-9ce5-d716233571cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/CFI/clo_G_CFI_dataset_dummy.pkl...\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.5617\n",
      "Epoch 2/100, Loss: 1.1333\n",
      "Epoch 3/100, Loss: 1.1500\n",
      "Epoch 4/100, Loss: 1.2029\n",
      "Epoch 5/100, Loss: 1.2136\n",
      "Epoch 6/100, Loss: 1.0650\n",
      "Epoch 7/100, Loss: 1.2400\n",
      "Epoch 8/100, Loss: 1.0281\n",
      "Epoch 9/100, Loss: 1.2741\n",
      "Epoch 10/100, Loss: 1.0241\n",
      "Epoch 11/100, Loss: 1.0919\n",
      "Epoch 12/100, Loss: 1.1757\n",
      "Epoch 13/100, Loss: 1.2222\n",
      "Epoch 14/100, Loss: 1.2038\n",
      "Epoch 15/100, Loss: 1.1877\n",
      "Epoch 16/100, Loss: 0.9175\n",
      "Epoch 17/100, Loss: 1.1704\n",
      "Epoch 18/100, Loss: 1.1847\n",
      "Epoch 19/100, Loss: 1.0567\n",
      "Epoch 20/100, Loss: 1.4482\n",
      "Epoch 21/100, Loss: 1.1147\n",
      "Epoch 22/100, Loss: 1.0983\n",
      "Epoch 23/100, Loss: 1.0925\n",
      "Epoch 24/100, Loss: 1.2326\n",
      "Epoch 25/100, Loss: 0.9373\n",
      "Epoch 26/100, Loss: 1.2782\n",
      "Epoch 27/100, Loss: 1.0371\n",
      "Epoch 28/100, Loss: 1.3068\n",
      "Epoch 29/100, Loss: 1.0910\n",
      "Epoch 30/100, Loss: 1.0186\n",
      "Epoch 31/100, Loss: 1.2213\n",
      "Epoch 32/100, Loss: 1.3757\n",
      "Epoch 33/100, Loss: 1.0726\n",
      "Epoch 34/100, Loss: 1.1949\n",
      "Epoch 35/100, Loss: 1.0664\n",
      "Epoch 36/100, Loss: 0.9677\n",
      "Epoch 37/100, Loss: 1.0804\n",
      "Epoch 38/100, Loss: 1.0570\n",
      "Epoch 39/100, Loss: 1.0863\n",
      "Epoch 40/100, Loss: 1.1430\n",
      "Epoch 41/100, Loss: 1.2910\n",
      "Epoch 42/100, Loss: 1.2748\n",
      "Epoch 43/100, Loss: 1.1256\n",
      "Epoch 44/100, Loss: 1.0189\n",
      "Epoch 45/100, Loss: 1.0882\n",
      "Epoch 46/100, Loss: 1.1993\n",
      "Epoch 47/100, Loss: 1.0348\n",
      "Epoch 48/100, Loss: 1.0638\n",
      "Epoch 49/100, Loss: 1.1268\n",
      "Epoch 50/100, Loss: 1.0712\n",
      "Epoch 51/100, Loss: 1.1329\n",
      "Epoch 52/100, Loss: 1.0548\n",
      "Epoch 53/100, Loss: 1.0333\n",
      "Epoch 54/100, Loss: 1.0197\n",
      "Epoch 55/100, Loss: 1.0940\n",
      "Epoch 56/100, Loss: 1.0629\n",
      "Epoch 57/100, Loss: 1.0538\n",
      "Epoch 58/100, Loss: 1.1808\n",
      "Epoch 59/100, Loss: 0.9503\n",
      "Epoch 60/100, Loss: 0.9788\n",
      "Epoch 61/100, Loss: 1.0550\n",
      "Epoch 62/100, Loss: 1.0740\n",
      "Epoch 63/100, Loss: 0.8227\n",
      "Epoch 64/100, Loss: 0.9171\n",
      "Epoch 65/100, Loss: 0.8920\n",
      "Epoch 66/100, Loss: 0.9489\n",
      "Epoch 67/100, Loss: 1.3889\n",
      "Epoch 68/100, Loss: 1.1237\n",
      "Epoch 69/100, Loss: 1.0036\n",
      "Epoch 70/100, Loss: 1.1818\n",
      "Epoch 71/100, Loss: 1.2013\n",
      "Epoch 72/100, Loss: 1.0135\n",
      "Epoch 73/100, Loss: 1.1719\n",
      "Epoch 74/100, Loss: 0.9120\n",
      "Epoch 75/100, Loss: 1.1066\n",
      "Epoch 76/100, Loss: 1.0970\n",
      "Epoch 77/100, Loss: 1.1015\n",
      "Epoch 78/100, Loss: 0.9572\n",
      "Epoch 79/100, Loss: 0.8618\n",
      "Epoch 80/100, Loss: 0.8490\n",
      "Epoch 81/100, Loss: 0.8732\n",
      "Epoch 82/100, Loss: 0.9495\n",
      "Epoch 83/100, Loss: 0.8401\n",
      "Epoch 84/100, Loss: 0.8815\n",
      "Epoch 85/100, Loss: 1.1793\n",
      "Epoch 86/100, Loss: 0.8536\n",
      "Epoch 87/100, Loss: 0.8505\n",
      "Epoch 88/100, Loss: 0.9332\n",
      "Epoch 89/100, Loss: 0.9809\n",
      "Epoch 90/100, Loss: 0.9483\n",
      "Epoch 91/100, Loss: 0.8479\n",
      "Epoch 92/100, Loss: 0.8920\n",
      "Epoch 93/100, Loss: 0.9304\n",
      "Epoch 94/100, Loss: 0.9135\n",
      "Epoch 95/100, Loss: 1.0518\n",
      "Epoch 96/100, Loss: 0.7371\n",
      "Epoch 97/100, Loss: 0.8510\n",
      "Epoch 98/100, Loss: 0.8402\n",
      "Epoch 99/100, Loss: 0.7936\n",
      "Epoch 100/100, Loss: 0.7107\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.7359\n",
      "Epoch 2/100, Loss: 1.6443\n",
      "Epoch 3/100, Loss: 1.5603\n",
      "Epoch 4/100, Loss: 1.5594\n",
      "Epoch 5/100, Loss: 1.3285\n",
      "Epoch 6/100, Loss: 1.2730\n",
      "Epoch 7/100, Loss: 1.1590\n",
      "Epoch 8/100, Loss: 1.4296\n",
      "Epoch 9/100, Loss: 1.0205\n",
      "Epoch 10/100, Loss: 1.3787\n",
      "Epoch 11/100, Loss: 1.0250\n",
      "Epoch 12/100, Loss: 1.0056\n",
      "Epoch 13/100, Loss: 0.6839\n",
      "Epoch 14/100, Loss: 0.6618\n",
      "Epoch 15/100, Loss: 0.5937\n",
      "Epoch 16/100, Loss: 1.0207\n",
      "Epoch 17/100, Loss: 0.9696\n",
      "Epoch 18/100, Loss: 0.3965\n",
      "Epoch 19/100, Loss: 0.3403\n",
      "Epoch 20/100, Loss: 0.5289\n",
      "Epoch 21/100, Loss: 0.8833\n",
      "Epoch 22/100, Loss: 0.8861\n",
      "Epoch 23/100, Loss: 0.4489\n",
      "Epoch 24/100, Loss: 0.8249\n",
      "Epoch 25/100, Loss: 0.3795\n",
      "Epoch 26/100, Loss: 0.9511\n",
      "Epoch 27/100, Loss: 0.2510\n",
      "Epoch 28/100, Loss: 0.9218\n",
      "Epoch 29/100, Loss: 0.1804\n",
      "Epoch 30/100, Loss: 0.2963\n",
      "Epoch 31/100, Loss: 0.1383\n",
      "Epoch 32/100, Loss: 0.1141\n",
      "Epoch 33/100, Loss: 0.3077\n",
      "Epoch 34/100, Loss: 0.1323\n",
      "Epoch 35/100, Loss: 0.3449\n",
      "Epoch 36/100, Loss: 0.1210\n",
      "Epoch 37/100, Loss: 0.0617\n",
      "Epoch 38/100, Loss: 0.0674\n",
      "Epoch 39/100, Loss: 0.5425\n",
      "Epoch 40/100, Loss: 0.3098\n",
      "Epoch 41/100, Loss: 0.4283\n",
      "Epoch 42/100, Loss: 0.3165\n",
      "Epoch 43/100, Loss: 0.2427\n",
      "Epoch 44/100, Loss: 0.1677\n",
      "Epoch 45/100, Loss: 0.2585\n",
      "Epoch 46/100, Loss: 0.5960\n",
      "Epoch 47/100, Loss: 0.2673\n",
      "Epoch 48/100, Loss: 0.3188\n",
      "Epoch 49/100, Loss: 0.6615\n",
      "Epoch 50/100, Loss: 0.5589\n",
      "Epoch 51/100, Loss: 0.8270\n",
      "Epoch 52/100, Loss: 0.1417\n",
      "Epoch 53/100, Loss: 0.1874\n",
      "Epoch 54/100, Loss: 0.0503\n",
      "Epoch 55/100, Loss: 0.5221\n",
      "Epoch 56/100, Loss: 0.0358\n",
      "Epoch 57/100, Loss: 0.2739\n",
      "Epoch 58/100, Loss: 0.3752\n",
      "Epoch 59/100, Loss: 0.1071\n",
      "Epoch 60/100, Loss: 0.1766\n",
      "Epoch 61/100, Loss: 0.7326\n",
      "Epoch 62/100, Loss: 0.2047\n",
      "Epoch 63/100, Loss: 0.4235\n",
      "Epoch 64/100, Loss: 0.5701\n",
      "Epoch 65/100, Loss: 0.9222\n",
      "Epoch 66/100, Loss: 0.6480\n",
      "Epoch 67/100, Loss: 0.8657\n",
      "Epoch 68/100, Loss: 0.6780\n",
      "Epoch 69/100, Loss: 0.3042\n",
      "Epoch 70/100, Loss: 0.5316\n",
      "Epoch 71/100, Loss: 0.8898\n",
      "Epoch 72/100, Loss: 0.4561\n",
      "Epoch 73/100, Loss: 0.5177\n",
      "Epoch 74/100, Loss: 1.0547\n",
      "Epoch 75/100, Loss: 0.2400\n",
      "Epoch 76/100, Loss: 0.8485\n",
      "Epoch 77/100, Loss: 0.0929\n",
      "Epoch 78/100, Loss: 0.1057\n",
      "Epoch 79/100, Loss: 0.2918\n",
      "Epoch 80/100, Loss: 0.1140\n",
      "Epoch 81/100, Loss: 0.5138\n",
      "Epoch 82/100, Loss: 1.0062\n",
      "Epoch 83/100, Loss: 0.2164\n",
      "Epoch 84/100, Loss: 1.0150\n",
      "Epoch 85/100, Loss: 1.0248\n",
      "Epoch 86/100, Loss: 0.7708\n",
      "Epoch 87/100, Loss: 0.4789\n",
      "Epoch 88/100, Loss: 0.2812\n",
      "Epoch 89/100, Loss: 0.3524\n",
      "Epoch 90/100, Loss: 0.4306\n",
      "Epoch 91/100, Loss: 0.4382\n",
      "Epoch 92/100, Loss: 0.6518\n",
      "Epoch 93/100, Loss: 0.2253\n",
      "Epoch 94/100, Loss: 0.2523\n",
      "Epoch 95/100, Loss: 0.5231\n",
      "Epoch 96/100, Loss: 0.3705\n",
      "Epoch 97/100, Loss: 0.1481\n",
      "Epoch 98/100, Loss: 0.2312\n",
      "Epoch 99/100, Loss: 0.1339\n",
      "Epoch 100/100, Loss: 0.4050\n",
      "Correctly classified non-isomorphic pairs: 2\n",
      "Correctly classified isomorphic pairs: 100\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/CFI/clo_G_CFI_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_CFI_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ff85e-577f-4b80-a811-588871ed0166",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BREC-4Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae001d9-5714-423d-a573-a081e0aa4904",
   "metadata": {},
   "source": [
    "## Original 4-Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba6742-4f93-4930-975c-7bb348ff2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_4Vertex_dgl_graphs = []\n",
    "for G in G_4Vertex:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_4Vertex_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/Vertex/G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_4Vertex_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_4Vertex_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c4a14-b82b-4cb9-9dc4-262f33acfe4e",
   "metadata": {},
   "source": [
    "## 4Vertex dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a275a7-5af3-412f-9a1f-0901c6e7761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_4Vertex_dgl_graphs, G_4Vertex_isomorphic_pair, G_4Vertex_original_indices = add_isomorphic_pairs_dgl(G_4Vertex_dgl_graphs, num_pairs=20)\n",
    "G_4Vertex_dummy_dgl = G_4Vertex_dgl_graphs + G_4Vertex_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/Vertex/G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467a97c-de9d-4cc1-8efa-481637c2cc62",
   "metadata": {},
   "source": [
    "## VN on 4Vertex original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cc408-fc2a-4f06-9adf-377f90699709",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_G_4Vertex_dgl = apply_vn(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/vn_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_4Vertex_dummy_dgl = apply_vn(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/vn_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d6af-6c4a-49f6-9f0c-737a43014c7c",
   "metadata": {},
   "source": [
    "## Degree Centrality on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13bc61-1e80-4eba-aae0-33658fae25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_G_4Vertex_dgl = degree_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/deg_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_4Vertex_dummy_dgl = degree_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/deg_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ffa8a-e9eb-4f8b-b3ff-9ff830dd7ed3",
   "metadata": {},
   "source": [
    "## Closeness Centrality on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "86467c8b-9d0b-4953-8b28-80b067c05abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 40 graphs to DGL format and saved to ../data/BREC/Vertex/clo_G_4Vertex_dataset.pkl.\n",
      "Converted 60 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Vertex/clo_G_4Vertex_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_4Vertex_dgl = closeness_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/clo_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_4Vertex_dummy_dgl = closeness_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/clo_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacea6b9-3c8d-4f59-a81b-a764c56c99c7",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfe860-7363-445c-9e62-b7846942e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_G_4Vertex_dgl = betweenness_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/bet_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_4Vertex_dummy_dgl = betweenness_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/bet_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0d76f-0811-4040-9f7a-ff0395778406",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626adc9-c30a-46e5-9fd8-84f82f0e06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_G_4Vertex_dgl = eigenvector_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/eig_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_4Vertex_dummy_dgl = eigenvector_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/eig_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de7d94-1af6-49a2-ad18-388edfea994d",
   "metadata": {},
   "source": [
    "## Distance Encoding on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e2c22-92e4-4c8c-a6dd-db329ad0d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_G_4Vertex_dgl = distance_encoding(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/DE_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_4Vertex_dummy_dgl = distance_encoding(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/DE_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043eee80-6d33-41f1-a2b4-fd34dad23ce8",
   "metadata": {},
   "source": [
    "## Graph Encoding on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90774e-503e-422c-a16c-46494e96f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_G_4Vertex_dgl = Graph_encoding(G_4Vertex_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/Vertex/GE_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_4Vertex_dummy_dgl = Graph_encoding(G_4Vertex_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/Vertex/GE_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2c331-e75b-4c34-93ca-50be8a507223",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fa579-4c1b-4ed5-863e-e41ee5f3c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_G_4Vertex_dgl = subgraph_dataset(G_4Vertex_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/Vertex/SE_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_4Vertex_dummy_dgl = subgraph_dataset(G_4Vertex_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/Vertex/SE_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc591596-eddf-4ad4-a65a-69cf733c48ee",
   "metadata": {},
   "source": [
    "## Extra Node on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbf1a2-2590-4662-bb4a-ab17f2555b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "exN_G_4Vertex_dgl = extra_node_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/exN_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_4Vertex_dummy_dgl = extra_node_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/exN_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d1838-01a2-4b62-baf0-581bcc314369",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on 4Vertex original and 4Vertex dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0ae36-5a3c-4a78-a4bc-4f97fa3c439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gle_G_4Vertex_dgl = graphlet_encoding_dataset(G_4Vertex_dgl_graphs)\n",
    "output_file = '../data/BREC/Vertex/gle_G_4Vertex_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_4Vertex_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_4Vertex_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_4Vertex_dummy_dgl = graphlet_encoding_dataset(G_4Vertex_dummy_dgl)\n",
    "output_file = '../data/BREC/Vertex/gle_G_4Vertex_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_4Vertex_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_4Vertex_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a5e2b-bafd-4883-8f8e-404efc526809",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "db264587-7e6f-426f-be7d-f33ff9474be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Vertex/clo_G_4Vertex_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {78075286731}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {307884}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n",
      "------------- Processing ../data/BREC/Vertex/clo_G_4Vertex_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {951029692925}\n",
      "Number of unique embeddings with GIN: 1\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {258143}\n",
      "Number of unique embeddings with PNA: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/Vertex/clo_G_4Vertex_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fef3bc-0f0a-48ba-893d-9bc7f52c0d44",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "58e2089e-0939-4c45-9b27-e8c9d424bed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Vertex/clo_G_4Vertex_dataset_dummy.pkl...\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 1.0000\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 1.0000\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 1.0000\n",
      "Epoch 18/100, Loss: 1.0000\n",
      "Epoch 19/100, Loss: 1.0000\n",
      "Epoch 20/100, Loss: 1.0000\n",
      "Epoch 21/100, Loss: 1.0000\n",
      "Epoch 22/100, Loss: 1.0000\n",
      "Epoch 23/100, Loss: 1.0000\n",
      "Epoch 24/100, Loss: 1.0000\n",
      "Epoch 25/100, Loss: 0.1329\n",
      "Epoch 26/100, Loss: 1.0000\n",
      "Epoch 27/100, Loss: 1.0000\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 0.7000\n",
      "Epoch 30/100, Loss: 1.0000\n",
      "Epoch 31/100, Loss: 1.0000\n",
      "Epoch 32/100, Loss: 0.9983\n",
      "Epoch 33/100, Loss: 1.0000\n",
      "Epoch 34/100, Loss: 1.0000\n",
      "Epoch 35/100, Loss: 1.0000\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 1.0000\n",
      "Epoch 38/100, Loss: 0.9982\n",
      "Epoch 39/100, Loss: 0.9988\n",
      "Epoch 40/100, Loss: 1.0000\n",
      "Epoch 41/100, Loss: 0.9988\n",
      "Epoch 42/100, Loss: 1.0000\n",
      "Epoch 43/100, Loss: 1.0000\n",
      "Epoch 44/100, Loss: 1.0000\n",
      "Epoch 45/100, Loss: 1.0000\n",
      "Epoch 46/100, Loss: 1.0000\n",
      "Epoch 47/100, Loss: 0.9998\n",
      "Epoch 48/100, Loss: 0.9987\n",
      "Epoch 49/100, Loss: 0.9981\n",
      "Epoch 50/100, Loss: 1.0000\n",
      "Epoch 51/100, Loss: 1.0000\n",
      "Epoch 52/100, Loss: 1.0000\n",
      "Epoch 53/100, Loss: 0.9999\n",
      "Epoch 54/100, Loss: 0.9985\n",
      "Epoch 55/100, Loss: 1.0000\n",
      "Epoch 56/100, Loss: 0.9987\n",
      "Epoch 57/100, Loss: 0.9993\n",
      "Epoch 58/100, Loss: 1.0000\n",
      "Epoch 59/100, Loss: 1.0000\n",
      "Epoch 60/100, Loss: 0.9994\n",
      "Epoch 61/100, Loss: 0.8413\n",
      "Epoch 62/100, Loss: 1.0000\n",
      "Epoch 63/100, Loss: 0.9987\n",
      "Epoch 64/100, Loss: 0.9998\n",
      "Epoch 65/100, Loss: 0.9982\n",
      "Epoch 66/100, Loss: 0.9986\n",
      "Epoch 67/100, Loss: 0.9988\n",
      "Epoch 68/100, Loss: 0.9998\n",
      "Epoch 69/100, Loss: 0.9983\n",
      "Epoch 70/100, Loss: 1.0000\n",
      "Epoch 71/100, Loss: 1.0000\n",
      "Epoch 72/100, Loss: 1.0000\n",
      "Epoch 73/100, Loss: 0.9984\n",
      "Epoch 74/100, Loss: 0.6053\n",
      "Epoch 75/100, Loss: 0.9983\n",
      "Epoch 76/100, Loss: 0.9980\n",
      "Epoch 77/100, Loss: 0.9173\n",
      "Epoch 78/100, Loss: 0.9979\n",
      "Epoch 79/100, Loss: 0.9567\n",
      "Epoch 80/100, Loss: 1.0000\n",
      "Epoch 81/100, Loss: 0.9993\n",
      "Epoch 82/100, Loss: 0.9997\n",
      "Epoch 83/100, Loss: 0.9984\n",
      "Epoch 84/100, Loss: 0.9526\n",
      "Epoch 85/100, Loss: 1.0000\n",
      "Epoch 86/100, Loss: 1.0000\n",
      "Epoch 87/100, Loss: 0.9983\n",
      "Epoch 88/100, Loss: 1.0000\n",
      "Epoch 89/100, Loss: 0.9994\n",
      "Epoch 90/100, Loss: 0.9986\n",
      "Epoch 91/100, Loss: 1.0000\n",
      "Epoch 92/100, Loss: 0.9982\n",
      "Epoch 93/100, Loss: 1.0000\n",
      "Epoch 94/100, Loss: 0.9982\n",
      "Epoch 95/100, Loss: 0.8343\n",
      "Epoch 96/100, Loss: 0.9994\n",
      "Epoch 97/100, Loss: 0.9974\n",
      "Epoch 98/100, Loss: 1.0000\n",
      "Epoch 99/100, Loss: 0.9991\n",
      "Epoch 100/100, Loss: 0.9976\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 20\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 1.0000\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 1.0000\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 1.0000\n",
      "Epoch 18/100, Loss: 1.0000\n",
      "Epoch 19/100, Loss: 1.0000\n",
      "Epoch 20/100, Loss: 1.0000\n",
      "Epoch 21/100, Loss: 1.0000\n",
      "Epoch 22/100, Loss: 1.0000\n",
      "Epoch 23/100, Loss: 1.0000\n",
      "Epoch 24/100, Loss: 1.0000\n",
      "Epoch 25/100, Loss: 1.0000\n",
      "Epoch 26/100, Loss: 1.0000\n",
      "Epoch 27/100, Loss: 1.0000\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 1.0000\n",
      "Epoch 30/100, Loss: 1.0000\n",
      "Epoch 31/100, Loss: 1.0000\n",
      "Epoch 32/100, Loss: 1.0000\n",
      "Epoch 33/100, Loss: 1.0000\n",
      "Epoch 34/100, Loss: 1.0000\n",
      "Epoch 35/100, Loss: 1.0000\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 1.0000\n",
      "Epoch 38/100, Loss: 1.0000\n",
      "Epoch 39/100, Loss: 1.0000\n",
      "Epoch 40/100, Loss: 1.0000\n",
      "Epoch 41/100, Loss: 1.0000\n",
      "Epoch 42/100, Loss: 1.0000\n",
      "Epoch 43/100, Loss: 1.0000\n",
      "Epoch 44/100, Loss: 1.0000\n",
      "Epoch 45/100, Loss: 1.0000\n",
      "Epoch 46/100, Loss: 1.0000\n",
      "Epoch 47/100, Loss: 1.0000\n",
      "Epoch 48/100, Loss: 1.0000\n",
      "Epoch 49/100, Loss: 1.0000\n",
      "Epoch 50/100, Loss: 1.0000\n",
      "Epoch 51/100, Loss: 1.0000\n",
      "Epoch 52/100, Loss: 1.0000\n",
      "Epoch 53/100, Loss: 1.0000\n",
      "Epoch 54/100, Loss: 1.0000\n",
      "Epoch 55/100, Loss: 1.0000\n",
      "Epoch 56/100, Loss: 1.0000\n",
      "Epoch 57/100, Loss: 1.0000\n",
      "Epoch 58/100, Loss: 1.0000\n",
      "Epoch 59/100, Loss: 1.0000\n",
      "Epoch 60/100, Loss: 1.0000\n",
      "Epoch 61/100, Loss: 1.0000\n",
      "Epoch 62/100, Loss: 1.0000\n",
      "Epoch 63/100, Loss: 1.0000\n",
      "Epoch 64/100, Loss: 1.0000\n",
      "Epoch 65/100, Loss: 1.0000\n",
      "Epoch 66/100, Loss: 1.0000\n",
      "Epoch 67/100, Loss: 1.0000\n",
      "Epoch 68/100, Loss: 1.0000\n",
      "Epoch 69/100, Loss: 1.0000\n",
      "Epoch 70/100, Loss: 1.0000\n",
      "Epoch 71/100, Loss: 1.0000\n",
      "Epoch 72/100, Loss: 1.0000\n",
      "Epoch 73/100, Loss: 1.0000\n",
      "Epoch 74/100, Loss: 1.0000\n",
      "Epoch 75/100, Loss: 1.0000\n",
      "Epoch 76/100, Loss: 1.0000\n",
      "Epoch 77/100, Loss: 1.0000\n",
      "Epoch 78/100, Loss: 1.0000\n",
      "Epoch 79/100, Loss: 1.0000\n",
      "Epoch 80/100, Loss: 1.0000\n",
      "Epoch 81/100, Loss: 1.0000\n",
      "Epoch 82/100, Loss: 1.0000\n",
      "Epoch 83/100, Loss: 1.0000\n",
      "Epoch 84/100, Loss: 1.0000\n",
      "Epoch 85/100, Loss: 1.0000\n",
      "Epoch 86/100, Loss: 1.0000\n",
      "Epoch 87/100, Loss: 1.0000\n",
      "Epoch 88/100, Loss: 1.0000\n",
      "Epoch 89/100, Loss: 1.0000\n",
      "Epoch 90/100, Loss: 1.0000\n",
      "Epoch 91/100, Loss: 1.0000\n",
      "Epoch 92/100, Loss: 1.0000\n",
      "Epoch 93/100, Loss: 1.0000\n",
      "Epoch 94/100, Loss: 1.0000\n",
      "Epoch 95/100, Loss: 1.0000\n",
      "Epoch 96/100, Loss: 1.0000\n",
      "Epoch 97/100, Loss: 1.0000\n",
      "Epoch 98/100, Loss: 1.0000\n",
      "Epoch 99/100, Loss: 1.0000\n",
      "Epoch 100/100, Loss: 1.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 20\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/Vertex/clo_G_4Vertex_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_4Vertex_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6798efd-7dcc-43d8-9391-860cd2ccae27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BREC-Distance_Regular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dfdc3-3ac2-4a37-ae90-7c0a8525ac8e",
   "metadata": {},
   "source": [
    "## Original Distance_Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729752f-595c-48c4-9cd7-2b91aa1933ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch_geometric.utils import to_dgl, from_dgl\n",
    "\n",
    "G_Distance_Regular_dgl_graphs = []\n",
    "for G in G_Distance_Regular:\n",
    "    dgl_graph = dgl.from_networkx(G)\n",
    "    dgl_graph.ndata['x'] = torch.ones((G.number_of_nodes(), 1), dtype=torch.float32)\n",
    "    G_Distance_Regular_dgl_graphs.append(dgl_graph)\n",
    "\n",
    "\n",
    "# Step 4: Save the list of DGL graphs using pickling\n",
    "output_file = '../data/BREC/Distance_Regular/G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Distance_Regular_dgl_graphs, f)\n",
    "\n",
    "print(f\"Converted {len(G_Distance_Regular_dgl_graphs)} graphs to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4799e-7afc-4208-93fc-63cf4e0ef231",
   "metadata": {},
   "source": [
    "## Distance_Regular dummy with isomorphic pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488142cd-199b-43c7-bb7c-c6b61fafcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_Distance_Regular_dgl_graphs, G_Distance_Regular_isomorphic_pair, G_Distance_Regular_original_indices = add_isomorphic_pairs_dgl(G_Distance_Regular_dgl_graphs, num_pairs=20)\n",
    "G_Distance_Regular_dummy_dgl = G_Distance_Regular_dgl_graphs + G_Distance_Regular_isomorphic_pair\n",
    "\n",
    "output_file = '../data/BREC/Distance_Regular/G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06949e9-58a6-46bd-af93-82bfc94e5638",
   "metadata": {},
   "source": [
    "## VN on 4Vertex original and CFI dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914771f-f0cc-49ea-bbd7-1c6cb3d5c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_G_Distance_Regular_dgl = apply_vn(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/vn_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "vn_G_Distance_Regular_dummy_dgl = apply_vn(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/vn_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(vn_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(vn_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca0b4a-7c76-4761-a72e-c1a9034dc282",
   "metadata": {},
   "source": [
    "## Degree Centrality on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b607ddd-7421-4532-9b8c-1f78244e3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_G_Distance_Regular_dgl = degree_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/deg_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "deg_G_Distance_Regular_dummy_dgl = degree_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/deg_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(deg_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(deg_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd393eda-da98-4cc5-997d-bb8f537e426d",
   "metadata": {},
   "source": [
    "## Closeness Centrality on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9cd27e59-fd60-47a3-a011-57d9cc80c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 40 graphs to DGL format and saved to ../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset.pkl.\n",
      "Converted 60 graphs (including isomorphisms) to DGL format and saved to ../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset_dummy.pkl.\n"
     ]
    }
   ],
   "source": [
    "clo_G_Distance_Regular_dgl = closeness_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "clo_G_Distance_Regular_dummy_dgl = closeness_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(clo_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(clo_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cd49f-af40-4df5-8244-df43d1110189",
   "metadata": {},
   "source": [
    "## Betweenness Centrality on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6582256-780f-4583-b8c5-0af8ff491e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_G_Distance_Regular_dgl = betweenness_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/bet_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "bet_G_Distance_Regular_dummy_dgl = betweenness_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/bet_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(bet_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(bet_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d75af-06b1-4125-94ab-2cd4c2bf0c9d",
   "metadata": {},
   "source": [
    "## Eigenvector Centrality on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013b472-3392-4b88-8a8d-ead4ff73636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_G_Distance_Regular_dgl = eigenvector_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/eig_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "eig_G_Distance_Regular_dummy_dgl = eigenvector_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/eig_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(eig_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(eig_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbfcbc-5f6c-4a67-ac25-40cc2d5ed925",
   "metadata": {},
   "source": [
    "## Distance Encoding on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e04c8e-5e6a-45c5-a220-cd501fe356f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE_G_Distance_Regular_dgl = distance_encoding(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/DE_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "DE_G_Distance_Regular_dummy_dgl = distance_encoding(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/DE_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(DE_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(DE_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44c658-955b-4409-9c86-eec31ae85266",
   "metadata": {},
   "source": [
    "## Graph Encoding on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d4612-e3fd-4808-b4da-e9f36ef6e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_G_Distance_Regular_dgl = Graph_encoding(G_Distance_Regular_dgl_graphs, k=3)\n",
    "output_file = '../data/BREC/Distance_Regular/GE_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "GE_G_Distance_Regular_dummy_dgl = Graph_encoding(G_Distance_Regular_dummy_dgl, k=3)\n",
    "output_file = '../data/BREC/Distance_Regular/GE_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(GE_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(GE_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07438aa4-b314-459b-ac8a-611944694f0c",
   "metadata": {},
   "source": [
    "## Subgraph Extraction on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc9719-4642-4514-9fec-23161adedb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_G_Distance_Regular_dgl = subgraph_dataset(G_Distance_Regular_dgl_graphs, radius=3)\n",
    "output_file = '../data/BREC/Distance_Regular/SE_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "SE_G_Distance_Regular_dummy_dgl = subgraph_dataset(G_Distance_Regular_dummy_dgl, radius=3)\n",
    "output_file = '../data/BREC/Distance_Regular/SE_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(SE_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(SE_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7a8b9-1218-4f44-936e-ca5560b96992",
   "metadata": {},
   "source": [
    "## Extra Node on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0351f29-fdd4-42e0-ae90-e3f0cb9db46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exN_G_Distance_Regular_dgl = extra_node_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/exN_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "exN_G_Distance_Regular_dummy_dgl = extra_node_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/exN_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(exN_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(exN_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0d89f-be0c-4f3a-a2cd-2bd3e63be8c9",
   "metadata": {},
   "source": [
    "## Graphlet-Based Encoding on Distance_Regular original and Distance_Regular dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f98542-54b9-454a-8ac5-75cd7c024272",
   "metadata": {},
   "outputs": [],
   "source": [
    "gle_G_Distance_Regular_dgl = graphlet_encoding_dataset(G_Distance_Regular_dgl_graphs)\n",
    "output_file = '../data/BREC/Distance_Regular/gle_G_Distance_Regular_dataset.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Distance_Regular_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Distance_Regular_dgl)} graphs to DGL format and saved to {output_file}.\")\n",
    "\n",
    "gle_G_Distance_Regular_dummy_dgl = graphlet_encoding_dataset(G_Distance_Regular_dummy_dgl)\n",
    "output_file = '../data/BREC/Distance_Regular/gle_G_Distance_Regular_dataset_dummy.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(gle_G_Distance_Regular_dummy_dgl, f)\n",
    "\n",
    "print(f\"Converted {len(gle_G_Distance_Regular_dummy_dgl)} graphs (including isomorphisms) to DGL format and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d53e37-9a6e-4321-83ce-100e338c67eb",
   "metadata": {},
   "source": [
    "## Equivalence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8e4892e9-423a-4b3a-a3ad-658cc87de480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset_dummy.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {466849564612, 34785838543400, 6916859309, 2162355892912, 2523934398525, 322262944637, 62518170206}\n",
      "Number of unique embeddings with GIN: 7\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {267618, 618403, 256965, 466823, 216104, 279598, 68847}\n",
      "Number of unique embeddings with PNA: 7\n",
      "\n",
      "------------- Processing ../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset.pkl...\n",
      "Computing equivalence classes using GIN model...\n",
      "embeddings:  {2551778690, 1642392394, 12874603253, 16375521270, 488394101, 483693049, 1842147293}\n",
      "Number of unique embeddings with GIN: 7\n",
      "\n",
      "Computing equivalence classes using PNA model...\n",
      "embeddings:  {27291, 85065, 18154, 79086, 14550, 16251, 69724}\n",
      "Number of unique embeddings with PNA: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the final integer embedding\n",
    "def calculate_integer_embedding(embedding):\n",
    "    sum_x = embedding.sum().item()\n",
    "\n",
    "    # Handle variance calculation only if there's more than one element\n",
    "    if embedding.numel() > 1:\n",
    "        var_x = embedding.var().item()\n",
    "    else:\n",
    "        var_x = 0.0  # Set variance to 0 if there's only one element\n",
    "\n",
    "    min_x = embedding.min().item()\n",
    "\n",
    "    # Handle NaN variance by setting it to 0\n",
    "    if np.isnan(var_x):\n",
    "        var_x = 0.0\n",
    "\n",
    "    final_embedding = int((sum_x * 100 + var_x * 10 + min_x * 10) * 10)\n",
    "    return final_embedding\n",
    "\n",
    "\n",
    "# Save graphs to a file\n",
    "def save_graphs_to_file(graphs, filepath):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(graphs, f)\n",
    "\n",
    "# Load graphs from a file\n",
    "def load_graphs_from_file(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "# Compute equivalence classes\n",
    "def compute_equivalence_classes(filepath, model, input_dim):\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    # Train the model for a single epoch with a random target\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    target_seed = 100\n",
    "    for g in graphs:\n",
    "        # Ensure that 'h' exists\n",
    "        if 'x' not in g.ndata:\n",
    "            g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(g, g.ndata['x'])  # The output is a tensor of shape (1, out_dim)\n",
    "        target = torch.randint(0, 2, output.shape)  # Target is a random tensor of the same shape as output\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(output, target.float())  # Ensure the target is a float tensor\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    embeddings = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for g in graphs:\n",
    "            if 'x' not in g.ndata:\n",
    "                g.ndata['x'] = torch.ones(g.number_of_nodes(), input_dim)  # Initialize with ones if 'h' is missing\n",
    "            embedding = model(g, g.ndata['x'])\n",
    "            final_embedding = calculate_integer_embedding(embedding)\n",
    "            embeddings.add(final_embedding)\n",
    "\n",
    "    print(\"embeddings: \", embeddings)\n",
    "    return len(embeddings)\n",
    "\n",
    "# Process all .pkl files in the current directory\n",
    "for filepath in glob.glob(\"../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset*.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'h' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=32, out_dim=8, num_layers=3)\n",
    "    pna_model = PNA(input_dim, hidden_dim=32, out_dim=8, num_layers=3,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]))  # Example degree tensor\n",
    "    \n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Computing equivalence classes using GIN model...\")\n",
    "    num_unique_embeddings_gin = compute_equivalence_classes(filepath, gin_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with GIN: {num_unique_embeddings_gin}\\n\")\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Computing equivalence classes using PNA model...\")\n",
    "    num_unique_embeddings_pna = compute_equivalence_classes(filepath, pna_model, input_dim)\n",
    "    print(f\"Number of unique embeddings with PNA: {num_unique_embeddings_pna}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fe10a-f50e-4e5c-b357-1faf16d8a88a",
   "metadata": {},
   "source": [
    "## Distinguishing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c9a1bb3-c3b4-4b73-a814-6e11ce31283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Processing ../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset_dummy.pkl...\n",
      "Using GIN model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 0.9999\n",
      "Epoch 14/100, Loss: 0.9999\n",
      "Epoch 15/100, Loss: 1.0000\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 0.9999\n",
      "Epoch 18/100, Loss: 0.9999\n",
      "Epoch 19/100, Loss: 0.9999\n",
      "Epoch 20/100, Loss: 0.9999\n",
      "Epoch 21/100, Loss: 0.9999\n",
      "Epoch 22/100, Loss: 0.9999\n",
      "Epoch 23/100, Loss: 0.9999\n",
      "Epoch 24/100, Loss: 0.9999\n",
      "Epoch 25/100, Loss: 0.9999\n",
      "Epoch 26/100, Loss: 0.9999\n",
      "Epoch 27/100, Loss: 0.9999\n",
      "Epoch 28/100, Loss: 0.9999\n",
      "Epoch 29/100, Loss: 0.9999\n",
      "Epoch 30/100, Loss: 0.9999\n",
      "Epoch 31/100, Loss: 0.9999\n",
      "Epoch 32/100, Loss: 0.9999\n",
      "Epoch 33/100, Loss: 0.9999\n",
      "Epoch 34/100, Loss: 0.9999\n",
      "Epoch 35/100, Loss: 0.9999\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 0.9999\n",
      "Epoch 38/100, Loss: 0.9999\n",
      "Epoch 39/100, Loss: 1.0000\n",
      "Epoch 40/100, Loss: 0.9999\n",
      "Epoch 41/100, Loss: 0.9999\n",
      "Epoch 42/100, Loss: 0.9999\n",
      "Epoch 43/100, Loss: 0.9999\n",
      "Epoch 44/100, Loss: 0.9999\n",
      "Epoch 45/100, Loss: 0.9999\n",
      "Epoch 46/100, Loss: 0.9999\n",
      "Epoch 47/100, Loss: 0.9999\n",
      "Epoch 48/100, Loss: 0.9999\n",
      "Epoch 49/100, Loss: 0.9999\n",
      "Epoch 50/100, Loss: 1.0000\n",
      "Epoch 51/100, Loss: 0.9999\n",
      "Epoch 52/100, Loss: 0.9999\n",
      "Epoch 53/100, Loss: 1.0000\n",
      "Epoch 54/100, Loss: 0.9999\n",
      "Epoch 55/100, Loss: 0.9999\n",
      "Epoch 56/100, Loss: 1.0000\n",
      "Epoch 57/100, Loss: 0.9999\n",
      "Epoch 58/100, Loss: 0.9999\n",
      "Epoch 59/100, Loss: 0.9999\n",
      "Epoch 60/100, Loss: 0.9999\n",
      "Epoch 61/100, Loss: 0.9999\n",
      "Epoch 62/100, Loss: 0.9999\n",
      "Epoch 63/100, Loss: 0.9999\n",
      "Epoch 64/100, Loss: 1.0000\n",
      "Epoch 65/100, Loss: 0.9999\n",
      "Epoch 66/100, Loss: 0.9999\n",
      "Epoch 67/100, Loss: 0.9999\n",
      "Epoch 68/100, Loss: 0.9999\n",
      "Epoch 69/100, Loss: 0.9999\n",
      "Epoch 70/100, Loss: 1.0000\n",
      "Epoch 71/100, Loss: 0.9999\n",
      "Epoch 72/100, Loss: 0.9999\n",
      "Epoch 73/100, Loss: 0.9999\n",
      "Epoch 74/100, Loss: 0.9999\n",
      "Epoch 75/100, Loss: 1.0000\n",
      "Epoch 76/100, Loss: 0.9999\n",
      "Epoch 77/100, Loss: 1.0000\n",
      "Epoch 78/100, Loss: 0.9999\n",
      "Epoch 79/100, Loss: 0.9999\n",
      "Epoch 80/100, Loss: 0.9999\n",
      "Epoch 81/100, Loss: 0.9999\n",
      "Epoch 82/100, Loss: 0.9999\n",
      "Epoch 83/100, Loss: 0.9999\n",
      "Epoch 84/100, Loss: 0.9999\n",
      "Epoch 85/100, Loss: 1.0000\n",
      "Epoch 86/100, Loss: 0.9999\n",
      "Epoch 87/100, Loss: 0.9999\n",
      "Epoch 88/100, Loss: 0.9999\n",
      "Epoch 89/100, Loss: 0.9999\n",
      "Epoch 90/100, Loss: 0.9999\n",
      "Epoch 91/100, Loss: 0.9999\n",
      "Epoch 92/100, Loss: 0.9999\n",
      "Epoch 93/100, Loss: 0.9999\n",
      "Epoch 94/100, Loss: 0.9999\n",
      "Epoch 95/100, Loss: 0.9999\n",
      "Epoch 96/100, Loss: 0.9999\n",
      "Epoch 97/100, Loss: 0.9999\n",
      "Epoch 98/100, Loss: 0.9999\n",
      "Epoch 99/100, Loss: 0.9999\n",
      "Epoch 100/100, Loss: 0.9999\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 20\n",
      "Incorrectly classified isomorphic pairs: 0\n",
      "Using PNA model...\n",
      "Epoch 1/100, Loss: 1.0000\n",
      "Epoch 2/100, Loss: 1.0000\n",
      "Epoch 3/100, Loss: 1.0000\n",
      "Epoch 4/100, Loss: 1.0000\n",
      "Epoch 5/100, Loss: 1.0000\n",
      "Epoch 6/100, Loss: 1.0000\n",
      "Epoch 7/100, Loss: 1.0000\n",
      "Epoch 8/100, Loss: 1.0000\n",
      "Epoch 9/100, Loss: 1.0000\n",
      "Epoch 10/100, Loss: 1.0000\n",
      "Epoch 11/100, Loss: 1.0000\n",
      "Epoch 12/100, Loss: 1.0000\n",
      "Epoch 13/100, Loss: 1.0000\n",
      "Epoch 14/100, Loss: 1.0000\n",
      "Epoch 15/100, Loss: 1.0000\n",
      "Epoch 16/100, Loss: 1.0000\n",
      "Epoch 17/100, Loss: 1.0000\n",
      "Epoch 18/100, Loss: 1.0000\n",
      "Epoch 19/100, Loss: 1.0000\n",
      "Epoch 20/100, Loss: 1.0000\n",
      "Epoch 21/100, Loss: 1.0000\n",
      "Epoch 22/100, Loss: 1.0000\n",
      "Epoch 23/100, Loss: 1.0000\n",
      "Epoch 24/100, Loss: 1.0000\n",
      "Epoch 25/100, Loss: 1.0000\n",
      "Epoch 26/100, Loss: 1.0000\n",
      "Epoch 27/100, Loss: 1.0000\n",
      "Epoch 28/100, Loss: 1.0000\n",
      "Epoch 29/100, Loss: 1.0000\n",
      "Epoch 30/100, Loss: 1.0000\n",
      "Epoch 31/100, Loss: 1.0000\n",
      "Epoch 32/100, Loss: 1.0000\n",
      "Epoch 33/100, Loss: 1.0000\n",
      "Epoch 34/100, Loss: 1.0000\n",
      "Epoch 35/100, Loss: 1.0000\n",
      "Epoch 36/100, Loss: 1.0000\n",
      "Epoch 37/100, Loss: 1.0000\n",
      "Epoch 38/100, Loss: 1.0000\n",
      "Epoch 39/100, Loss: 1.0000\n",
      "Epoch 40/100, Loss: 1.0000\n",
      "Epoch 41/100, Loss: 1.0000\n",
      "Epoch 42/100, Loss: 1.0000\n",
      "Epoch 43/100, Loss: 1.0000\n",
      "Epoch 44/100, Loss: 1.0000\n",
      "Epoch 45/100, Loss: 1.0000\n",
      "Epoch 46/100, Loss: 1.0000\n",
      "Epoch 47/100, Loss: 1.0000\n",
      "Epoch 48/100, Loss: 1.0000\n",
      "Epoch 49/100, Loss: 1.0000\n",
      "Epoch 50/100, Loss: 1.0000\n",
      "Epoch 51/100, Loss: 1.0000\n",
      "Epoch 52/100, Loss: 1.0000\n",
      "Epoch 53/100, Loss: 1.0000\n",
      "Epoch 54/100, Loss: 1.0000\n",
      "Epoch 55/100, Loss: 1.0000\n",
      "Epoch 56/100, Loss: 1.0000\n",
      "Epoch 57/100, Loss: 1.0000\n",
      "Epoch 58/100, Loss: 1.0000\n",
      "Epoch 59/100, Loss: 1.0000\n",
      "Epoch 60/100, Loss: 1.0000\n",
      "Epoch 61/100, Loss: 1.0000\n",
      "Epoch 62/100, Loss: 1.0000\n",
      "Epoch 63/100, Loss: 1.0000\n",
      "Epoch 64/100, Loss: 1.0000\n",
      "Epoch 65/100, Loss: 1.0000\n",
      "Epoch 66/100, Loss: 1.0000\n",
      "Epoch 67/100, Loss: 1.0000\n",
      "Epoch 68/100, Loss: 1.0000\n",
      "Epoch 69/100, Loss: 1.0000\n",
      "Epoch 70/100, Loss: 1.0000\n",
      "Epoch 71/100, Loss: 1.0000\n",
      "Epoch 72/100, Loss: 1.0000\n",
      "Epoch 73/100, Loss: 1.0000\n",
      "Epoch 74/100, Loss: 1.0000\n",
      "Epoch 75/100, Loss: 1.0000\n",
      "Epoch 76/100, Loss: 1.0000\n",
      "Epoch 77/100, Loss: 1.0000\n",
      "Epoch 78/100, Loss: 1.0000\n",
      "Epoch 79/100, Loss: 1.0000\n",
      "Epoch 80/100, Loss: 1.0000\n",
      "Epoch 81/100, Loss: 1.0000\n",
      "Epoch 82/100, Loss: 1.0000\n",
      "Epoch 83/100, Loss: 1.0000\n",
      "Epoch 84/100, Loss: 1.0000\n",
      "Epoch 85/100, Loss: 1.0000\n",
      "Epoch 86/100, Loss: 1.0000\n",
      "Epoch 87/100, Loss: 1.0000\n",
      "Epoch 88/100, Loss: 1.0000\n",
      "Epoch 89/100, Loss: 1.0000\n",
      "Epoch 90/100, Loss: 1.0000\n",
      "Epoch 91/100, Loss: 1.0000\n",
      "Epoch 92/100, Loss: 1.0000\n",
      "Epoch 93/100, Loss: 1.0000\n",
      "Epoch 94/100, Loss: 1.0000\n",
      "Epoch 95/100, Loss: 1.0000\n",
      "Epoch 96/100, Loss: 1.0000\n",
      "Epoch 97/100, Loss: 1.0000\n",
      "Epoch 98/100, Loss: 1.0000\n",
      "Epoch 99/100, Loss: 1.0000\n",
      "Epoch 100/100, Loss: 1.0000\n",
      "Correctly classified non-isomorphic pairs: 0\n",
      "Correctly classified isomorphic pairs: 20\n",
      "Incorrectly classified isomorphic pairs: 0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GINConv, SumPooling, PNAConv\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define a GIN model\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feats, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GINConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim)\n",
    "                ), 'sum'))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(GINConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(out_dim, out_dim)\n",
    "            ), 'sum'))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(out_dim))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            h = layer(g, h)\n",
    "            h = batch_norm(h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "# Define a PNA model\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, out_dim, num_layers, aggregators, scalers, deg):\n",
    "        super(PNA, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(PNAConv(in_feats, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PNAConv(hidden_dim, hidden_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(PNAConv(hidden_dim, out_dim, aggregators, scalers, deg))\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = SumPooling()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = torch.round(h * 100) / 100\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "            h = F.relu(h)\n",
    "        g_embedding = self.pool(g, h)\n",
    "        return g_embedding\n",
    "\n",
    "\n",
    "\n",
    "for filepath in glob.glob(\"../data/BREC/Distance_Regular/clo_G_Distance_Regular_dataset_dummy.pkl\"):\n",
    "    print(f\"------------- Processing {filepath}...\")\n",
    "\n",
    "    # Load graphs to determine input_dim\n",
    "    graphs = load_graphs_from_file(filepath)\n",
    "\n",
    "    non_isomorphic_pairs, isomorphic_pairs = organize_pairs(graphs, G_Distance_Regular_original_indices)\n",
    "\n",
    "    if 'x' in graphs[0].ndata:\n",
    "        input_dim = graphs[0].ndata['x'].size(1)  # Determine the input dimension dynamically\n",
    "    else:\n",
    "        # If 'x' does not exist, assume a default input dimension\n",
    "        input_dim = 1\n",
    "\n",
    "\n",
    "    # Initialize the models\n",
    "    gin_model = GIN(input_dim, hidden_dim=16, out_dim=8, num_layers=4).to(device)\n",
    "    pna_model = PNA(input_dim, hidden_dim=16, out_dim=8, num_layers=4,\n",
    "                    aggregators=['mean', 'max', 'sum', 'min', 'std'],\n",
    "                    scalers=['identity', 'amplification', 'attenuation'],\n",
    "                    deg=torch.tensor([1.0]).to(device)).to(device)  # Example degree tensor\n",
    "\n",
    "    # Compute the number of unique embeddings using GIN\n",
    "    print(\"Using GIN model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, gin_model, input_dim)\n",
    "\n",
    "    # Compute the number of unique embeddings using PNA\n",
    "    print(\"Using PNA model...\")\n",
    "    non_isomorphic_different_count, isomorphic_same_count, isomorphic_different_count = evaluate_model_with_pairs(non_isomorphic_pairs, isomorphic_pairs, pna_model, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a06c43-d8dd-487c-80ed-d9b73eaedd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
