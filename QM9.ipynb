{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/molnet_publish/qm9.zip\n",
      "Extracting dataset/QM9/raw/qm9.zip\n",
      "Downloading https://ndownloader.figshare.com/files/3195404\n",
      "/home/zhifei/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import QM9\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_scipy_sparse_matrix\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "qm9 = QM9(root='dataset/QM9')\n",
    "qm9_dataset = qm9[:10000]\n",
    "\n",
    "y_target = pd.DataFrame(qm9_dataset.data.y.numpy())\n",
    "qm9_dataset.data.y = torch.Tensor(y_target[0])   # Change target value\n",
    "\n",
    "qm9_dataset = qm9_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class GINENet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr):\n",
    "        super(GINENet, self).__init__()\n",
    "        \n",
    "        # Define GINE layers with the specified edge_dim\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(num_node_features, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()),\n",
    "            edge_dim=edge_attr)\n",
    "        \n",
    "        # Define linear layers for classification or regression\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through GINE layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PNAConv, global_add_pool\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "\n",
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dim_h, edge_attr, aggregators, scalers, deg):\n",
    "        super(PNANet, self).__init__()\n",
    "        \n",
    "        # Define PNA layers with specified aggregators, scalers, and degree tensor\n",
    "        self.conv1 = PNAConv(\n",
    "            in_channels=num_node_features,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv2 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        self.conv3 = PNAConv(\n",
    "            in_channels=dim_h,\n",
    "            out_channels=dim_h,\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_attr\n",
    "        )\n",
    "        \n",
    "        # Define linear layers for final graph-level output\n",
    "        self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
    "        self.lin2 = Linear(dim_h * 3, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Pass node features and edge attributes through PNA layers\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr)\n",
    "\n",
    "        # Apply global pooling for graph-level output\n",
    "        h1 = global_add_pool(h1, batch)\n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate pooled features and pass through final linear layers\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "        h = self.lin1(h).relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.transforms import VirtualNode, AddLaplacianEigenvectorPE\n",
    "from torch_geometric.utils import from_networkx, to_networkx, to_undirected, to_dense_adj, degree\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CustomQM9Dataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomQM9Dataset, self).__init__()\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "        \n",
    "\n",
    "###Visual Node\n",
    "def apply_vn(pyg_dataset):\n",
    "    vn_dataset = copy.deepcopy(pyg_dataset)  # Make a deep copy to preserve the original dataset\n",
    "    transform = VirtualNode()\n",
    "\n",
    "    # Create a list to store transformed graphs\n",
    "    transformed_data_list = []\n",
    "\n",
    "    for data in vn_dataset:\n",
    "        transformed_data_list.append(transform(data))\n",
    "\n",
    "    vn_dataset = CustomQM9Dataset(transformed_data_list)\n",
    "\n",
    "    return vn_dataset\n",
    "\n",
    "###Centrality\n",
    "def add_centrality_to_node_features(data, centrality_measure='degree'):\n",
    "    G = to_networkx(data, node_attrs=['x'], to_undirected=True)\n",
    "\n",
    "    # Compute the centrality measure\n",
    "    if centrality_measure == 'degree':\n",
    "        centrality = nx.degree_centrality(G)\n",
    "    elif centrality_measure == 'closeness':\n",
    "        centrality = nx.closeness_centrality(G)\n",
    "    elif centrality_measure == 'betweenness':\n",
    "        centrality = nx.betweenness_centrality(G)\n",
    "    elif centrality_measure == 'eigenvector':\n",
    "        if not nx.is_connected(G):\n",
    "        # Handle connected components separately\n",
    "            centrality = {}\n",
    "            for component in nx.connected_components(G):\n",
    "                subgraph = G.subgraph(component)\n",
    "                sub_centrality = nx.eigenvector_centrality(subgraph, max_iter=500, tol=1e-4)\n",
    "                centrality.update(sub_centrality)\n",
    "        else:\n",
    "            centrality = nx.eigenvector_centrality(G, max_iter=500, tol=1e-4)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "    \n",
    "    # Convert centrality to tensor and add as node feature\n",
    "    centrality_values = list(centrality.values())\n",
    "    centrality_tensor = torch.tensor(centrality_values, dtype=torch.float).view(-1, 1)\n",
    "    centrality_tensor = (centrality_tensor - centrality_tensor.mean()) / (centrality_tensor.std() + 1e-8)\n",
    "    data.x = torch.cat([data.x, centrality_tensor], dim=-1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def centrality(dataset, centrality_measure='degree'):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    addCentrality_list = []\n",
    "    for data in original_dataset:\n",
    "        if centrality_measure == 'degree':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='degree')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'closeness':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='closeness')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'betweenness':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='betweenness')\n",
    "            addCentrality_list.append(data)\n",
    "        elif centrality_measure == 'eigenvector':\n",
    "            data = add_centrality_to_node_features(data, centrality_measure='eigenvector')\n",
    "            addCentrality_list.append(data)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown centrality measure: {centrality_measure}')\n",
    "        \n",
    "    addCentrality_dataset = CustomQM9Dataset(addCentrality_list)\n",
    "\n",
    "    return addCentrality_dataset\n",
    "\n",
    "###Distance Encoding\n",
    "def distance_encoding_node_augmentation(data):\n",
    "    G = to_networkx(data, node_attrs=['x'], to_undirected = True)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Initialize the distance matrix with infinity\n",
    "    distance_matrix = [[float('inf')] * num_nodes for _ in range(num_nodes)]\n",
    "    shortest_paths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    # Populate the distance matrix with actual shortest path lengths\n",
    "    for i in range(num_nodes):\n",
    "        distance_matrix[i][i] = 0  # Distance to self is 0\n",
    "        if i in shortest_paths:\n",
    "            for j, d in shortest_paths[i].items():\n",
    "                distance_matrix[i][j] = d\n",
    "\n",
    "    # Convert the distance matrix to a tensor\n",
    "    distance_tensor = torch.tensor(distance_matrix, dtype=torch.float)\n",
    "    \n",
    "    # Example: Add average distance to node features\n",
    "    finite_distances = torch.where(distance_tensor == float('inf'), torch.tensor(float('nan')), distance_tensor)\n",
    "    average_distance = torch.nanmean(finite_distances, dim=1).view(-1, 1)  # Use nanmean to ignore infinities\n",
    "    data.x = torch.cat([data.x, average_distance], dim=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def distance_encoding_edge_rewiring(data):\n",
    "    \"\"\"\n",
    "    Add edges between all pairs of nodes with shortest path distance as a new edge attribute,\n",
    "    while preserving original edge attributes.\n",
    "    \"\"\"\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'], to_undirected=True)\n",
    "    G_transformed = G.copy()\n",
    "\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    shortest_paths = {}\n",
    "\n",
    "    # Compute shortest paths for each connected component\n",
    "    for component in connected_components:\n",
    "        subgraph = G.subgraph(component)\n",
    "        component_paths = dict(nx.all_pairs_shortest_path_length(subgraph))\n",
    "        shortest_paths.update(component_paths)\n",
    "\n",
    "    num_edge_attrs = data.edge_attr.shape[1] if data.edge_attr is not None else 0\n",
    "\n",
    "    nodes = list(G.nodes)\n",
    "    for i in nodes:\n",
    "        for j in nodes:\n",
    "            if i != j:\n",
    "                if G.has_edge(i, j):\n",
    "                    original_attr = G[i][j].get('edge_attr', [])\n",
    "                    if not isinstance(original_attr, list):\n",
    "                        original_attr = [original_attr]\n",
    "                    G_transformed[i][j]['edge_attr'] = original_attr + [1]\n",
    "                else:\n",
    "                    if j in shortest_paths[i]:\n",
    "                        distance = shortest_paths[i][j]\n",
    "                    else:\n",
    "                        distance = 1e9  # Replace inf with a large finite value\n",
    "                    new_attr = [0] * num_edge_attrs + [distance]\n",
    "                    G_transformed.add_edge(i, j, edge_attr=new_attr)\n",
    "\n",
    "    new_data = from_networkx(G_transformed, group_node_attrs=['x'], group_edge_attrs=['edge_attr'])\n",
    "    new_data.edge_attr = torch.tensor(new_data.edge_attr, dtype=torch.float)\n",
    "\n",
    "    # Check for invalid values\n",
    "    if torch.isnan(new_data.edge_attr).any() or torch.isinf(new_data.edge_attr).any():\n",
    "        raise ValueError(\"Edge attributes contain invalid values!\")\n",
    "\n",
    "    new_data.y = data.y\n",
    "    return new_data\n",
    "\n",
    "def distance_encoding(dataset, method = 'node_augmentation'):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    distance_encoding_list = []\n",
    "    for data in original_dataset:\n",
    "        if method == 'node_augmentation':\n",
    "            data = distance_encoding_node_augmentation(data)\n",
    "            distance_encoding_list.append(data)\n",
    "        elif method == 'edge_rewiring':\n",
    "            data = distance_encoding_edge_rewiring(data)\n",
    "            distance_encoding_list.append(data)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown distance encoding method: {method}')\n",
    "    distance_encoding_dataset = CustomQM9Dataset(distance_encoding_list)\n",
    "    return distance_encoding_dataset\n",
    "\n",
    "###Subgraph Extraction\n",
    "def extract_local_subgraph_features(data, radius=2):\n",
    "    # Convert PyG data to NetworkX graph\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'], to_undirected=True)\n",
    "\n",
    "    # Initialize a list to store subgraph features for each node\n",
    "    subgraph_sizes = []\n",
    "    subgraph_degrees = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        # Extract the ego graph (subgraph) around the node\n",
    "        subgraph = nx.ego_graph(G, node, radius=radius)\n",
    "        \n",
    "        # Example feature 1: Size of the subgraph (number of nodes)\n",
    "        subgraph_size = subgraph.number_of_nodes()\n",
    "        subgraph_sizes.append(subgraph_size)\n",
    "        \n",
    "        # Example feature 2: Average degree of the subgraph\n",
    "        subgraph_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "        subgraph_degrees.append(subgraph_degree)\n",
    "        \n",
    "    # Convert the features to tensors and add them as node features\n",
    "    subgraph_sizes_tensor = torch.tensor(subgraph_sizes, dtype=torch.float).view(-1, 1)\n",
    "    subgraph_degrees_tensor = torch.tensor(subgraph_degrees, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Concatenate the new features to the existing node features\n",
    "    data.x = torch.cat([data.x, subgraph_sizes_tensor, subgraph_degrees_tensor], dim=-1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def subgraph_extraction(dataset, radius=2):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    subgraph_extraction_list = []\n",
    "    for data in original_dataset:\n",
    "        data = extract_local_subgraph_features(data, radius=radius)\n",
    "        subgraph_extraction_list.append(data)\n",
    "    subgraph_extraction_dataset = CustomQM9Dataset(subgraph_extraction_list)\n",
    "    return subgraph_extraction_dataset\n",
    "\n",
    "def canonicalize_eigenvectors(eigenvectors):\n",
    "    \"\"\"\n",
    "    Canonicalize eigenvectors by fixing their signs for consistency.\n",
    "    This ensures that isomorphic graphs will have the same eigenvectors.\n",
    "    \"\"\"\n",
    "    for i in range(eigenvectors.shape[1]):\n",
    "        if eigenvectors[0, i] < 0:  # Flip sign if the first element is negative\n",
    "            eigenvectors[:, i] = -eigenvectors[:, i]\n",
    "    return eigenvectors\n",
    "\n",
    "def add_canonicalized_laplacian_pe_pyg(data, k=5, max_features=12):\n",
    "    \"\"\"\n",
    "    Add canonicalized Laplacian positional encoding to a PyG data object.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): PyG data object.\n",
    "        k (int): Number of Laplacian eigenvectors to compute.\n",
    "        max_features (int): Total desired node feature dimensions after encoding.\n",
    "\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): PyG data object with Laplacian PE appended to node features.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert PyG graph to NetworkX graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Step 2: Compute sparse adjacency matrix\n",
    "    adj = nx.to_scipy_sparse_array(G, format='csr').astype(np.float64)\n",
    "\n",
    "    # Step 3: Compute normalized Laplacian matrix\n",
    "    laplacian = csgraph.laplacian(adj, normed=True)\n",
    "\n",
    "    # Step 4: Handle small graphs\n",
    "    num_nodes = adj.shape[0]\n",
    "    if num_nodes <= 1:\n",
    "        raise ValueError(f\"Graph has too few nodes ({num_nodes}) for Laplacian PE.\")\n",
    "    num_eigenvectors = min(k, num_nodes - 1)  # Ensure k < num_nodes\n",
    "\n",
    "    # Step 5: Compute the smallest eigenvectors using sparse methods\n",
    "    eigenvalues, eigenvectors = eigsh(laplacian, k=num_eigenvectors, which='SM')  # Smallest magnitude eigenvalues\n",
    "\n",
    "    # Step 6: Canonicalize eigenvectors\n",
    "    eigenvectors = canonicalize_eigenvectors(torch.tensor(eigenvectors, dtype=torch.float))\n",
    "\n",
    "    # Step 7: Pad eigenvectors if less than max_features\n",
    "    padding = torch.zeros((eigenvectors.shape[0], max_features - num_eigenvectors))\n",
    "    eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "\n",
    "    # Step 8: Append the eigenvectors as new node features\n",
    "    if 'x' in data:\n",
    "        num_existing_features = data.x.shape[1]\n",
    "        padding_existing = torch.zeros((data.x.shape[0], max_features - num_existing_features))\n",
    "        data.x = torch.cat([data.x, padding_existing, eigenvectors], dim=1)\n",
    "    else:\n",
    "        data.x = eigenvectors\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_encoding(dataset, k=3, max_features=12, batch_size=100):\n",
    "    \"\"\"\n",
    "    Apply canonicalized Laplacian positional encoding to a PyG dataset in batches.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of torch_geometric.data.Data): List of PyG data objects.\n",
    "        k (int): Number of Laplacian eigenvectors to compute.\n",
    "        max_features (int): Total desired node feature dimensions after encoding.\n",
    "        batch_size (int): Number of graphs to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset (list of torch_geometric.data.Data): List of PyG data objects with Laplacian PE added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        for data in batch:\n",
    "            data_copy = data.clone()  # Ensure original dataset remains unchanged\n",
    "            try:\n",
    "                graph_pe = add_canonicalized_laplacian_pe_pyg(data_copy, k=k, max_features=max_features)\n",
    "                encoded_dataset.append(graph_pe)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing graph (index {i}): {e}\")\n",
    "    ge_dataset = CustomQM9Dataset(encoded_dataset)\n",
    "    return ge_dataset\n",
    "\n",
    "###Add Extra Node on Each Edge\n",
    "def add_extra_node_on_each_edge(data):\n",
    "    # Convert PyG data to a NetworkX graph for easier manipulation\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs = ['edge_attr'])\n",
    "    \n",
    "    # Original number of nodes\n",
    "    num_original_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Prepare lists for new features\n",
    "    edges = list(G.edges(data=True))\n",
    "    new_node_features = []\n",
    "    new_edges_src = []\n",
    "    new_edges_dst = []\n",
    "    new_edge_features = []\n",
    "\n",
    "    for u, v, edge_data in edges:\n",
    "        # Remove the original edge\n",
    "        G.remove_edge(u, v)\n",
    "\n",
    "        # Create new node as the mean of connected node features\n",
    "        new_node_id = num_original_nodes + len(new_node_features)\n",
    "        new_node_feature = (data.x[u] + data.x[v]) / 2\n",
    "        new_node_features.append(new_node_feature)\n",
    "        \n",
    "        # Add new node with feature\n",
    "        G.add_node(new_node_id, x=new_node_feature)\n",
    "\n",
    "        # Add edges from new node to each original node\n",
    "        G.add_edge(u, new_node_id)\n",
    "        G.add_edge(new_node_id, v)\n",
    "\n",
    "        # Use original edge feature for each new edge\n",
    "        edge_feature = edge_data['edge_attr']\n",
    "        edge_feature_tensor = (\n",
    "            edge_feature if isinstance(edge_feature, torch.Tensor) else torch.tensor(edge_feature)\n",
    "        )\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (u, new_node_id)\n",
    "        new_edge_features.append(edge_feature_tensor)  # for edge (new_node_id, v)\n",
    "    \n",
    "    # Convert back to PyG Data object\n",
    "    modified_data = from_networkx(G)\n",
    "\n",
    "    # Update node features\n",
    "    modified_data.x = torch.cat([data.x, torch.stack(new_node_features)], dim=0)\n",
    "\n",
    "    # Update edge features to include only the new edges\n",
    "    modified_data.edge_attr = torch.stack(new_edge_features)  # Only include new edge features\n",
    "\n",
    "    # Preserve any additional global attributes\n",
    "    modified_data.y = data.y\n",
    "    \n",
    "    return modified_data\n",
    "\n",
    "def extra_node(dataset):\n",
    "    original_dataset = copy.deepcopy(dataset)\n",
    "    extra_node_list = []\n",
    "    for data in original_dataset:\n",
    "        data = add_extra_node_on_each_edge(data)\n",
    "        extra_node_list.append(data)\n",
    "    extra_node_dataset = CustomQM9Dataset(extra_node_list)\n",
    "    return extra_node_dataset\n",
    "\n",
    "def count_3_star(G):\n",
    "    \"\"\"Count 3-star graphlets for each node.\"\"\"\n",
    "    star_counts = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degree = len(neighbors)\n",
    "        # Count the number of 3-combinations of neighbors\n",
    "        star_counts[node] = max(0, (degree * (degree - 1) * (degree - 2)) // 6)\n",
    "    return star_counts\n",
    "\n",
    "def count_tailed_triangle(G):\n",
    "    \"\"\"Count tailed triangle graphlets for each node.\"\"\"\n",
    "    tail_counts = {node: 0 for node in G.nodes()}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        for neighbor in neighbors:\n",
    "            for other in neighbors:\n",
    "                if neighbor != other and G.has_edge(neighbor, other):\n",
    "                    for extra in G.neighbors(node):\n",
    "                        if extra not in {neighbor, other}:\n",
    "                            tail_counts[node] += 1\n",
    "    return tail_counts\n",
    "\n",
    "def count_4_cycle(G):\n",
    "    \"\"\"Count 4-cycle graphlets for each node in an undirected graph G.\"\"\"\n",
    "    cycle_counts = {node: 0 for node in G.nodes()}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        # Iterate over all unique neighbor pairs\n",
    "        for i, neighbor1 in enumerate(neighbors):\n",
    "            for neighbor2 in neighbors[i + 1:]:\n",
    "                # Find common neighbors of neighbor1 and neighbor2\n",
    "                shared_neighbors = set(G.neighbors(neighbor1)).intersection(G.neighbors(neighbor2))\n",
    "                # Add the count of shared neighbors to node's 4-cycle count\n",
    "                cycle_counts[node] += len(shared_neighbors)\n",
    "    \n",
    "    # Each 4-cycle is counted 4 times (once per node in the cycle)\n",
    "    cycle_counts = {node: count // 4 for node, count in cycle_counts.items()}\n",
    "    \n",
    "    return cycle_counts\n",
    "\n",
    "def graphlet_based_encoding_pyg(data):\n",
    "    \"\"\"\n",
    "    Add graphlet-based features (3-star, triangle, tailed triangle, 4-cycle) to node features in PyG.\n",
    "\n",
    "    Args:\n",
    "        data: PyG Data object.\n",
    "\n",
    "    Returns:\n",
    "        data: PyG Data object with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    # Convert PyG graph to NetworkX\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Count graphlets\n",
    "    triangle_counts = nx.triangles(G)  # Triangle counts\n",
    "    star_counts = count_3_star(G)  # 3-star graphlets\n",
    "    tail_counts = count_tailed_triangle(G)  # Tailed triangles\n",
    "    cycle_counts = count_4_cycle(G)  # 4-cycles\n",
    "\n",
    "    # Combine features into tensors\n",
    "    num_nodes = data.num_nodes\n",
    "    triangle_tensor = torch.tensor([triangle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    star_tensor = torch.tensor([star_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    tail_tensor = torch.tensor([tail_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "    cycle_tensor = torch.tensor([cycle_counts[node] for node in range(num_nodes)], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Concatenate all graphlet features\n",
    "    graphlet_features = torch.cat([triangle_tensor, star_tensor, tail_tensor, cycle_tensor], dim=1)\n",
    "\n",
    "    # Add to node features\n",
    "    if data.x is not None:\n",
    "        data.x = torch.cat([data.x, graphlet_features], dim=1)\n",
    "    else:\n",
    "        data.x = graphlet_features\n",
    "\n",
    "    return data\n",
    "\n",
    "def graphlet_encoding_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Apply graphlet-based encoding to a PyG dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of PyG Data objects.\n",
    "\n",
    "    Returns:\n",
    "        encoded_dataset: List of PyG Data objects with graphlet-based features added.\n",
    "    \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for data in dataset:\n",
    "        data_copy = data.clone()  # Use PyG's clone method for deep copy\n",
    "        graph_encoded = graphlet_based_encoding_pyg(data_copy)\n",
    "        encoded_dataset.append(graph_encoded)\n",
    "    gle_dataset = CustomQM9Dataset(encoded_dataset)\n",
    "    return gle_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_649844/3475245796.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data.edge_attr = torch.tensor(new_data.edge_attr, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "qm9_vn = apply_vn(qm9_dataset)\n",
    "qm9_deg = centrality(qm9_dataset, centrality_measure='degree')\n",
    "qm9_clo = centrality(qm9_dataset, centrality_measure='closeness')\n",
    "qm9_bet = centrality(qm9_dataset, centrality_measure='betweenness')\n",
    "qm9_eig = centrality(qm9_dataset, centrality_measure='eigenvector')\n",
    "qm9_de_n = distance_encoding(qm9_dataset, method='node_augmentation')\n",
    "qm9_de_g = distance_encoding(qm9_dataset, method='edge_rewiring')\n",
    "qm9_ge = graph_encoding(qm9_dataset, k=3)\n",
    "qm9_se = subgraph_extraction(qm9_dataset, radius=3)\n",
    "qm9_exN = extra_node(qm9_dataset)\n",
    "qm9_gle = graphlet_encoding_dataset(qm9_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
    "        model (nn.Module): GNN model to train on\n",
    "        loss (nn.functional): loss function to use during training\n",
    "        optimizer (torch.optim): optimizer during training\n",
    "\n",
    "    Returns:\n",
    "        float: training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data.x = data.x.float()\n",
    "\n",
    "        out = model(data)\n",
    "\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        current_loss += l / len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model\n",
    "\n",
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): validation set in batches\n",
    "        model (nn.Module): current trained model\n",
    "        loss (nn.functional): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "    return val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): test dataset\n",
    "        model (nn.Module): trained model\n",
    "\n",
    "    Returns:\n",
    "        float: test loss\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        # NOTE\n",
    "        # out = out.view(d.y.size())\n",
    "        l = loss(out, torch.reshape(data.y, (len(data.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    \"\"\"Training over all epochs\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs to train for\n",
    "        model (nn.Module): the current model\n",
    "        train_loader (DataLoader): training data in batches\n",
    "        val_loader (DataLoader): validation data in batches\n",
    "        path (string): path to save the best model\n",
    "\n",
    "    Returns:\n",
    "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.L1Loss()\n",
    "\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loader, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "        train_loss[epoch] = epoch_loss.detach().cpu().numpy()\n",
    "        val_loss[epoch] = v_loss.detach().cpu().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch: \"\n",
    "                + str(epoch)\n",
    "                + \", Train loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return best_loss, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.transforms import AddLaplacianEigenvectorPE\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "import copy\n",
    "\n",
    "# List of datasets with their respective names and transformations\n",
    "datasets = [\n",
    "    ('Base', qm9_dataset),\n",
    "    ('VN', qm9_vn),\n",
    "    ('DEG', qm9_deg),\n",
    "    ('BET', qm9_bet),\n",
    "    ('CLO', qm9_clo),\n",
    "    ('DE_N', qm9_de_n),\n",
    "    ('DE_G', qm9_de_g),\n",
    "    ('EXN', qm9_exN),\n",
    "    ('SE', qm9_se),\n",
    "    ('GE', qm9_ge),    # this is referred to Laplacian PE\n",
    "    ('EIG', qm9_eig),\n",
    "    ('GLE', qm9_gle)\n",
    "]\n",
    "\n",
    "\n",
    "# Common parameters\n",
    "data_size = 10000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "epochs = 101\n",
    "dim_h = 64\n",
    "aggregators = ['mean', 'min', 'max', 'std']\n",
    "scalers = ['identity', 'amplification', 'attenuation']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loop through each dataset transformation\n",
    "for name, dataset in datasets:\n",
    "    \n",
    "    # Normalize target\n",
    "    data_mean = dataset.data.y[:train_index].mean()\n",
    "    data_std = dataset.data.y[:train_index].std()\n",
    "    dataset.data.y = (dataset.data.y - data_mean) / data_std\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset[:train_index], batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(dataset[train_index:test_index], batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(dataset[test_index:val_index], batch_size=64, shuffle=True)\n",
    "\n",
    "    # Determine maximum degree and degree histogram\n",
    "    max_degree = int(max([degree(data.edge_index[1], num_nodes=data.num_nodes).max().item() for data in dataset[:train_index]]))\n",
    "    deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "    for data in dataset[:train_index]:\n",
    "        d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "        deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "    # Set model parameters\n",
    "    num_features = dataset[0].x.shape[1]\n",
    "    edge_attr = dataset[0].edge_attr.shape[1]\n",
    "    model = PNANet(num_features, dim_h=64, edge_attr=edge_attr, aggregators=aggregators, scalers=scalers, deg=deg).to(device)\n",
    "\n",
    "    # Train and save model with dataset-specific path\n",
    "    model_path = f\"./data/QM9/PNA_0_model_{name}.pt\"         # the number 0 represents the target number\n",
    "    pna_best_loss, pna_train_loss, pna_val_loss = train_epochs(epochs, model, train_loader, val_loader, model_path)\n",
    "    \n",
    "\n",
    "    # Print summary for each dataset\n",
    "    print(f\"\\nCompleted training for dataset: {name}\")\n",
    "    print(f\"  Model saved at: {model_path}\")\n",
    "    print(f\"  Best Validation Loss: {pna_best_loss}\")\n",
    "    print(f\"  Final Training Loss: {pna_train_loss[-1]}\")\n",
    "    print(f\"  Final Validation Loss: {pna_val_loss[-1]}\\n\")\n",
    "\n",
    "    # load our model\n",
    "    model = PNANet(num_features, dim_h, edge_attr, aggregators, scalers, deg).to(device)\n",
    "    model.load_state_dict(torch.load(f\"./data/QM9/PNA_0_model_{name}.pt\"))   \n",
    "\n",
    "    # calculate test loss\n",
    "    pna_test_loss = testing(test_loader, model)\n",
    "\n",
    "    print(\"Test Loss for PNA: \" + str(pna_test_loss.item()))\n",
    "\n",
    "    model = GINENet(num_features, dim_h=64, edge_attr=edge_attr).to(device)\n",
    "\n",
    "    # Train and save model with dataset-specific path\n",
    "    model_path = f\"./data/QM9/GIN_0_model_{name}.pt\"\n",
    "    gin_best_loss, gin_train_loss, gin_val_loss = train_epochs(epochs, model, train_loader, val_loader, model_path)\n",
    "\n",
    "    \n",
    "    # Print summary for each dataset\n",
    "    print(f\"\\nCompleted training for dataset: {name}\")\n",
    "    print(f\"  Model saved at: {model_path}\")\n",
    "    print(f\"  Best Validation Loss: {gin_best_loss}\")\n",
    "    print(f\"  Final Training Loss: {gin_train_loss[-1]}\")\n",
    "    print(f\"  Final Validation Loss: {gin_val_loss[-1]}\\n\")\n",
    "\n",
    "    # load our model\n",
    "    model = GINENet(num_features, dim_h=64, edge_attr=edge_attr).to(device)\n",
    "    model.load_state_dict(torch.load(f\"./data/QM9/GIN_0_model_{name}.pt\"))\n",
    "\n",
    "    # calculate test loss\n",
    "    gin_test_loss = testing(test_loader, model)\n",
    "\n",
    "    print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
